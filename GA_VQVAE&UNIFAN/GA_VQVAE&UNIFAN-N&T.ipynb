{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f045d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from torch.utils.data import random_split,Dataset,DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from joblib.externals.loky.backend.context import get_context\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import tarfile\n",
    "\n",
    "import time\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae930554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "from sklearn.metrics.cluster import adjusted_rand_score, adjusted_mutual_info_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a3d3a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unifan.networks import Encoder, Decoder, Set2Gene\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bb2e1a",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56b897c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n",
    "AE_batch_size=128\n",
    "AE_learning_rate = 0.001\n",
    "AE_decay_factor = 0.9\n",
    "AE_epochs = 300\n",
    "AE_random_seed = 123\n",
    "best_ari = 0\n",
    "best_nmi = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f0f7c2",
   "metadata": {},
   "source": [
    "# My Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63a14402",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read(\"data/cortex.h5ad\", dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67f813b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_matrix = adata.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aab3ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = adata.X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13fcb7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_true = adata.obs[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29821fc7",
   "metadata": {},
   "source": [
    "# Dataset&DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7af102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self,gene_matrix):\n",
    "        \n",
    "        self.gene_matrix = torch.from_numpy(gene_matrix)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.gene_matrix.shape[0]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        X = self.gene_matrix[idx]\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2c62125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loader(X,batch_size):\n",
    "\n",
    "    #print(type(X))\n",
    "    mydataset = MyDataset(X)\n",
    "\n",
    "    dataloader = DataLoader(dataset=mydataset,batch_size=batch_size,shuffle=True)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40651976",
   "metadata": {},
   "source": [
    "# autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5d444bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, input_dim: int = 10000,\n",
    "                 hidden_dim:int = 128,encoder_dim: int = 128, emission_dim: int = 128,\n",
    "                 num_layers_encoder: int = 2,num_layers_decoder: int = 2,\n",
    "                 z_dim: int = 32,\n",
    "                 dropout_rate:float = 0.1):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(input_dim, z_dim, num_layers=num_layers_encoder,hidden_dim=encoder_dim)\n",
    "        self.decoder = Decoder(z_dim, input_dim, num_layers=num_layers_decoder,hidden_dim=emission_dim)\n",
    "\n",
    "        # initialize loss\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        x = data\n",
    "\n",
    "        z_e,_ = self.encoder(x)\n",
    "\n",
    "        x_e,_ = self.decoder(z_e)\n",
    "\n",
    "        return x_e, z_e\n",
    "\n",
    "    def _loss_reconstruct(self, x, x_e):\n",
    "\n",
    "        l_e = self.mse_loss(x, x_e)\n",
    "        \n",
    "        mse_l = l_e\n",
    "\n",
    "        return mse_l\n",
    "\n",
    "    def loss(self, x, x_e):\n",
    "        \n",
    "        l = self._loss_reconstruct(x, x_e)\n",
    "        \n",
    "        return l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3573ce1d",
   "metadata": {},
   "source": [
    "# Pretrain Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94c37430",
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_dataloader = create_loader(gene_matrix,batch_size = AE_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dfba797",
   "metadata": {},
   "outputs": [],
   "source": [
    "AE = autoencoder(input_dim = G).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "619080b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_optimizer = torch.optim.Adam(AE.parameters(), lr=AE_learning_rate)\n",
    "AE_scheduler = torch.optim.lr_scheduler.StepLR(AE_optimizer, 1000,AE_decay_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9080c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_result_AE(model,dataloader,clusters_true,gene_matrix):\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    gene_matrix = torch.from_numpy(gene_matrix).to(device)\n",
    "\n",
    "    x_e,z_e = model(gene_matrix)\n",
    "    \n",
    "    z_e = z_e.detach().cpu().numpy()\n",
    "    \n",
    "    adata.obsm['X_unifan'] = z_e\n",
    "    \n",
    "    sc.pp.neighbors(adata, n_pcs=32,use_rep='X_unifan', random_state=123)\n",
    "    \n",
    "    sc.tl.leiden(adata, resolution=1, random_state=123)\n",
    "    \n",
    "    clusters_pre = adata.obs['leiden'].astype('int').values  # original as string\n",
    "    \n",
    "    ari = adjusted_rand_score(clusters_pre, clusters_true)\n",
    "\n",
    "    nmi = adjusted_mutual_info_score(clusters_pre, clusters_true)\n",
    "\n",
    "    return ari,nmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77ebd78a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 total_loss:0.578575849533081 ari:0.1354668020671266 nmi:0.350115993041843\n",
      "epoch:2 total_loss:0.4777005612850189 ari:0.1354668020671266 nmi:0.350115993041843\n",
      "epoch:3 total_loss:0.24634608626365662 ari:0.1354668020671266 nmi:0.350115993041843\n",
      "epoch:4 total_loss:0.1783933937549591 ari:0.1354668020671266 nmi:0.350115993041843\n",
      "epoch:5 total_loss:0.17381924390792847 ari:0.1354668020671266 nmi:0.350115993041843\n",
      "epoch:6 total_loss:0.1695861518383026 ari:0.1354668020671266 nmi:0.350115993041843\n",
      "epoch:7 total_loss:0.16246333718299866 ari:0.1354668020671266 nmi:0.350115993041843\n",
      "epoch:8 total_loss:0.15150859951972961 ari:0.1354668020671266 nmi:0.350115993041843\n",
      "epoch:9 total_loss:0.13707037270069122 ari:0.1354668020671266 nmi:0.350115993041843\n",
      "epoch:10 total_loss:0.13028106093406677 ari:0.1354668020671266 nmi:0.350115993041843\n",
      "epoch:11 total_loss:0.12571397423744202 ari:0.1354668020671266 nmi:0.350115993041843\n",
      "epoch:12 total_loss:0.12524577975273132 ari:0.1354668020671266 nmi:0.350115993041843\n",
      "epoch:13 total_loss:0.11860541999340057 ari:0.1354668020671266 nmi:0.350115993041843\n",
      "epoch:14 total_loss:0.11044071614742279 ari:0.13762778462570166 nmi:0.35908522675027943\n",
      "epoch:15 total_loss:0.10199512541294098 ari:0.13762778462570166 nmi:0.35908522675027943\n",
      "epoch:16 total_loss:0.09911511093378067 ari:0.13762778462570166 nmi:0.35908522675027943\n",
      "epoch:17 total_loss:0.09778741002082825 ari:0.13762778462570166 nmi:0.35908522675027943\n",
      "epoch:18 total_loss:0.08874452859163284 ari:0.13762778462570166 nmi:0.35908522675027943\n",
      "epoch:19 total_loss:0.09970027953386307 ari:0.13762778462570166 nmi:0.35908522675027943\n",
      "epoch:20 total_loss:0.09968361258506775 ari:0.13762778462570166 nmi:0.35908522675027943\n",
      "epoch:21 total_loss:0.08648046106100082 ari:0.13762778462570166 nmi:0.35908522675027943\n",
      "epoch:22 total_loss:0.07762754708528519 ari:0.13762778462570166 nmi:0.35908522675027943\n",
      "epoch:23 total_loss:0.0725201815366745 ari:0.13762778462570166 nmi:0.35908522675027943\n",
      "epoch:24 total_loss:0.07294953614473343 ari:0.13762778462570166 nmi:0.35908522675027943\n",
      "epoch:25 total_loss:0.07181079685688019 ari:0.13762778462570166 nmi:0.35908522675027943\n",
      "epoch:26 total_loss:0.14883743226528168 ari:0.13762778462570166 nmi:0.35908522675027943\n",
      "epoch:27 total_loss:0.0975521057844162 ari:0.13762778462570166 nmi:0.35908522675027943\n",
      "epoch:28 total_loss:0.08646321296691895 ari:0.13762778462570166 nmi:0.35908522675027943\n",
      "epoch:29 total_loss:0.07280442118644714 ari:0.1379123947940209 nmi:0.36564843686398185\n",
      "epoch:30 total_loss:0.06282120198011398 ari:0.1379123947940209 nmi:0.36564843686398185\n",
      "epoch:31 total_loss:0.07170718908309937 ari:0.13807600938225809 nmi:0.3663432145597961\n",
      "epoch:32 total_loss:0.058990079909563065 ari:0.13848831976248624 nmi:0.36940426692302397\n",
      "epoch:33 total_loss:0.05857294425368309 ari:0.13848831976248624 nmi:0.36940426692302397\n",
      "epoch:34 total_loss:0.0710458904504776 ari:0.13848831976248624 nmi:0.36940426692302397\n",
      "epoch:35 total_loss:0.06923585385084152 ari:0.13848831976248624 nmi:0.36940426692302397\n",
      "epoch:36 total_loss:0.0580216683447361 ari:0.15075731296929584 nmi:0.37369087279537433\n",
      "epoch:37 total_loss:0.05421611666679382 ari:0.1560137065776201 nmi:0.37775167630710915\n",
      "epoch:38 total_loss:0.06498042494058609 ari:0.1560137065776201 nmi:0.37775167630710915\n",
      "epoch:39 total_loss:0.06332458555698395 ari:0.1612654570818053 nmi:0.38543631702099157\n",
      "epoch:40 total_loss:0.06686478108167648 ari:0.1612654570818053 nmi:0.38543631702099157\n",
      "epoch:41 total_loss:0.09318264573812485 ari:0.1612654570818053 nmi:0.38543631702099157\n",
      "epoch:42 total_loss:0.0787942036986351 ari:0.1612654570818053 nmi:0.38543631702099157\n",
      "epoch:43 total_loss:0.06628637760877609 ari:0.1612654570818053 nmi:0.38543631702099157\n",
      "epoch:44 total_loss:0.0554615743458271 ari:0.1612654570818053 nmi:0.38543631702099157\n",
      "epoch:45 total_loss:0.05811452865600586 ari:0.1612654570818053 nmi:0.38543631702099157\n",
      "epoch:46 total_loss:0.06019281595945358 ari:0.1612654570818053 nmi:0.38543631702099157\n",
      "epoch:47 total_loss:0.05431506782770157 ari:0.1612654570818053 nmi:0.38543631702099157\n",
      "epoch:48 total_loss:0.0528755709528923 ari:0.16444910948549935 nmi:0.4029844324022864\n",
      "epoch:49 total_loss:0.052622806280851364 ari:0.16444910948549935 nmi:0.4029844324022864\n",
      "epoch:50 total_loss:0.05153874680399895 ari:0.16813582558567952 nmi:0.41012525141173756\n",
      "epoch:51 total_loss:0.051158707588911057 ari:0.16813582558567952 nmi:0.41012525141173756\n",
      "epoch:52 total_loss:0.051087312400341034 ari:0.16813582558567952 nmi:0.41012525141173756\n",
      "epoch:53 total_loss:0.050790466368198395 ari:0.16813582558567952 nmi:0.41012525141173756\n",
      "epoch:54 total_loss:0.050778016448020935 ari:0.16813582558567952 nmi:0.41012525141173756\n",
      "epoch:55 total_loss:0.050265245139598846 ari:0.16813582558567952 nmi:0.41012525141173756\n",
      "epoch:56 total_loss:0.052398260682821274 ari:0.1695853793703745 nmi:0.4117172628107484\n",
      "epoch:57 total_loss:0.05110730975866318 ari:0.1695853793703745 nmi:0.4117172628107484\n",
      "epoch:58 total_loss:0.05085224658250809 ari:0.1695853793703745 nmi:0.4117172628107484\n",
      "epoch:59 total_loss:0.050140492618083954 ari:0.17115103311898913 nmi:0.4131932032349126\n",
      "epoch:60 total_loss:0.05958596244454384 ari:0.17115103311898913 nmi:0.4131932032349126\n",
      "epoch:61 total_loss:0.054864902049303055 ari:0.17115103311898913 nmi:0.4131932032349126\n",
      "epoch:62 total_loss:0.051508303731679916 ari:0.17115103311898913 nmi:0.4131932032349126\n",
      "epoch:63 total_loss:0.05005618929862976 ari:0.17115103311898913 nmi:0.4131932032349126\n",
      "epoch:64 total_loss:0.052441030740737915 ari:0.17115103311898913 nmi:0.4131932032349126\n",
      "epoch:65 total_loss:0.04968811571598053 ari:0.17115103311898913 nmi:0.4131932032349126\n",
      "epoch:66 total_loss:0.05418990179896355 ari:0.17115103311898913 nmi:0.4131932032349126\n",
      "epoch:67 total_loss:0.053619079291820526 ari:0.17115103311898913 nmi:0.4131932032349126\n",
      "epoch:68 total_loss:0.04902761057019234 ari:0.17115103311898913 nmi:0.4131932032349126\n",
      "epoch:69 total_loss:0.04721949249505997 ari:0.17115103311898913 nmi:0.4131932032349126\n",
      "epoch:70 total_loss:0.04509694501757622 ari:0.17115103311898913 nmi:0.4131932032349126\n",
      "epoch:71 total_loss:0.04364626854658127 ari:0.17115103311898913 nmi:0.4131932032349126\n",
      "epoch:72 total_loss:0.04202718660235405 ari:0.18641784822988425 nmi:0.42791454325487965\n",
      "epoch:73 total_loss:0.04380884766578674 ari:0.18641784822988425 nmi:0.42791454325487965\n",
      "epoch:74 total_loss:0.05352224037051201 ari:0.18641784822988425 nmi:0.42791454325487965\n",
      "epoch:75 total_loss:0.05949285998940468 ari:0.18641784822988425 nmi:0.42791454325487965\n",
      "epoch:76 total_loss:0.05388790741562843 ari:0.18641784822988425 nmi:0.42791454325487965\n",
      "epoch:77 total_loss:0.05031564086675644 ari:0.18641784822988425 nmi:0.42791454325487965\n",
      "epoch:78 total_loss:0.049639441072940826 ari:0.18641784822988425 nmi:0.42791454325487965\n",
      "epoch:79 total_loss:0.04944080114364624 ari:0.18641784822988425 nmi:0.42791454325487965\n",
      "epoch:80 total_loss:0.04909578710794449 ari:0.18641784822988425 nmi:0.42791454325487965\n",
      "epoch:81 total_loss:0.04985448345541954 ari:0.18641784822988425 nmi:0.42791454325487965\n",
      "epoch:82 total_loss:0.04887790232896805 ari:0.18641784822988425 nmi:0.42791454325487965\n",
      "epoch:83 total_loss:0.04841698333621025 ari:0.18641784822988425 nmi:0.42791454325487965\n",
      "epoch:84 total_loss:0.04824665188789368 ari:0.18641784822988425 nmi:0.42791454325487965\n",
      "epoch:85 total_loss:0.04781188443303108 ari:0.18978289849873703 nmi:0.43123400785507243\n",
      "epoch:86 total_loss:0.04763137921690941 ari:0.18978289849873703 nmi:0.43123400785507243\n",
      "epoch:87 total_loss:0.04801156744360924 ari:0.18978289849873703 nmi:0.43123400785507243\n",
      "epoch:88 total_loss:0.049611739814281464 ari:0.18978289849873703 nmi:0.43123400785507243\n",
      "epoch:89 total_loss:0.04607373848557472 ari:0.18978289849873703 nmi:0.43123400785507243\n",
      "epoch:90 total_loss:0.04099227488040924 ari:0.19307238395189924 nmi:0.43738758715263143\n",
      "epoch:91 total_loss:0.04683103412389755 ari:0.20032198834830323 nmi:0.4394020182911538\n",
      "epoch:92 total_loss:0.048799172043800354 ari:0.2007141364282903 nmi:0.45785123606882006\n",
      "epoch:93 total_loss:0.04569702595472336 ari:0.2007141364282903 nmi:0.45785123606882006\n",
      "epoch:94 total_loss:0.04149907827377319 ari:0.2007141364282903 nmi:0.45785123606882006\n",
      "epoch:95 total_loss:0.042419321835041046 ari:0.2007141364282903 nmi:0.45785123606882006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:96 total_loss:0.046766847372055054 ari:0.2007141364282903 nmi:0.45785123606882006\n",
      "epoch:97 total_loss:0.04572204872965813 ari:0.2007141364282903 nmi:0.45785123606882006\n",
      "epoch:98 total_loss:0.04616906866431236 ari:0.2007141364282903 nmi:0.45785123606882006\n",
      "epoch:99 total_loss:0.05821797251701355 ari:0.2007141364282903 nmi:0.45785123606882006\n",
      "epoch:100 total_loss:0.062024082988500595 ari:0.2007141364282903 nmi:0.45785123606882006\n",
      "epoch:101 total_loss:0.04951942712068558 ari:0.2007141364282903 nmi:0.45785123606882006\n",
      "epoch:102 total_loss:0.045553192496299744 ari:0.2007141364282903 nmi:0.45785123606882006\n",
      "epoch:103 total_loss:0.042643703520298004 ari:0.2007141364282903 nmi:0.45785123606882006\n",
      "epoch:104 total_loss:0.039803870022296906 ari:0.2007141364282903 nmi:0.45785123606882006\n",
      "epoch:105 total_loss:0.03604646027088165 ari:0.2007141364282903 nmi:0.45785123606882006\n",
      "epoch:106 total_loss:0.03486742824316025 ari:0.2007141364282903 nmi:0.45785123606882006\n",
      "epoch:107 total_loss:0.036284055560827255 ari:0.2007141364282903 nmi:0.45785123606882006\n",
      "epoch:108 total_loss:0.040398720651865005 ari:0.2007141364282903 nmi:0.45785123606882006\n",
      "epoch:109 total_loss:0.04715069383382797 ari:0.2007141364282903 nmi:0.45785123606882006\n",
      "epoch:110 total_loss:0.04179561510682106 ari:0.21228398523822659 nmi:0.46955629154666967\n",
      "epoch:111 total_loss:0.036545634269714355 ari:0.21228398523822659 nmi:0.46955629154666967\n",
      "epoch:112 total_loss:0.03515002503991127 ari:0.21228398523822659 nmi:0.46955629154666967\n",
      "epoch:113 total_loss:0.03475095331668854 ari:0.21228398523822659 nmi:0.46955629154666967\n",
      "epoch:114 total_loss:0.03546888381242752 ari:0.21228398523822659 nmi:0.46955629154666967\n",
      "epoch:115 total_loss:0.03518570587038994 ari:0.21228398523822659 nmi:0.46955629154666967\n",
      "epoch:116 total_loss:0.03519191965460777 ari:0.21228398523822659 nmi:0.46955629154666967\n",
      "epoch:117 total_loss:0.03641386702656746 ari:0.21228398523822659 nmi:0.46955629154666967\n",
      "epoch:118 total_loss:0.0352088026702404 ari:0.21228398523822659 nmi:0.46955629154666967\n",
      "epoch:119 total_loss:0.035040292888879776 ari:0.22032280140803792 nmi:0.4799636928136147\n",
      "epoch:120 total_loss:0.036430783569812775 ari:0.22032280140803792 nmi:0.4799636928136147\n",
      "epoch:121 total_loss:0.03662073239684105 ari:0.22032280140803792 nmi:0.4799636928136147\n",
      "epoch:122 total_loss:0.03932690992951393 ari:0.22032280140803792 nmi:0.4799636928136147\n",
      "epoch:123 total_loss:0.040804870426654816 ari:0.22032280140803792 nmi:0.4799636928136147\n",
      "epoch:124 total_loss:0.03584877401590347 ari:0.22032280140803792 nmi:0.4799636928136147\n",
      "epoch:125 total_loss:0.03313962370157242 ari:0.22032280140803792 nmi:0.4799636928136147\n",
      "epoch:126 total_loss:0.03331390395760536 ari:0.22032280140803792 nmi:0.4799636928136147\n",
      "epoch:127 total_loss:0.03312433883547783 ari:0.22032280140803792 nmi:0.4799636928136147\n",
      "epoch:128 total_loss:0.03380785509943962 ari:0.22418098601553002 nmi:0.4870539049143079\n",
      "epoch:129 total_loss:0.03386359289288521 ari:0.22418098601553002 nmi:0.4870539049143079\n",
      "epoch:130 total_loss:0.03462119773030281 ari:0.22418098601553002 nmi:0.4870539049143079\n",
      "epoch:131 total_loss:0.03600329905748367 ari:0.22418098601553002 nmi:0.4870539049143079\n",
      "epoch:132 total_loss:0.03533042222261429 ari:0.22418098601553002 nmi:0.4870539049143079\n",
      "epoch:133 total_loss:0.033541489392519 ari:0.22418098601553002 nmi:0.4870539049143079\n",
      "epoch:134 total_loss:0.032465096563100815 ari:0.22418098601553002 nmi:0.4870539049143079\n",
      "epoch:135 total_loss:0.031765829771757126 ari:0.22418098601553002 nmi:0.4870539049143079\n",
      "epoch:136 total_loss:0.031778983771800995 ari:0.22418098601553002 nmi:0.4870539049143079\n",
      "epoch:137 total_loss:0.031919680535793304 ari:0.22418098601553002 nmi:0.4870539049143079\n",
      "epoch:138 total_loss:0.03199773281812668 ari:0.22418098601553002 nmi:0.4870539049143079\n",
      "epoch:139 total_loss:0.032464370131492615 ari:0.22418098601553002 nmi:0.4870539049143079\n",
      "epoch:140 total_loss:0.032420288771390915 ari:0.23046772217754105 nmi:0.49095344147187575\n",
      "epoch:141 total_loss:0.034424688667058945 ari:0.23046772217754105 nmi:0.49095344147187575\n",
      "epoch:142 total_loss:0.03405146673321724 ari:0.23046772217754105 nmi:0.49095344147187575\n",
      "epoch:143 total_loss:0.03354271873831749 ari:0.24038612302595957 nmi:0.5090613720234579\n",
      "epoch:144 total_loss:0.033457111567258835 ari:0.24038612302595957 nmi:0.5090613720234579\n",
      "epoch:145 total_loss:0.03558139503002167 ari:0.24038612302595957 nmi:0.5090613720234579\n",
      "epoch:146 total_loss:0.033516187220811844 ari:0.24038612302595957 nmi:0.5090613720234579\n",
      "epoch:147 total_loss:0.031639423221349716 ari:0.24038612302595957 nmi:0.5090613720234579\n",
      "epoch:148 total_loss:0.0315181128680706 ari:0.24038612302595957 nmi:0.5090613720234579\n",
      "epoch:149 total_loss:0.03192758187651634 ari:0.24038612302595957 nmi:0.5090613720234579\n",
      "epoch:150 total_loss:0.034629158675670624 ari:0.24038612302595957 nmi:0.5090613720234579\n",
      "epoch:151 total_loss:0.03663098067045212 ari:0.24038612302595957 nmi:0.5090613720234579\n",
      "epoch:152 total_loss:0.035266004502773285 ari:0.24038612302595957 nmi:0.5090613720234579\n",
      "epoch:153 total_loss:0.03289547190070152 ari:0.24038612302595957 nmi:0.5090613720234579\n",
      "epoch:154 total_loss:0.03121616132557392 ari:0.24038612302595957 nmi:0.5090613720234579\n",
      "epoch:155 total_loss:0.03119026869535446 ari:0.24038612302595957 nmi:0.5090613720234579\n",
      "epoch:156 total_loss:0.030397949740290642 ari:0.24038612302595957 nmi:0.5090613720234579\n",
      "epoch:157 total_loss:0.030154580250382423 ari:0.24038612302595957 nmi:0.5090613720234579\n",
      "epoch:158 total_loss:0.030484985560178757 ari:0.24038612302595957 nmi:0.5090613720234579\n",
      "epoch:159 total_loss:0.03073889948427677 ari:0.2440249630691315 nmi:0.5146254319368342\n",
      "epoch:160 total_loss:0.03096085973083973 ari:0.2440249630691315 nmi:0.5146254319368342\n",
      "epoch:161 total_loss:0.031252920627593994 ari:0.2440249630691315 nmi:0.5146254319368342\n",
      "epoch:162 total_loss:0.03123517334461212 ari:0.2642884894982944 nmi:0.5269987072502137\n",
      "epoch:163 total_loss:0.03187836334109306 ari:0.2744083812754055 nmi:0.5360891417999423\n",
      "epoch:164 total_loss:0.031504467129707336 ari:0.2744083812754055 nmi:0.5360891417999423\n",
      "epoch:165 total_loss:0.034272149205207825 ari:0.2744083812754055 nmi:0.5360891417999423\n",
      "epoch:166 total_loss:0.030669592320919037 ari:0.277746566159949 nmi:0.5385734675070969\n",
      "epoch:167 total_loss:0.030903872102499008 ari:0.277746566159949 nmi:0.5385734675070969\n",
      "epoch:168 total_loss:0.029579581692814827 ari:0.277746566159949 nmi:0.5385734675070969\n",
      "epoch:169 total_loss:0.030406404286623 ari:0.277746566159949 nmi:0.5385734675070969\n",
      "epoch:170 total_loss:0.030367637053132057 ari:0.277746566159949 nmi:0.5385734675070969\n",
      "epoch:171 total_loss:0.03063708171248436 ari:0.277746566159949 nmi:0.5385734675070969\n",
      "epoch:172 total_loss:0.03193368390202522 ari:0.277746566159949 nmi:0.5385734675070969\n",
      "epoch:173 total_loss:0.031046394258737564 ari:0.277746566159949 nmi:0.5385734675070969\n",
      "epoch:174 total_loss:0.031554922461509705 ari:0.277746566159949 nmi:0.5385734675070969\n",
      "epoch:175 total_loss:0.029493961483240128 ari:0.277746566159949 nmi:0.5385734675070969\n",
      "epoch:176 total_loss:0.028447598218917847 ari:0.277746566159949 nmi:0.5385734675070969\n",
      "epoch:177 total_loss:0.02880466915667057 ari:0.277746566159949 nmi:0.5385734675070969\n",
      "epoch:178 total_loss:0.028207719326019287 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:179 total_loss:0.02814880944788456 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:180 total_loss:0.027775565162301064 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:181 total_loss:0.02804562821984291 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:182 total_loss:0.027847597375512123 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:183 total_loss:0.027486924082040787 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:184 total_loss:0.028124215081334114 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:185 total_loss:0.02793167717754841 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:186 total_loss:0.02739160880446434 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:187 total_loss:0.02787090092897415 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:188 total_loss:0.02713175117969513 ari:0.29086062247198063 nmi:0.5488297512251205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:189 total_loss:0.026857364922761917 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:190 total_loss:0.02648441307246685 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:191 total_loss:0.026217784732580185 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:192 total_loss:0.02613312005996704 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:193 total_loss:0.026132192462682724 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:194 total_loss:0.026009725406765938 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:195 total_loss:0.026218293234705925 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:196 total_loss:0.02689262293279171 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:197 total_loss:0.026413673534989357 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:198 total_loss:0.026313144713640213 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:199 total_loss:0.0257274117320776 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:200 total_loss:0.026792103424668312 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:201 total_loss:0.028982169926166534 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:202 total_loss:0.030796634033322334 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:203 total_loss:0.026055626571178436 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:204 total_loss:0.02567453496158123 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:205 total_loss:0.025960782542824745 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:206 total_loss:0.027845583856105804 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:207 total_loss:0.027319898828864098 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:208 total_loss:0.03124055452644825 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:209 total_loss:0.026237592101097107 ari:0.29086062247198063 nmi:0.5488297512251205\n",
      "epoch:210 total_loss:0.027477070689201355 ari:0.2965360690781216 nmi:0.5630110145600448\n",
      "epoch:211 total_loss:0.026921071112155914 ari:0.2965360690781216 nmi:0.5630110145600448\n",
      "epoch:212 total_loss:0.02557854913175106 ari:0.2965360690781216 nmi:0.5630110145600448\n",
      "epoch:213 total_loss:0.02491418831050396 ari:0.2965360690781216 nmi:0.5630110145600448\n",
      "epoch:214 total_loss:0.024907059967517853 ari:0.2965360690781216 nmi:0.5630110145600448\n",
      "epoch:215 total_loss:0.02456154115498066 ari:0.2965360690781216 nmi:0.5630110145600448\n",
      "epoch:216 total_loss:0.024825023487210274 ari:0.2965360690781216 nmi:0.5630110145600448\n",
      "epoch:217 total_loss:0.026185544207692146 ari:0.2965360690781216 nmi:0.5630110145600448\n",
      "epoch:218 total_loss:0.026432637125253677 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:219 total_loss:0.026491573080420494 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:220 total_loss:0.025430787354707718 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:221 total_loss:0.02648373506963253 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:222 total_loss:0.027548471465706825 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:223 total_loss:0.026674732565879822 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:224 total_loss:0.025057882070541382 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:225 total_loss:0.025347575545310974 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:226 total_loss:0.024782847613096237 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:227 total_loss:0.025217924267053604 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:228 total_loss:0.02591758593916893 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:229 total_loss:0.027397409081459045 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:230 total_loss:0.026910854503512383 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:231 total_loss:0.026460617780685425 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:232 total_loss:0.026416456326842308 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:233 total_loss:0.028383130207657814 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:234 total_loss:0.02709863893687725 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:235 total_loss:0.025363337248563766 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:236 total_loss:0.025248127058148384 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:237 total_loss:0.02635038271546364 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:238 total_loss:0.02448432706296444 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:239 total_loss:0.024647407233715057 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:240 total_loss:0.024283606559038162 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:241 total_loss:0.024360937997698784 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:242 total_loss:0.024140747264027596 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:243 total_loss:0.024354202672839165 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:244 total_loss:0.025331847369670868 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:245 total_loss:0.025604451075196266 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:246 total_loss:0.024931982159614563 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:247 total_loss:0.025746209546923637 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:248 total_loss:0.024744953960180283 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:249 total_loss:0.02550792135298252 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:250 total_loss:0.024671645835042 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:251 total_loss:0.023938534781336784 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:252 total_loss:0.024120992049574852 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:253 total_loss:0.023686878383159637 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:254 total_loss:0.02361856959760189 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:255 total_loss:0.023512670770287514 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:256 total_loss:0.023515593260526657 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:257 total_loss:0.023473165929317474 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:258 total_loss:0.023351730778813362 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:259 total_loss:0.023425351828336716 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:260 total_loss:0.023404382169246674 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:261 total_loss:0.023404408246278763 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:262 total_loss:0.023509537801146507 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:263 total_loss:0.023288743570446968 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:264 total_loss:0.023408368229866028 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:265 total_loss:0.02375738136470318 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:266 total_loss:0.023572158068418503 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:267 total_loss:0.02396712265908718 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:268 total_loss:0.02414976805448532 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:269 total_loss:0.024325545877218246 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:270 total_loss:0.024721143767237663 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:271 total_loss:0.02760540321469307 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:272 total_loss:0.03141656517982483 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:273 total_loss:0.02894449606537819 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:274 total_loss:0.025288080796599388 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:275 total_loss:0.024145349860191345 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:276 total_loss:0.02416393905878067 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:277 total_loss:0.024104908108711243 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:278 total_loss:0.026485208421945572 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:279 total_loss:0.024658260866999626 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:280 total_loss:0.024602701887488365 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:281 total_loss:0.023626690730452538 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:282 total_loss:0.02444252371788025 ari:0.3112921929585136 nmi:0.5794293557866982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:283 total_loss:0.023505432531237602 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:284 total_loss:0.023157795891165733 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:285 total_loss:0.023089606314897537 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:286 total_loss:0.023495810106396675 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:287 total_loss:0.023581087589263916 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:288 total_loss:0.023626849055290222 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:289 total_loss:0.022976554930210114 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:290 total_loss:0.023284701630473137 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:291 total_loss:0.02346745692193508 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:292 total_loss:0.023128047585487366 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:293 total_loss:0.022786790505051613 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:294 total_loss:0.02284906432032585 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:295 total_loss:0.023612724617123604 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:296 total_loss:0.02409319393336773 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:297 total_loss:0.0238846093416214 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:298 total_loss:0.02426593005657196 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:299 total_loss:0.023721667006611824 ari:0.3112921929585136 nmi:0.5794293557866982\n",
      "epoch:300 total_loss:0.022630766034126282 ari:0.3112921929585136 nmi:0.5794293557866982\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(AE_epochs):\n",
    "    \n",
    "    total_loss = 0\n",
    "    for batch_idx,X_batch in enumerate(AE_dataloader):\n",
    "        X_batch = X_batch.to(device).float()\n",
    "        \n",
    "        AE_optimizer.zero_grad()\n",
    "\n",
    "        x_e, z_e = AE(X_batch)\n",
    "\n",
    "        loss = AE.loss(X_batch.float(), x_e.float())\n",
    "        \n",
    "        total_loss += loss\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        AE_optimizer.step()\n",
    "        AE_scheduler.step()\n",
    "\n",
    "    ari,nmi = cal_result_AE(AE,AE_dataloader,clusters_true,gene_matrix)\n",
    "    \n",
    "    if best_ari < ari and best_nmi < nmi:\n",
    "        \n",
    "        best_ari = ari\n",
    "        \n",
    "        best_nmi = nmi\n",
    "        \n",
    "    print(f\"epoch:{epoch+1} total_loss:{total_loss/len(AE_dataloader.sampler)} ari:{best_ari} nmi:{best_nmi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "171d1cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "AE.to(torch.device(\"cpu\")).eval()\n",
    "z_init,_ = AE.encoder(torch.from_numpy(gene_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "574040ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_init = z_init.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08e7e874",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.AnnData(X=z_init)\n",
    "adata.obsm['X_unifan'] = z_init\n",
    "sc.pp.neighbors(adata, n_pcs=32,use_rep='X_unifan', random_state=AE_random_seed)\n",
    "sc.tl.leiden(adata, resolution=1, random_state=AE_random_seed)\n",
    "clusters_pre = adata.obs['leiden'].astype('int').values  # original as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "602bc1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster = pd.DataFrame(z_init)\n",
    "cluster_labels = np.unique(clusters_pre)\n",
    "M = len(set(cluster_labels))  # set as number of clusters\n",
    "df_cluster['cluster'] = clusters_pre\n",
    "\n",
    "# get centroids\n",
    "centroids = df_cluster.groupby('cluster').mean().values\n",
    "centroids_torch = torch.from_numpy(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35f25e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = torch.from_numpy(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "851a69a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_smaller = adjusted_rand_score(clusters_pre,\n",
    "                                  clusters_true)\n",
    "nmi_smaller = adjusted_mutual_info_score(clusters_pre, clusters_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2bc5595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5750889813077029"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmi_smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63bc8510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30627858255204143"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ari_smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d9be3b",
   "metadata": {},
   "source": [
    "# VQVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f43a9a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE_T(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim: int = 10000, z_dim: int = 32,\n",
    "                  encoder_dim: int = 128, emission_dim: int = 128,\n",
    "                 num_layers_encoder: int = 2,num_layers_decoder: int = 2,\n",
    "                 beta: float = 1.0,gama:float =0.25,\n",
    "                 n_clusters: int = 16,\n",
    "                 hidden_dim: int = 128, dropout_rate: float = 0.1, use_t_dist: bool = True,\n",
    "                 centroids: torch.Tensor = None):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # initialize parameters\n",
    "        self.z_dim = z_dim\n",
    "        self.beta = beta\n",
    "        self.gama = gama\n",
    "        self.n_clusters = n_clusters\n",
    "        self.use_t_dist = use_t_dist\n",
    "        \n",
    "        # initialize centroids embeddings\n",
    "        if centroids is not None:\n",
    "            self.embeddings = nn.Parameter(centroids, requires_grad=True)\n",
    "        else:\n",
    "            self.embeddings = nn.Parameter(torch.randn(self.n_clusters, self.z_dim) * 0.05, requires_grad=True)\n",
    "\n",
    "        # initialize loss\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "        self.encoder = Encoder(input_dim, z_dim, num_layers=num_layers_encoder, hidden_dim=encoder_dim,\n",
    "                                   dropout_rate=dropout_rate)\n",
    "        \n",
    "        self.decoder = Decoder(z_dim, input_dim, num_layers=num_layers_decoder,hidden_dim=emission_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # get encoding\n",
    "        z_e, _ = self.encoder(x)\n",
    "\n",
    "        # get the index of embedding closed to the encoding\n",
    "        k, z_dist, dist_prob = self._get_clusters(z_e)\n",
    "\n",
    "        # get embeddings (discrete representations)\n",
    "        z_q = self._get_embeddings(k)\n",
    "\n",
    "        # decode embedding (discrete representation) and encoding\n",
    "        x_q, _ = self.decoder(z_e + (z_q-z_e).detach())\n",
    "\n",
    "        return x_q, z_e, z_q\n",
    "\n",
    "    def _get_clusters(self, z_e):\n",
    "\n",
    "\n",
    "        _z_dist = (z_e.unsqueeze(1) - self.embeddings.unsqueeze(0)) ** 2\n",
    "        z_dist = torch.sum(_z_dist, dim=-1)\n",
    "        if self.use_t_dist:\n",
    "            dist_prob = self._t_dist_sim(z_dist, df=10)\n",
    "            k = torch.argmax(dist_prob, dim=-1)\n",
    "        else:\n",
    "            k = torch.argmin(z_dist, dim=-1)\n",
    "            dist_prob = None\n",
    "\n",
    "        return k, z_dist, dist_prob\n",
    "\n",
    "    def _t_dist_sim(self, z_dist, df=10):\n",
    "\n",
    "\n",
    "        _factor = - ((df + 1) / 2)\n",
    "        dist_prob = torch.pow((1 + z_dist / df), _factor)\n",
    "        dist_prob = dist_prob / dist_prob.sum(axis=1).unsqueeze(1)\n",
    "\n",
    "        return dist_prob\n",
    "\n",
    "    def _get_embeddings(self, k):\n",
    "\n",
    "\n",
    "        k = k.long()\n",
    "        _z_q = []\n",
    "        for i in range(len(k)):\n",
    "            _z_q.append(self.embeddings[k[i]])\n",
    "\n",
    "        z_q = torch.stack(_z_q)\n",
    "\n",
    "        return z_q\n",
    "\n",
    "\n",
    "    def _loss_reconstruct(self,x,x_q, z_e, z_q):\n",
    "\n",
    "\n",
    "        l_x = self.mse_loss(x, x_q)\n",
    "        \n",
    "        l_q = self.mse_loss(z_e.detach(),z_q)\n",
    "        \n",
    "        l_e = self.mse_loss(z_e,z_q.detach())\n",
    "        \n",
    "        mse_l = l_e + self.beta*l_q + self.gama*l_x\n",
    "        \n",
    "        return mse_l\n",
    "\n",
    "\n",
    "    def loss(self,x ,x_e ,z_e ,z_q):\n",
    "\n",
    "        mse_l = self._loss_reconstruct(x,x_e,z_e,z_q)\n",
    "\n",
    "        return mse_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c9bdfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE_N(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim: int = 10000, z_dim: int = 32,\n",
    "                  encoder_dim: int = 128, emission_dim: int = 128,\n",
    "                 num_layers_encoder: int = 2,num_layers_decoder: int = 2,\n",
    "                 beta: float = 1.0,gama:float =0.25,\n",
    "                 n_clusters: int = 16,\n",
    "                 hidden_dim: int = 128, dropout_rate: float = 0.1, use_t_dist: bool = False,\n",
    "                 centroids: torch.Tensor = None):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # initialize parameters\n",
    "        self.z_dim = z_dim\n",
    "        self.beta = beta\n",
    "        self.gama = gama\n",
    "        self.n_clusters = n_clusters\n",
    "        self.use_t_dist = use_t_dist\n",
    "        \n",
    "        # initialize centroids embeddings\n",
    "        if centroids is not None:\n",
    "            self.embeddings = nn.Parameter(centroids, requires_grad=True)\n",
    "        else:\n",
    "            self.embeddings = nn.Parameter(torch.randn(self.n_clusters, self.z_dim) * 0.05, requires_grad=True)\n",
    "\n",
    "        # initialize loss\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "        self.encoder = Encoder(input_dim, z_dim, num_layers=num_layers_encoder, hidden_dim=encoder_dim,\n",
    "                                   dropout_rate=dropout_rate)\n",
    "        \n",
    "        self.decoder = Decoder(z_dim, input_dim, num_layers=num_layers_decoder,hidden_dim=emission_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # get encoding\n",
    "        z_e, _ = self.encoder(x)\n",
    "\n",
    "        # get the index of embedding closed to the encoding\n",
    "        k, z_dist, dist_prob = self._get_clusters(z_e)\n",
    "\n",
    "        # get embeddings (discrete representations)\n",
    "        z_q = self._get_embeddings(k)\n",
    "\n",
    "        # decode embedding (discrete representation) and encoding\n",
    "        x_q, _ = self.decoder(z_e + (z_q-z_e).detach())\n",
    "\n",
    "        return x_q, z_e, z_q\n",
    "\n",
    "    def _get_clusters(self, z_e):\n",
    "\n",
    "\n",
    "        _z_dist = (z_e.unsqueeze(1) - self.embeddings.unsqueeze(0)) ** 2\n",
    "        z_dist = torch.sum(_z_dist, dim=-1)\n",
    "        if self.use_t_dist:\n",
    "            dist_prob = self._t_dist_sim(z_dist, df=10)\n",
    "            k = torch.argmax(dist_prob, dim=-1)\n",
    "        else:\n",
    "            k = torch.argmin(z_dist, dim=-1)\n",
    "            dist_prob = None\n",
    "\n",
    "        return k, z_dist, dist_prob\n",
    "\n",
    "    def _t_dist_sim(self, z_dist, df=10):\n",
    "\n",
    "\n",
    "        _factor = - ((df + 1) / 2)\n",
    "        dist_prob = torch.pow((1 + z_dist / df), _factor)\n",
    "        dist_prob = dist_prob / dist_prob.sum(axis=1).unsqueeze(1)\n",
    "\n",
    "        return dist_prob\n",
    "\n",
    "    def _get_embeddings(self, k):\n",
    "\n",
    "\n",
    "        k = k.long()\n",
    "        _z_q = []\n",
    "        for i in range(len(k)):\n",
    "            _z_q.append(self.embeddings[k[i]])\n",
    "\n",
    "        z_q = torch.stack(_z_q)\n",
    "\n",
    "        return z_q\n",
    "\n",
    "\n",
    "    def _loss_reconstruct(self,x,x_q, z_e, z_q):\n",
    "\n",
    "\n",
    "        l_x = self.mse_loss(x, x_q)\n",
    "        \n",
    "        l_q = self.mse_loss(z_e.detach(),z_q)\n",
    "        \n",
    "        l_e = self.mse_loss(z_e,z_q.detach())\n",
    "        \n",
    "        mse_l = l_e + self.beta*l_q + self.gama*l_x\n",
    "        \n",
    "        return mse_l\n",
    "\n",
    "\n",
    "    def loss(self,x ,x_e ,z_e ,z_q):\n",
    "\n",
    "        mse_l = self._loss_reconstruct(x,x_e,z_e,z_q)\n",
    "\n",
    "        return mse_l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be158f0",
   "metadata": {},
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8bba4d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adam_training(model,n_epochs,learning_rate,dataloader,clusters_true,gene_matrix,\n",
    "                non_blocking = True,decay_factor = 0.9):\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1000, decay_factor)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        total_loss = 0\n",
    "    \n",
    "        for batch_idx, X_batch in enumerate(dataloader):\n",
    "            \n",
    "            X_batch = X_batch.to(device, non_blocking=non_blocking).float()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            x_q, z_e, z_q = model(X_batch)\n",
    "\n",
    "            l = model.loss(X_batch,x_q,z_e,z_q)\n",
    "       \n",
    "            total_loss += l\n",
    "\n",
    "            l.backward(retain_graph=True)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "        ari,nmi = cal_result(model,dataloader,clusters_true,gene_matrix)\n",
    "        global best_ari,best_nmi\n",
    "        if best_ari < ari and best_nmi < nmi:\n",
    "\n",
    "            best_ari = ari\n",
    "\n",
    "            best_nmi = nmi\n",
    "        \n",
    "        print(\"------------------------------\")\n",
    "        print(f\"epoch:{epoch+1} total_loss:{total_loss/len(dataloader)} \\n ari:{ari.item()} nmi:{nmi.item()} \\n best ari:{best_ari.item()} bset nmi:{best_nmi.item()}\")\n",
    "        \n",
    "            \n",
    "    # move network back to cpu and return\n",
    "    model.cpu()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df200708",
   "metadata": {},
   "source": [
    "# GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64e10e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossover_and_mutation(parents, sigma=0.05):\n",
    "\n",
    "    \n",
    "    base_sd = parents[0].state_dict()\n",
    "    keys = base_sd                    # use all layers to be affected\n",
    "    \n",
    "    # Sum of the weights of the parent\n",
    "    for i in range(1, len(parents)):\n",
    "        parent_sd = parents[i].state_dict()\n",
    "        for key in keys:\n",
    "            base_sd[key] = base_sd[key] + parent_sd[key]\n",
    "            \n",
    "    \n",
    "    # Average and add mutation\n",
    "    num_parents = len(parents)\n",
    "    \n",
    "    for key in keys:\n",
    "        \n",
    "        tensor_size = base_sd[key].size()\n",
    "        random_tensor = torch.normal(mean=0.0, std=sigma, size=tensor_size).to(device)\n",
    "        \n",
    "        base_sd[key] = (base_sd[key] / num_parents) + random_tensor\n",
    "    \n",
    "    # create offspring\n",
    "    \n",
    "    if random.randint(0,1) == 0:\n",
    "        offspring = VQVAE_T(input_dim = gene_matrix.shape[1],\n",
    "                            beta = 1,gama=0.25, n_clusters = centroids.shape[0])\n",
    "    \n",
    "        offspring.load_state_dict(base_sd)\n",
    "    else:\n",
    "    \n",
    "        offspring = VQVAE_N(input_dim = gene_matrix.shape[1],\n",
    "                            beta = 1,gama=0.25, n_clusters = centroids.shape[0])\n",
    "    \n",
    "        offspring.load_state_dict(base_sd)\n",
    "        \n",
    "    return offspring\n",
    "    \n",
    "\n",
    "def create_offspring(population,fitness,rho,sigma):\n",
    "\n",
    "    \n",
    "    # Perform selection\n",
    "    parents = random.choices(population, weights=fitness, k=rho) \n",
    "    \n",
    "    # Perform crossover and mutation\n",
    "    offspring = crossover_and_mutation(parents, sigma)\n",
    "    \n",
    "    \n",
    "    return offspring\n",
    "\n",
    "def sigmoid(x):\n",
    "    \n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def GA_training(population, pop_size, offspring_size, elitist_level, rho, sigma, dataloader,clusters_true,gene_matrix):\n",
    "    \n",
    "    #Calculate fitness of trained population\n",
    "\n",
    "    fitness = [cal_result(population[i],dataloader,clusters_true,gene_matrix)[0]+cal_result(population[i],dataloader,clusters_true,gene_matrix)[1] \n",
    "                                for i in range(pop_size)]\n",
    "    \n",
    "    print(f\"--- -- Finished fitness evaluation, length: {len(fitness)}\")\n",
    "    \n",
    "    #Create offspring population\n",
    "    fitness_weighted = [f for f in fitness]   # take inverse of loss so lower losses get higher fitness-values\n",
    "    \n",
    "    offspring_population = [create_offspring(population,fitness_weighted, rho, sigma) for i in range(offspring_size)]\n",
    "    \n",
    "    print(\"--- -- Finished creating offspring population\")\n",
    "    \n",
    "    #Evaluate fitness of offsprings \n",
    "    \n",
    "    offspring_fitness = [cal_result(offspring_population[i],dataloader,clusters_true,gene_matrix)[0]+cal_result(offspring_population[i],dataloader,clusters_true,gene_matrix)[1] \n",
    "                                                          for i in range(offspring_size)]\n",
    "    \n",
    "    print(\"--- -- Finished evaluating fitness of offspring population\")\n",
    "    \n",
    "    # Combine fitness and population lists\n",
    "    \n",
    "    combined_fitness = fitness + offspring_fitness\n",
    "    combined_population = population + offspring_population\n",
    "    \n",
    "    # sort and select population by their fitness values\n",
    "    \n",
    "    sorted_population = [pop for _, pop in sorted(zip(combined_fitness, combined_population), key=lambda pair: pair[0])]\n",
    "    sorted_fitness = [loss for loss, _ in sorted(zip(combined_fitness, combined_population), key=lambda pair: pair[0])]\n",
    "    \n",
    "    m = int(pop_size * elitist_level)\n",
    "    new_population = sorted_population[0:m]\n",
    "    \n",
    "    # Fill up rest of population\n",
    "    difference = pop_size - m\n",
    "    remaining_population = list(set(sorted_population) - set(new_population))\n",
    "    filler_population = random.sample(remaining_population, difference)\n",
    "    \n",
    "    # assemble new population and return\n",
    "    new_population = new_population + filler_population\n",
    "    \n",
    "    return new_population, sorted_fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1bc59b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_loss(model,dataloader,non_blocking=True):\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_idx, X_batch in enumerate(dataloader):\n",
    "        \n",
    "        X_batch = X_batch.to(device, non_blocking=non_blocking).float()\n",
    "\n",
    "        x_e, z_e, z_q = model(X_batch)\n",
    "\n",
    "        l = model.loss(X_batch,x_e, z_e, z_q)\n",
    "        \n",
    "        total_loss += l\n",
    "        \n",
    "    model.cpu()\n",
    "\n",
    "    return float(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a6f9539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_result(model,dataloader,clusters_true,gene_matrix):\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    gene_matrix = torch.from_numpy(gene_matrix).to(device)\n",
    "\n",
    "    x_e,z_e,z_q = model(gene_matrix)\n",
    "    \n",
    "    z_e = z_e.detach().cpu().numpy()\n",
    "    \n",
    "    adata.obsm['X_unifan'] = z_e\n",
    "    \n",
    "    sc.pp.neighbors(adata, n_pcs=32,use_rep='X_unifan', random_state=123)\n",
    "    \n",
    "    sc.tl.leiden(adata, resolution=1, random_state=123)\n",
    "    \n",
    "    clusters_pre = adata.obs['leiden'].astype('int').values  # original as string\n",
    "    \n",
    "    ari = adjusted_rand_score(clusters_pre, clusters_true)\n",
    "\n",
    "    nmi = adjusted_mutual_info_score(clusters_pre, clusters_true)\n",
    "    \n",
    "    ari = torch.from_numpy(np.array(ari)).to(device)\n",
    "    nmi = torch.from_numpy(np.array(nmi)).to(device)\n",
    "    \n",
    "    return (ari,nmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70a2acb",
   "metadata": {},
   "source": [
    "# GA Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7d8796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GA_Neural_train(population,\n",
    "                    pop_size,\n",
    "                    max_generations, \n",
    "                    SGD_steps, GA_steps, \n",
    "                    offspring_size, elitist_level, rho,\n",
    "                    learning_rate,\n",
    "                    dataloader,\n",
    "                    clusters_true,\n",
    "                    gene_matrix):\n",
    "    \n",
    "    \n",
    "    print(f\"Starting with population of size: {pop_size}\")\n",
    "\n",
    "    for k in range(max_generations):\n",
    "        print(f\"Currently in generation {k+1}\")\n",
    "        \n",
    "        #Adam\n",
    "        print(f\"--- Starting Adam\")\n",
    "        \n",
    "        # Sequential version\n",
    "        \n",
    "        population_copy = []\n",
    "        \n",
    "        for i in range(pop_size):\n",
    "            \n",
    "            model = Adam_training(population[i],SGD_steps,learning_rate,dataloader,clusters_true,gene_matrix)\n",
    "            population_copy.append(model)\n",
    "        \n",
    "        print(f\"--- Finished Adam\")\n",
    "        \n",
    "        population = population_copy\n",
    "         \n",
    "        # GA\n",
    "        print(f\"--- Starting Model GA\")\n",
    "        GA_start = time.time()\n",
    "        sorted_fitness = []          # store the sorted fitness values to maybe use in data collection\n",
    "        for i in range(0, GA_steps):\n",
    "            \n",
    "            sigma = 0.01 / (k+1)\n",
    "            population, sorted_fitness = GA_training(population, \n",
    "                                                     pop_size, offspring_size, elitist_level, rho, sigma, dataloader,\n",
    "                                                     clusters_true,gene_matrix)\n",
    "        \n",
    "        GA_end = time.time()\n",
    "        \n",
    "        print(f\"--- Finished Model GA,Time:{(GA_end - GA_start) * 1000}ms\")\n",
    "        \n",
    "        \n",
    "    print(f\"Finished training process\")\n",
    "    \n",
    "    return population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2247f9a1",
   "metadata": {},
   "source": [
    "# Train VQVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f4367a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "#device_train = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 1024\n",
    "pop_size_T = 5\n",
    "pop_size_N = 5\n",
    "max_generations = 50\n",
    "SGD_steps = 10\n",
    "GA_steps = 1\n",
    "offspring_size = 30\n",
    "elitist_level = 0.4\n",
    "rho = 4\n",
    "learning_rate = 1e-5\n",
    "in_feats = 400\n",
    "n_hidden = 200\n",
    "weight_decay = 5e-4\n",
    "best_nmi = 0\n",
    "best_ari = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7094ceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_loader(gene_matrix,batch_size = batch_size)\n",
    "pop_size  = pop_size_T + pop_size_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7caf6397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create population and start training process\n",
    "population_T = [VQVAE_T(input_dim = gene_matrix.shape[1],\n",
    "                            beta = 1,gama=0.25, n_clusters = centroids.shape[0],centroids = centroids).to(device)\n",
    "                     for i in range(pop_size_T)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da257720",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_N = [VQVAE_N(input_dim = gene_matrix.shape[1],\n",
    "                            beta = 1,gama=0.25, n_clusters = centroids.shape[0],centroids = centroids).to(device)\n",
    "                     for i in range(pop_size_N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96319356",
   "metadata": {},
   "outputs": [],
   "source": [
    "population = population_T + population_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c158087",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with population of size: 10\n",
      "Currently in generation 1\n",
      "--- Starting Adam\n",
      "------------------------------\n",
      "epoch:1 total_loss:116.62741088867188 \n",
      " ari:0.27351531056082723 nmi:0.4398371786005851 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:2 total_loss:114.07743835449219 \n",
      " ari:0.19649953505169468 nmi:0.4161126552097777 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:3 total_loss:112.17552185058594 \n",
      " ari:0.24446408813638498 nmi:0.4234200971548962 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:4 total_loss:110.59776306152344 \n",
      " ari:0.25564905243911284 nmi:0.42014647978772546 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:5 total_loss:109.08436584472656 \n",
      " ari:0.2397818305482188 nmi:0.4210009006399322 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:6 total_loss:107.73296356201172 \n",
      " ari:0.24602123402809484 nmi:0.42822986473125146 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:7 total_loss:106.3287353515625 \n",
      " ari:0.2349649809890326 nmi:0.4056230677497483 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:8 total_loss:104.98818969726562 \n",
      " ari:0.2190974940548924 nmi:0.39826014227600476 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:9 total_loss:103.5980453491211 \n",
      " ari:0.22476357342989384 nmi:0.4162151676868472 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:10 total_loss:102.26361083984375 \n",
      " ari:0.17525453480923725 nmi:0.39274082748058126 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:1 total_loss:119.006591796875 \n",
      " ari:0.20968762601659377 nmi:0.3711155003175042 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:2 total_loss:116.56285095214844 \n",
      " ari:0.21530012068313664 nmi:0.38226859089855714 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:3 total_loss:114.88414001464844 \n",
      " ari:0.21860246699192049 nmi:0.38818798214465744 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:4 total_loss:113.4364242553711 \n",
      " ari:0.2026221588659306 nmi:0.3907022427229535 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:5 total_loss:111.92494201660156 \n",
      " ari:0.2043625259726228 nmi:0.3923008928027862 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:6 total_loss:110.55315399169922 \n",
      " ari:0.19345537302509624 nmi:0.3892651645426284 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:7 total_loss:109.12911987304688 \n",
      " ari:0.19499305101481448 nmi:0.3836633797165387 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:8 total_loss:107.69700622558594 \n",
      " ari:0.19301562289962532 nmi:0.38831920527469077 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:9 total_loss:106.26326751708984 \n",
      " ari:0.18592200176773552 nmi:0.3833784212486664 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:10 total_loss:104.72078704833984 \n",
      " ari:0.19244546730336443 nmi:0.3755489963299486 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:1 total_loss:119.90460205078125 \n",
      " ari:0.1874883554535885 nmi:0.3930296485913652 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:2 total_loss:117.20504760742188 \n",
      " ari:0.20652816235411944 nmi:0.3998066310062006 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:3 total_loss:114.96501159667969 \n",
      " ari:0.20130236492061107 nmi:0.3987040986101149 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:4 total_loss:112.72972106933594 \n",
      " ari:0.17348644732155702 nmi:0.3714521494628186 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:5 total_loss:110.63346862792969 \n",
      " ari:0.19656961813882567 nmi:0.3932364057143001 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:6 total_loss:108.49595642089844 \n",
      " ari:0.1652737334606972 nmi:0.37191524044516844 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:7 total_loss:106.40061950683594 \n",
      " ari:0.16576361514507615 nmi:0.3590989304745407 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:8 total_loss:104.43731689453125 \n",
      " ari:0.16813261872851387 nmi:0.36313156518315626 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:9 total_loss:102.326416015625 \n",
      " ari:0.15494491658460627 nmi:0.3505284821582659 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:10 total_loss:100.37483978271484 \n",
      " ari:0.17071834234476757 nmi:0.3498572324860959 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:1 total_loss:121.60771179199219 \n",
      " ari:0.21007492469003672 nmi:0.3797471167283071 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:2 total_loss:118.66389465332031 \n",
      " ari:0.2125052633691107 nmi:0.39769704674848366 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:3 total_loss:116.83102416992188 \n",
      " ari:0.2086729106263849 nmi:0.39871152156308576 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:4 total_loss:115.4033203125 \n",
      " ari:0.18626555608463993 nmi:0.3776274528538446 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:5 total_loss:114.125732421875 \n",
      " ari:0.1983675923747333 nmi:0.37543612572978197 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:6 total_loss:112.90414428710938 \n",
      " ari:0.17104955323906346 nmi:0.3629039220744808 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:7 total_loss:111.73377990722656 \n",
      " ari:0.1787895187021481 nmi:0.374389555389311 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:8 total_loss:110.6197738647461 \n",
      " ari:0.19346757330714845 nmi:0.3582495872931261 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:9 total_loss:109.50801849365234 \n",
      " ari:0.19717322536944462 nmi:0.36371927441967833 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:10 total_loss:108.37799072265625 \n",
      " ari:0.18654589465922267 nmi:0.362349869501404 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:1 total_loss:119.24598693847656 \n",
      " ari:0.17815977665811436 nmi:0.3687514262487915 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:2 total_loss:117.17460632324219 \n",
      " ari:0.20698402877131877 nmi:0.3945598822695818 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:3 total_loss:115.58401489257812 \n",
      " ari:0.19964021299276907 nmi:0.3697729649354875 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:4 total_loss:113.89874267578125 \n",
      " ari:0.17002573302121043 nmi:0.3678307023008879 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:5 total_loss:112.32862854003906 \n",
      " ari:0.1764123907234381 nmi:0.37842107084609244 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:6 total_loss:110.761962890625 \n",
      " ari:0.18642578961889103 nmi:0.3793697636398294 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "epoch:7 total_loss:109.08184051513672 \n",
      " ari:0.1611259018207618 nmi:0.3524989976413478 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:8 total_loss:107.55641174316406 \n",
      " ari:0.19751229646754775 nmi:0.3662451158929468 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:9 total_loss:105.89459228515625 \n",
      " ari:0.16111378438818824 nmi:0.34561020559336414 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:10 total_loss:104.30343627929688 \n",
      " ari:0.16300387244603642 nmi:0.34405635671716894 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:1 total_loss:114.5887680053711 \n",
      " ari:0.17667774236424766 nmi:0.38371788886947683 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:2 total_loss:112.44002532958984 \n",
      " ari:0.19300955128354774 nmi:0.40184527152263566 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:3 total_loss:110.66769409179688 \n",
      " ari:0.17804919125641233 nmi:0.38742869844127775 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:4 total_loss:108.91963195800781 \n",
      " ari:0.18892684055572134 nmi:0.3847466996828935 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:5 total_loss:107.26432800292969 \n",
      " ari:0.17933954781826447 nmi:0.37189884173853555 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:6 total_loss:105.51256561279297 \n",
      " ari:0.16916233591199134 nmi:0.3740093365780623 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:7 total_loss:103.735595703125 \n",
      " ari:0.15959916359251056 nmi:0.3641909333860797 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:8 total_loss:101.9536361694336 \n",
      " ari:0.1633284225256233 nmi:0.36124018465763497 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:9 total_loss:100.25802612304688 \n",
      " ari:0.16104544826361855 nmi:0.3653398768465175 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:10 total_loss:98.51420593261719 \n",
      " ari:0.15791374974973016 nmi:0.358804605315005 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:1 total_loss:117.57382202148438 \n",
      " ari:0.22724009816671312 nmi:0.4173386343420165 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:2 total_loss:115.35757446289062 \n",
      " ari:0.23637872470490937 nmi:0.4224755703290289 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:3 total_loss:113.667724609375 \n",
      " ari:0.21930151179124294 nmi:0.40378771853887874 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:4 total_loss:112.03608703613281 \n",
      " ari:0.18987189631688212 nmi:0.3802216882875314 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:5 total_loss:110.53093719482422 \n",
      " ari:0.1888643375137318 nmi:0.385206526834944 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:6 total_loss:109.01876068115234 \n",
      " ari:0.18479750295662267 nmi:0.3799431676466688 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:7 total_loss:107.50686645507812 \n",
      " ari:0.18048964060926714 nmi:0.3766935324692706 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:8 total_loss:106.03262329101562 \n",
      " ari:0.15875462206501298 nmi:0.35100544500091363 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:9 total_loss:104.58274841308594 \n",
      " ari:0.16491625899811196 nmi:0.36735524399897634 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:10 total_loss:103.13023376464844 \n",
      " ari:0.15837536881665484 nmi:0.3572283882843843 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:1 total_loss:125.5 \n",
      " ari:0.17507725058170556 nmi:0.35324623407831235 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:2 total_loss:122.73955535888672 \n",
      " ari:0.17435169508455003 nmi:0.3577776327651237 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:3 total_loss:120.86387634277344 \n",
      " ari:0.1853169122837953 nmi:0.36192046805950845 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:4 total_loss:119.1250228881836 \n",
      " ari:0.1636729127394991 nmi:0.34487280360191863 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:5 total_loss:117.48942565917969 \n",
      " ari:0.18215399434904472 nmi:0.3661961308733514 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:6 total_loss:115.90235900878906 \n",
      " ari:0.21090476368573688 nmi:0.3674758528873502 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:7 total_loss:114.42584228515625 \n",
      " ari:0.1853456227748113 nmi:0.34489019352692507 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:8 total_loss:112.87646484375 \n",
      " ari:0.1812869912079925 nmi:0.34588317445830774 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:9 total_loss:111.35807800292969 \n",
      " ari:0.19773995777250938 nmi:0.3558881027891851 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:10 total_loss:109.76736450195312 \n",
      " ari:0.19522321785113084 nmi:0.36127082114842196 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:1 total_loss:118.11708068847656 \n",
      " ari:0.19359678167702934 nmi:0.3734595084899768 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:2 total_loss:115.52468872070312 \n",
      " ari:0.1933344483225897 nmi:0.37230588912365953 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:3 total_loss:113.704833984375 \n",
      " ari:0.212513353556086 nmi:0.3793753029871268 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:4 total_loss:112.1124496459961 \n",
      " ari:0.19822078150743597 nmi:0.38371141020481025 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:5 total_loss:110.57585144042969 \n",
      " ari:0.23794180973348167 nmi:0.39858676592140524 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:6 total_loss:109.08270263671875 \n",
      " ari:0.18801884823067738 nmi:0.368680935006233 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:7 total_loss:107.68529510498047 \n",
      " ari:0.19976114091734357 nmi:0.3743267919031859 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:8 total_loss:106.40150451660156 \n",
      " ari:0.16432057639504513 nmi:0.361730101543704 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:9 total_loss:105.22136688232422 \n",
      " ari:0.17036948752363804 nmi:0.3505951494890757 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:10 total_loss:103.97527313232422 \n",
      " ari:0.1830922217492994 nmi:0.36058431332266516 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:1 total_loss:119.05565643310547 \n",
      " ari:0.19192165545284112 nmi:0.37490207047551755 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:2 total_loss:116.50518035888672 \n",
      " ari:0.192675046208543 nmi:0.38435036575065235 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n",
      "------------------------------\n",
      "epoch:3 total_loss:114.47964477539062 \n",
      " ari:0.20598141885511043 nmi:0.3904720257431172 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "epoch:4 total_loss:112.48332214355469 \n",
      " ari:0.1881445968085869 nmi:0.37580548435177913 \n",
      " best ari:0.27351531056082723 bset nmi:0.4398371786005851\n"
     ]
    }
   ],
   "source": [
    "Train_start = time.time()\n",
    "trained_population = GA_Neural_train(population=population,\n",
    "                                    pop_size = pop_size,\n",
    "                                    max_generations=max_generations,\n",
    "                                    SGD_steps=SGD_steps,GA_steps=GA_steps,\n",
    "                                    offspring_size=offspring_size,elitist_level=elitist_level,rho=rho,\n",
    "                                    learning_rate=learning_rate,\n",
    "                                    dataloader=dataloader,clusters_true=clusters_true,gene_matrix=gene_matrix)\n",
    "Train_end = time.time()\n",
    "print(f\"All Time:{(Train_end-Train_start)*1000}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9de88c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
