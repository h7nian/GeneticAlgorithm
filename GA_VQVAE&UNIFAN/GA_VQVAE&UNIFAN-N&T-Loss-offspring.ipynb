{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f045d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from torch.utils.data import random_split,Dataset,DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from joblib.externals.loky.backend.context import get_context\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import tarfile\n",
    "\n",
    "import time\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae930554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "from sklearn.metrics.cluster import adjusted_rand_score, adjusted_mutual_info_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a3d3a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unifan.networks import Encoder, Decoder, Set2Gene\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bb2e1a",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56b897c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n",
    "AE_batch_size=128\n",
    "AE_learning_rate = 0.001\n",
    "AE_decay_factor = 0.9\n",
    "AE_epochs = 300\n",
    "AE_random_seed = 123\n",
    "best_ari = 0\n",
    "best_nmi = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e014b22",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "281f9867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(adata):\n",
    "    \n",
    "    \n",
    "    sc.pp.filter_cells(adata, min_genes=20)\n",
    "    sc.pp.filter_genes(adata, min_cells=3)\n",
    "    \n",
    "    data = adata.copy()\n",
    "    \n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    sc.pp.normalize_total(data, target_sum=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "    \n",
    "    sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5)\n",
    "    \n",
    "    data = data[:, adata.var.highly_variable]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f0f7c2",
   "metadata": {},
   "source": [
    "# My Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63a14402",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read(\"data/cortex.h5ad\", dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2f59b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = processing(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67f813b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_matrix = adata.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aab3ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = adata.X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13fcb7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_true = adata.obs[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29821fc7",
   "metadata": {},
   "source": [
    "# Dataset&DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7af102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self,gene_matrix):\n",
    "        \n",
    "        self.gene_matrix = torch.from_numpy(gene_matrix)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.gene_matrix.shape[0]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        X = self.gene_matrix[idx]\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2c62125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loader(X,batch_size):\n",
    "\n",
    "    #print(type(X))\n",
    "    mydataset = MyDataset(X)\n",
    "\n",
    "    dataloader = DataLoader(dataset=mydataset,batch_size=batch_size,shuffle=True)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40651976",
   "metadata": {},
   "source": [
    "# autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5d444bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, input_dim: int = 10000,\n",
    "                 hidden_dim:int = 128,encoder_dim: int = 128, emission_dim: int = 128,\n",
    "                 num_layers_encoder: int = 2,num_layers_decoder: int = 2,\n",
    "                 z_dim: int = 32,\n",
    "                 dropout_rate:float = 0.1):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(input_dim, z_dim, num_layers=num_layers_encoder,hidden_dim=encoder_dim)\n",
    "        self.decoder = Decoder(z_dim, input_dim, num_layers=num_layers_decoder,hidden_dim=emission_dim)\n",
    "        \n",
    "\n",
    "        # initialize loss\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        x = data\n",
    "\n",
    "        z_e,_ = self.encoder(x)\n",
    "\n",
    "        x_e,_ = self.decoder(z_e)\n",
    "\n",
    "        return x_e, z_e\n",
    "\n",
    "    def _loss_reconstruct(self, x, x_e):\n",
    "\n",
    "        l_e = self.mse_loss(x, x_e)\n",
    "        \n",
    "        mse_l = l_e\n",
    "\n",
    "        return mse_l\n",
    "\n",
    "    def loss(self, x, x_e):\n",
    "        \n",
    "        l = self._loss_reconstruct(x, x_e)\n",
    "        \n",
    "        return l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3573ce1d",
   "metadata": {},
   "source": [
    "# Pretrain Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94c37430",
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_dataloader = create_loader(gene_matrix,batch_size = AE_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dfba797",
   "metadata": {},
   "outputs": [],
   "source": [
    "AE = autoencoder(input_dim = G,num_layers_decoder=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "619080b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_optimizer = torch.optim.Adam(AE.parameters(), lr=AE_learning_rate)\n",
    "AE_scheduler = torch.optim.lr_scheduler.StepLR(AE_optimizer, 1000,AE_decay_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9080c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_result_AE(model,dataloader,clusters_true,gene_matrix):\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    gene_matrix = torch.from_numpy(gene_matrix).to(device)\n",
    "\n",
    "    x_e,z_e = model(gene_matrix)\n",
    "    \n",
    "    z_e = z_e.detach().cpu().numpy()\n",
    "    \n",
    "    adata.obsm['X_unifan'] = z_e\n",
    "    \n",
    "    sc.pp.neighbors(adata, n_pcs=32,use_rep='X_unifan', random_state=123)\n",
    "    \n",
    "    sc.tl.leiden(adata, resolution=0.5, random_state=123)\n",
    "    \n",
    "    clusters_pre = adata.obs['leiden'].astype('int').values  # original as string\n",
    "    \n",
    "    ari = adjusted_rand_score(clusters_pre, clusters_true)\n",
    "\n",
    "    nmi = adjusted_mutual_info_score(clusters_pre, clusters_true)\n",
    "\n",
    "    return ari,nmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77ebd78a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 total_loss:0.16327348351478577 ari:0.4706816793932312 nmi:0.6633013612583779\n",
      "epoch:2 total_loss:0.1452455073595047 ari:0.4706816793932312 nmi:0.6633013612583779\n",
      "epoch:3 total_loss:0.1253722459077835 ari:0.4706816793932312 nmi:0.6633013612583779\n",
      "epoch:4 total_loss:0.10423865914344788 ari:0.4706816793932312 nmi:0.6633013612583779\n",
      "epoch:5 total_loss:0.09151306748390198 ari:0.4706816793932312 nmi:0.6633013612583779\n",
      "epoch:6 total_loss:0.0834527313709259 ari:0.4706816793932312 nmi:0.6633013612583779\n",
      "epoch:7 total_loss:0.06227501854300499 ari:0.4706816793932312 nmi:0.6633013612583779\n",
      "epoch:8 total_loss:0.06521636992692947 ari:0.4706816793932312 nmi:0.6633013612583779\n",
      "epoch:9 total_loss:0.06770803034305573 ari:0.4706816793932312 nmi:0.6633013612583779\n",
      "epoch:10 total_loss:0.04950794577598572 ari:0.4706816793932312 nmi:0.6633013612583779\n",
      "epoch:11 total_loss:0.044275205582380295 ari:0.4706816793932312 nmi:0.6633013612583779\n",
      "epoch:12 total_loss:0.039720430970191956 ari:0.4706816793932312 nmi:0.6633013612583779\n",
      "epoch:13 total_loss:0.041199784725904465 ari:0.4706816793932312 nmi:0.6633013612583779\n",
      "epoch:14 total_loss:0.03932574391365051 ari:0.4706816793932312 nmi:0.6633013612583779\n",
      "epoch:15 total_loss:0.040393996983766556 ari:0.4706816793932312 nmi:0.6633013612583779\n",
      "epoch:16 total_loss:0.03669990971684456 ari:0.4706816793932312 nmi:0.6633013612583779\n",
      "epoch:17 total_loss:0.03604508191347122 ari:0.4706816793932312 nmi:0.6633013612583779\n",
      "epoch:18 total_loss:0.034301016479730606 ari:0.4706816793932312 nmi:0.6633013612583779\n",
      "epoch:19 total_loss:0.034836024045944214 ari:0.4706816793932312 nmi:0.6633013612583779\n",
      "epoch:20 total_loss:0.03492812439799309 ari:0.4706816793932312 nmi:0.6633013612583779\n",
      "epoch:21 total_loss:0.03408303111791611 ari:0.5067942431947207 nmi:0.6668732958317097\n",
      "epoch:22 total_loss:0.033316802233457565 ari:0.5067942431947207 nmi:0.6668732958317097\n",
      "epoch:23 total_loss:0.03353867307305336 ari:0.5067942431947207 nmi:0.6668732958317097\n",
      "epoch:24 total_loss:0.039440590888261795 ari:0.5067942431947207 nmi:0.6668732958317097\n",
      "epoch:25 total_loss:0.03266675025224686 ari:0.5067942431947207 nmi:0.6668732958317097\n",
      "epoch:26 total_loss:0.032827891409397125 ari:0.5067942431947207 nmi:0.6668732958317097\n",
      "epoch:27 total_loss:0.03652377799153328 ari:0.5067942431947207 nmi:0.6668732958317097\n",
      "epoch:28 total_loss:0.03287268429994583 ari:0.5067942431947207 nmi:0.6668732958317097\n",
      "epoch:29 total_loss:0.03076200559735298 ari:0.5067942431947207 nmi:0.6668732958317097\n",
      "epoch:30 total_loss:0.02784188650548458 ari:0.5067942431947207 nmi:0.6668732958317097\n",
      "epoch:31 total_loss:0.027008000761270523 ari:0.5067942431947207 nmi:0.6668732958317097\n",
      "epoch:32 total_loss:0.026489196345210075 ari:0.5067942431947207 nmi:0.6668732958317097\n",
      "epoch:33 total_loss:0.02886616252362728 ari:0.5067942431947207 nmi:0.6668732958317097\n",
      "epoch:34 total_loss:0.030657371506094933 ari:0.5067942431947207 nmi:0.6668732958317097\n",
      "epoch:35 total_loss:0.03739907220005989 ari:0.5126145922106655 nmi:0.6675397315808562\n",
      "epoch:36 total_loss:0.029672477394342422 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:37 total_loss:0.026728397235274315 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:38 total_loss:0.028663182631134987 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:39 total_loss:0.02750670723617077 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:40 total_loss:0.02483886107802391 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:41 total_loss:0.0280438419431448 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:42 total_loss:0.024507226422429085 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:43 total_loss:0.023914970457553864 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:44 total_loss:0.024039924144744873 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:45 total_loss:0.023811452090740204 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:46 total_loss:0.025834787636995316 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:47 total_loss:0.02375345304608345 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:48 total_loss:0.023553060367703438 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:49 total_loss:0.023347048088908195 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:50 total_loss:0.023245031014084816 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:51 total_loss:0.02353048138320446 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:52 total_loss:0.023146066814661026 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:53 total_loss:0.022843385115265846 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:54 total_loss:0.022868333384394646 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:55 total_loss:0.02353251725435257 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:56 total_loss:0.02717289701104164 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:57 total_loss:0.028627511113882065 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:58 total_loss:0.025430109351873398 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:59 total_loss:0.024164622649550438 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:60 total_loss:0.023715173825621605 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:61 total_loss:0.022828787565231323 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:62 total_loss:0.02415178157389164 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:63 total_loss:0.027986975386738777 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:64 total_loss:0.02591189183294773 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:65 total_loss:0.022621992975473404 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:66 total_loss:0.021830419078469276 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:67 total_loss:0.02167162299156189 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:68 total_loss:0.022882884368300438 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:69 total_loss:0.02468559518456459 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:70 total_loss:0.022465191781520844 ari:0.530448981779764 nmi:0.6815801534530451\n",
      "epoch:71 total_loss:0.025880038738250732 ari:0.5383845167651116 nmi:0.6841528421942745\n",
      "epoch:72 total_loss:0.032917700707912445 ari:0.5383845167651116 nmi:0.6841528421942745\n",
      "epoch:73 total_loss:0.0531708225607872 ari:0.5383845167651116 nmi:0.6841528421942745\n",
      "epoch:74 total_loss:0.04448492452502251 ari:0.5383845167651116 nmi:0.6841528421942745\n",
      "epoch:75 total_loss:0.06412772834300995 ari:0.5383845167651116 nmi:0.6841528421942745\n",
      "epoch:76 total_loss:0.05026717856526375 ari:0.5383845167651116 nmi:0.6841528421942745\n",
      "epoch:77 total_loss:0.028405966237187386 ari:0.5383845167651116 nmi:0.6841528421942745\n",
      "epoch:78 total_loss:0.025876076892018318 ari:0.5383845167651116 nmi:0.6841528421942745\n",
      "epoch:79 total_loss:0.032879527658224106 ari:0.5383845167651116 nmi:0.6841528421942745\n",
      "epoch:80 total_loss:0.027262335643172264 ari:0.5383845167651116 nmi:0.6841528421942745\n",
      "epoch:81 total_loss:0.024447215721011162 ari:0.5383845167651116 nmi:0.6841528421942745\n",
      "epoch:82 total_loss:0.021369563415646553 ari:0.5383845167651116 nmi:0.6841528421942745\n",
      "epoch:83 total_loss:0.020528333261609077 ari:0.5383845167651116 nmi:0.6841528421942745\n",
      "epoch:84 total_loss:0.020446810871362686 ari:0.5383845167651116 nmi:0.6841528421942745\n",
      "epoch:85 total_loss:0.020393969491124153 ari:0.5383845167651116 nmi:0.6841528421942745\n",
      "epoch:86 total_loss:0.01998746767640114 ari:0.5383845167651116 nmi:0.6841528421942745\n",
      "epoch:87 total_loss:0.020007681101560593 ari:0.5383845167651116 nmi:0.6841528421942745\n",
      "epoch:88 total_loss:0.0200488418340683 ari:0.5383845167651116 nmi:0.6841528421942745\n",
      "epoch:89 total_loss:0.019900891929864883 ari:0.5383845167651116 nmi:0.6841528421942745\n",
      "epoch:90 total_loss:0.01980104111135006 ari:0.5383845167651116 nmi:0.6841528421942745\n",
      "epoch:91 total_loss:0.01980436220765114 ari:0.5641069674476631 nmi:0.6915853392852442\n",
      "epoch:92 total_loss:0.01966007798910141 ari:0.5641069674476631 nmi:0.6915853392852442\n",
      "epoch:93 total_loss:0.019556628540158272 ari:0.5641069674476631 nmi:0.6915853392852442\n",
      "epoch:94 total_loss:0.020001770928502083 ari:0.5641069674476631 nmi:0.6915853392852442\n",
      "epoch:95 total_loss:0.019620057195425034 ari:0.5641069674476631 nmi:0.6915853392852442\n",
      "epoch:96 total_loss:0.019451849162578583 ari:0.5641069674476631 nmi:0.6915853392852442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:97 total_loss:0.01940995268523693 ari:0.5641069674476631 nmi:0.6915853392852442\n",
      "epoch:98 total_loss:0.019407309591770172 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:99 total_loss:0.019351104274392128 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:100 total_loss:0.019431812688708305 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:101 total_loss:0.019248533993959427 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:102 total_loss:0.019264720380306244 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:103 total_loss:0.01914284937083721 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:104 total_loss:0.01912068948149681 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:105 total_loss:0.019152432680130005 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:106 total_loss:0.019087698310613632 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:107 total_loss:0.019104955717921257 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:108 total_loss:0.019450409337878227 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:109 total_loss:0.019278360530734062 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:110 total_loss:0.019028134644031525 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:111 total_loss:0.019023684784770012 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:112 total_loss:0.01880594529211521 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:113 total_loss:0.018851343542337418 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:114 total_loss:0.019128473475575447 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:115 total_loss:0.018964087590575218 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:116 total_loss:0.018824273720383644 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:117 total_loss:0.01872718706727028 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:118 total_loss:0.01853981241583824 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:119 total_loss:0.018782200291752815 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:120 total_loss:0.01861828751862049 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:121 total_loss:0.018437450751662254 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:122 total_loss:0.01874835230410099 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:123 total_loss:0.019278548657894135 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:124 total_loss:0.018694596365094185 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:125 total_loss:0.018394777551293373 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:126 total_loss:0.018495388329029083 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:127 total_loss:0.018390700221061707 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:128 total_loss:0.01883578486740589 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:129 total_loss:0.01804870367050171 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:130 total_loss:0.01781802996993065 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:131 total_loss:0.017992986366152763 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:132 total_loss:0.019647881388664246 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:133 total_loss:0.020147191360592842 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:134 total_loss:0.01876983977854252 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:135 total_loss:0.022516462951898575 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:136 total_loss:0.022776219993829727 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:137 total_loss:0.021867651492357254 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:138 total_loss:0.019418738782405853 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:139 total_loss:0.018267080187797546 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:140 total_loss:0.018222332000732422 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:141 total_loss:0.017975427210330963 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:142 total_loss:0.017612142488360405 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:143 total_loss:0.021159229800105095 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:144 total_loss:0.018530528992414474 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:145 total_loss:0.018327413126826286 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:146 total_loss:0.018176667392253876 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:147 total_loss:0.017907358705997467 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:148 total_loss:0.018518663942813873 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:149 total_loss:0.017382275313138962 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:150 total_loss:0.017016157507896423 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:151 total_loss:0.017111103981733322 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:152 total_loss:0.016871854662895203 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:153 total_loss:0.017094647511839867 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:154 total_loss:0.016809513792395592 ari:0.565304948087691 nmi:0.6922163170525595\n",
      "epoch:155 total_loss:0.016697194427251816 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:156 total_loss:0.016689952462911606 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:157 total_loss:0.017052579671144485 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:158 total_loss:0.017341407015919685 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:159 total_loss:0.017023345455527306 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:160 total_loss:0.01743304915726185 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:161 total_loss:0.017246538773179054 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:162 total_loss:0.018660668283700943 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:163 total_loss:0.017430907115340233 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:164 total_loss:0.01730908825993538 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:165 total_loss:0.0166595708578825 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:166 total_loss:0.017116867005825043 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:167 total_loss:0.017296064645051956 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:168 total_loss:0.01712856814265251 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:169 total_loss:0.01706632226705551 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:170 total_loss:0.017025554552674294 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:171 total_loss:0.01669800840318203 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:172 total_loss:0.017076892778277397 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:173 total_loss:0.01834776997566223 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:174 total_loss:0.01765737682580948 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:175 total_loss:0.017669551074504852 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:176 total_loss:0.01663748361170292 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:177 total_loss:0.016605228185653687 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:178 total_loss:0.016672184690833092 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:179 total_loss:0.01667768321931362 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:180 total_loss:0.01633146032691002 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:181 total_loss:0.01632130891084671 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:182 total_loss:0.01632041297852993 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:183 total_loss:0.016224080696702003 ari:0.6487664157657619 nmi:0.6937230268360429\n",
      "epoch:184 total_loss:0.01681608334183693 ari:0.6605466847872475 nmi:0.7043657855512444\n",
      "epoch:185 total_loss:0.016102636232972145 ari:0.6605466847872475 nmi:0.7043657855512444\n",
      "epoch:186 total_loss:0.016096943989396095 ari:0.6605466847872475 nmi:0.7043657855512444\n",
      "epoch:187 total_loss:0.01600121706724167 ari:0.6784445567900751 nmi:0.705557619747758\n",
      "epoch:188 total_loss:0.01622483879327774 ari:0.6784445567900751 nmi:0.705557619747758\n",
      "epoch:189 total_loss:0.01626230776309967 ari:0.6784445567900751 nmi:0.705557619747758\n",
      "epoch:190 total_loss:0.016200140118598938 ari:0.6784445567900751 nmi:0.705557619747758\n",
      "epoch:191 total_loss:0.016200127080082893 ari:0.6784445567900751 nmi:0.705557619747758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:192 total_loss:0.01602635718882084 ari:0.6784445567900751 nmi:0.705557619747758\n",
      "epoch:193 total_loss:0.01791669800877571 ari:0.6784445567900751 nmi:0.705557619747758\n",
      "epoch:194 total_loss:0.016913410276174545 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:195 total_loss:0.016701264306902885 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:196 total_loss:0.019234267994761467 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:197 total_loss:0.018354618921875954 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:198 total_loss:0.018858924508094788 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:199 total_loss:0.016893258318305016 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:200 total_loss:0.016256151720881462 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:201 total_loss:0.01612790673971176 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:202 total_loss:0.016701025888323784 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:203 total_loss:0.01644170470535755 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:204 total_loss:0.016564764082431793 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:205 total_loss:0.016902104020118713 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:206 total_loss:0.016461310908198357 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:207 total_loss:0.01601208746433258 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:208 total_loss:0.015911223366856575 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:209 total_loss:0.017725275829434395 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:210 total_loss:0.022334303706884384 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:211 total_loss:0.01885984279215336 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:212 total_loss:0.023695096373558044 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:213 total_loss:0.01919529400765896 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:214 total_loss:0.016449270769953728 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:215 total_loss:0.015867499634623528 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:216 total_loss:0.015316426753997803 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:217 total_loss:0.015349477529525757 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:218 total_loss:0.01535409688949585 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:219 total_loss:0.015417065471410751 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:220 total_loss:0.01522698625922203 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:221 total_loss:0.015198335982859135 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:222 total_loss:0.01541972253471613 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:223 total_loss:0.01522343885153532 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:224 total_loss:0.01538253203034401 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:225 total_loss:0.015214831568300724 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:226 total_loss:0.014965430833399296 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:227 total_loss:0.015000173822045326 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:228 total_loss:0.01505669392645359 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:229 total_loss:0.015101614408195019 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:230 total_loss:0.015105384401977062 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:231 total_loss:0.014949142932891846 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:232 total_loss:0.015051394701004028 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:233 total_loss:0.015008687973022461 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:234 total_loss:0.015382914803922176 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:235 total_loss:0.015306908637285233 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:236 total_loss:0.015109478496015072 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:237 total_loss:0.015076040290296078 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:238 total_loss:0.01624179817736149 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:239 total_loss:0.01864965632557869 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:240 total_loss:0.018695905804634094 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:241 total_loss:0.01594911329448223 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:242 total_loss:0.015182162635028362 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:243 total_loss:0.015665369108319283 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:244 total_loss:0.015195910818874836 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:245 total_loss:0.015460356138646603 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:246 total_loss:0.015312667936086655 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:247 total_loss:0.015349439345300198 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:248 total_loss:0.014870578423142433 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:249 total_loss:0.015197223983705044 ari:0.7079299746789033 nmi:0.7216107751656855\n",
      "epoch:250 total_loss:0.014839617535471916 ari:0.7116139607793519 nmi:0.7219660976623433\n",
      "epoch:251 total_loss:0.014733637683093548 ari:0.7116139607793519 nmi:0.7219660976623433\n",
      "epoch:252 total_loss:0.014853913336992264 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:253 total_loss:0.014606432057917118 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:254 total_loss:0.01461849082261324 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:255 total_loss:0.014624420553445816 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:256 total_loss:0.014766751788556576 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:257 total_loss:0.014636961743235588 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:258 total_loss:0.014702357351779938 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:259 total_loss:0.014596830122172832 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:260 total_loss:0.01461358554661274 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:261 total_loss:0.014806626364588737 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:262 total_loss:0.014597095549106598 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:263 total_loss:0.014479374513030052 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:264 total_loss:0.014286275953054428 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:265 total_loss:0.01441140566021204 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:266 total_loss:0.01447442639619112 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:267 total_loss:0.014398853294551373 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:268 total_loss:0.014459707774221897 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:269 total_loss:0.014763508923351765 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:270 total_loss:0.014651486650109291 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:271 total_loss:0.014702491462230682 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:272 total_loss:0.014525636099278927 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:273 total_loss:0.014329093508422375 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:274 total_loss:0.014367264695465565 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:275 total_loss:0.014316105283796787 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:276 total_loss:0.014289399608969688 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:277 total_loss:0.01428028754889965 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:278 total_loss:0.014275908470153809 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:279 total_loss:0.014470255002379417 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:280 total_loss:0.014547019265592098 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:281 total_loss:0.014473293907940388 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:282 total_loss:0.01439920999109745 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:283 total_loss:0.014674214646220207 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:284 total_loss:0.014713067561388016 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:285 total_loss:0.014888000674545765 ari:0.7131274812263191 nmi:0.7225695717211571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:286 total_loss:0.014645556919276714 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:287 total_loss:0.01593327522277832 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:288 total_loss:0.017791781574487686 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:289 total_loss:0.014721155166625977 ari:0.7131274812263191 nmi:0.7225695717211571\n",
      "epoch:290 total_loss:0.014353224076330662 ari:0.716038964269005 nmi:0.7229155208359529\n",
      "epoch:291 total_loss:0.01426575519144535 ari:0.716038964269005 nmi:0.7229155208359529\n",
      "epoch:292 total_loss:0.01452100370079279 ari:0.716038964269005 nmi:0.7229155208359529\n",
      "epoch:293 total_loss:0.015033071860671043 ari:0.716038964269005 nmi:0.7229155208359529\n",
      "epoch:294 total_loss:0.014333422295749187 ari:0.716038964269005 nmi:0.7229155208359529\n",
      "epoch:295 total_loss:0.014183432795107365 ari:0.716038964269005 nmi:0.7229155208359529\n",
      "epoch:296 total_loss:0.014314747415482998 ari:0.716038964269005 nmi:0.7229155208359529\n",
      "epoch:297 total_loss:0.014063561335206032 ari:0.716038964269005 nmi:0.7229155208359529\n",
      "epoch:298 total_loss:0.014004575088620186 ari:0.716038964269005 nmi:0.7229155208359529\n",
      "epoch:299 total_loss:0.01389267761260271 ari:0.716038964269005 nmi:0.7229155208359529\n",
      "epoch:300 total_loss:0.01396331936120987 ari:0.716038964269005 nmi:0.7229155208359529\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(AE_epochs):\n",
    "    \n",
    "    total_loss = 0\n",
    "    for batch_idx,X_batch in enumerate(AE_dataloader):\n",
    "        X_batch = X_batch.to(device).float()\n",
    "        \n",
    "        AE_optimizer.zero_grad()\n",
    "\n",
    "        x_e, z_e = AE(X_batch)\n",
    "\n",
    "        loss = AE.loss(X_batch.float(), x_e.float())\n",
    "        \n",
    "        total_loss += loss\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        AE_optimizer.step()\n",
    "        AE_scheduler.step()\n",
    "\n",
    "    ari,nmi = cal_result_AE(AE,AE_dataloader,clusters_true,gene_matrix)\n",
    "    \n",
    "    if best_ari < ari and best_nmi < nmi:\n",
    "        \n",
    "        best_ari = ari\n",
    "        \n",
    "        best_nmi = nmi\n",
    "        \n",
    "    print(f\"epoch:{epoch+1} total_loss:{total_loss/len(AE_dataloader.sampler)} ari:{best_ari} nmi:{best_nmi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "171d1cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "AE.to(torch.device(\"cpu\")).eval()\n",
    "z_init,_ = AE.encoder(torch.from_numpy(gene_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "574040ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_init = z_init.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08e7e874",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.AnnData(X=z_init)\n",
    "adata.obsm['X_unifan'] = z_init\n",
    "sc.pp.neighbors(adata, n_pcs=32,use_rep='X_unifan', random_state=AE_random_seed)\n",
    "sc.tl.leiden(adata, resolution=0.5, random_state=AE_random_seed)\n",
    "clusters_pre = adata.obs['leiden'].astype('int').values  # original as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "602bc1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster = pd.DataFrame(z_init)\n",
    "cluster_labels = np.unique(clusters_pre)\n",
    "M = len(set(cluster_labels))  # set as number of clusters\n",
    "df_cluster['cluster'] = clusters_pre\n",
    "\n",
    "# get centroids\n",
    "centroids = df_cluster.groupby('cluster').mean().values\n",
    "centroids_torch = torch.from_numpy(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35f25e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = torch.from_numpy(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "851a69a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_smaller = adjusted_rand_score(clusters_pre,\n",
    "                                  clusters_true)\n",
    "nmi_smaller = adjusted_mutual_info_score(clusters_pre, clusters_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2bc5595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6900660378155391"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmi_smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63bc8510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6470483905440783"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ari_smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d9be3b",
   "metadata": {},
   "source": [
    "# VQVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f43a9a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE_T(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim: int = 10000, z_dim: int = 32,\n",
    "                  encoder_dim: int = 128, emission_dim: int = 128,\n",
    "                 num_layers_encoder: int = 2,num_layers_decoder: int = 2,\n",
    "                 beta: float = 1.0,gama:float =0.25,\n",
    "                 n_clusters: int = 16,\n",
    "                 hidden_dim: int = 128, dropout_rate: float = 0.1, use_t_dist: bool = True,\n",
    "                 centroids: torch.Tensor = None):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # initialize parameters\n",
    "        self.z_dim = z_dim\n",
    "        self.beta = beta\n",
    "        self.gama = gama\n",
    "        self.n_clusters = n_clusters\n",
    "        self.use_t_dist = use_t_dist\n",
    "        \n",
    "        # initialize centroids embeddings\n",
    "        if centroids is not None:\n",
    "            self.embeddings = nn.Parameter(centroids, requires_grad=True)\n",
    "        else:\n",
    "            self.embeddings = nn.Parameter(torch.randn(self.n_clusters, self.z_dim) * 0.05, requires_grad=True)\n",
    "\n",
    "        # initialize loss\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "        self.encoder = Encoder(input_dim, z_dim, num_layers=num_layers_encoder, hidden_dim=encoder_dim,\n",
    "                                   dropout_rate=dropout_rate)\n",
    "        \n",
    "        self.decoder = Decoder(z_dim, input_dim, num_layers=num_layers_decoder,hidden_dim=emission_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # get encoding\n",
    "        z_e, _ = self.encoder(x)\n",
    "\n",
    "        # get the index of embedding closed to the encoding\n",
    "        k, z_dist, dist_prob = self._get_clusters(z_e)\n",
    "\n",
    "        # get embeddings (discrete representations)\n",
    "        z_q = self._get_embeddings(k)\n",
    "\n",
    "        # decode embedding (discrete representation) and encoding\n",
    "        x_q, _ = self.decoder(z_e + (z_q-z_e).detach())\n",
    "\n",
    "        return x_q, z_e, z_q\n",
    "\n",
    "    def _get_clusters(self, z_e):\n",
    "\n",
    "\n",
    "        _z_dist = (z_e.unsqueeze(1) - self.embeddings.unsqueeze(0)) ** 2\n",
    "        z_dist = torch.sum(_z_dist, dim=-1)\n",
    "        if self.use_t_dist:\n",
    "            dist_prob = self._t_dist_sim(z_dist, df=10)\n",
    "            k = torch.argmax(dist_prob, dim=-1)\n",
    "        else:\n",
    "            k = torch.argmin(z_dist, dim=-1)\n",
    "            dist_prob = None\n",
    "\n",
    "        return k, z_dist, dist_prob\n",
    "\n",
    "    def _t_dist_sim(self, z_dist, df=10):\n",
    "\n",
    "\n",
    "        _factor = - ((df + 1) / 2)\n",
    "        dist_prob = torch.pow((1 + z_dist / df), _factor)\n",
    "        dist_prob = dist_prob / dist_prob.sum(axis=1).unsqueeze(1)\n",
    "\n",
    "        return dist_prob\n",
    "\n",
    "    def _get_embeddings(self, k):\n",
    "\n",
    "\n",
    "        k = k.long()\n",
    "        _z_q = []\n",
    "        for i in range(len(k)):\n",
    "            _z_q.append(self.embeddings[k[i]])\n",
    "\n",
    "        z_q = torch.stack(_z_q)\n",
    "\n",
    "        return z_q\n",
    "\n",
    "\n",
    "    def _loss_reconstruct(self,x,x_q, z_e, z_q):\n",
    "\n",
    "\n",
    "        l_x = self.mse_loss(x, x_q)\n",
    "        \n",
    "        l_q = self.mse_loss(z_e.detach(),z_q)\n",
    "        \n",
    "        l_e = self.mse_loss(z_e,z_q.detach())\n",
    "        \n",
    "        mse_l = l_e + self.beta*l_q + self.gama*l_x\n",
    "        \n",
    "        return mse_l\n",
    "\n",
    "\n",
    "    def loss(self,x ,x_e ,z_e ,z_q):\n",
    "\n",
    "        mse_l = self._loss_reconstruct(x,x_e,z_e,z_q)\n",
    "\n",
    "        return mse_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c9bdfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE_N(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim: int = 10000, z_dim: int = 32,\n",
    "                  encoder_dim: int = 128, emission_dim: int = 128,\n",
    "                 num_layers_encoder: int = 2,num_layers_decoder: int = 2,\n",
    "                 beta: float = 1.0,gama:float =0.25,\n",
    "                 n_clusters: int = 16,\n",
    "                 hidden_dim: int = 128, dropout_rate: float = 0.1, use_t_dist: bool = False,\n",
    "                 centroids: torch.Tensor = None):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # initialize parameters\n",
    "        self.z_dim = z_dim\n",
    "        self.beta = beta\n",
    "        self.gama = gama\n",
    "        self.n_clusters = n_clusters\n",
    "        self.use_t_dist = use_t_dist\n",
    "        \n",
    "        # initialize centroids embeddings\n",
    "        if centroids is not None:\n",
    "            self.embeddings = nn.Parameter(centroids, requires_grad=True)\n",
    "        else:\n",
    "            self.embeddings = nn.Parameter(torch.randn(self.n_clusters, self.z_dim) * 0.05, requires_grad=True)\n",
    "\n",
    "        # initialize loss\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "        self.encoder = Encoder(input_dim, z_dim, num_layers=num_layers_encoder, hidden_dim=encoder_dim,\n",
    "                                   dropout_rate=dropout_rate)\n",
    "        \n",
    "        self.decoder = Decoder(z_dim, input_dim, num_layers=num_layers_decoder,hidden_dim=emission_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # get encoding\n",
    "        z_e, _ = self.encoder(x)\n",
    "\n",
    "        # get the index of embedding closed to the encoding\n",
    "        k, z_dist, dist_prob = self._get_clusters(z_e)\n",
    "\n",
    "        # get embeddings (discrete representations)\n",
    "        z_q = self._get_embeddings(k)\n",
    "\n",
    "        # decode embedding (discrete representation) and encoding\n",
    "        x_q, _ = self.decoder(z_e + (z_q-z_e).detach())\n",
    "\n",
    "        return x_q, z_e, z_q\n",
    "\n",
    "    def _get_clusters(self, z_e):\n",
    "\n",
    "\n",
    "        _z_dist = (z_e.unsqueeze(1) - self.embeddings.unsqueeze(0)) ** 2\n",
    "        z_dist = torch.sum(_z_dist, dim=-1)\n",
    "        if self.use_t_dist:\n",
    "            dist_prob = self._t_dist_sim(z_dist, df=10)\n",
    "            k = torch.argmax(dist_prob, dim=-1)\n",
    "        else:\n",
    "            k = torch.argmin(z_dist, dim=-1)\n",
    "            dist_prob = None\n",
    "\n",
    "        return k, z_dist, dist_prob\n",
    "\n",
    "    def _t_dist_sim(self, z_dist, df=10):\n",
    "\n",
    "\n",
    "        _factor = - ((df + 1) / 2)\n",
    "        dist_prob = torch.pow((1 + z_dist / df), _factor)\n",
    "        dist_prob = dist_prob / dist_prob.sum(axis=1).unsqueeze(1)\n",
    "\n",
    "        return dist_prob\n",
    "\n",
    "    def _get_embeddings(self, k):\n",
    "\n",
    "\n",
    "        k = k.long()\n",
    "        _z_q = []\n",
    "        for i in range(len(k)):\n",
    "            _z_q.append(self.embeddings[k[i]])\n",
    "\n",
    "        z_q = torch.stack(_z_q)\n",
    "\n",
    "        return z_q\n",
    "\n",
    "\n",
    "    def _loss_reconstruct(self,x,x_q, z_e, z_q):\n",
    "\n",
    "\n",
    "        l_x = self.mse_loss(x, x_q)\n",
    "        \n",
    "        l_q = self.mse_loss(z_e.detach(),z_q)\n",
    "        \n",
    "        l_e = self.mse_loss(z_e,z_q.detach())\n",
    "        \n",
    "        mse_l = l_e + self.beta*l_q + self.gama*l_x\n",
    "        \n",
    "        return mse_l\n",
    "\n",
    "\n",
    "    def loss(self,x ,x_e ,z_e ,z_q):\n",
    "\n",
    "        mse_l = self._loss_reconstruct(x,x_e,z_e,z_q)\n",
    "\n",
    "        return mse_l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be158f0",
   "metadata": {},
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8bba4d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adam_training(model,n_epochs,learning_rate,dataloader,clusters_true,gene_matrix,\n",
    "                non_blocking = True,decay_factor = 0.9):\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1000, decay_factor)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        total_loss = 0\n",
    "    \n",
    "        for batch_idx, X_batch in enumerate(dataloader):\n",
    "            \n",
    "            X_batch = X_batch.to(device, non_blocking=non_blocking).float()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            x_q, z_e, z_q = model(X_batch)\n",
    "\n",
    "            l = model.loss(X_batch,x_q,z_e,z_q)\n",
    "       \n",
    "            total_loss += l\n",
    "\n",
    "            l.backward(retain_graph=True)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "        ari,nmi = cal_result(model,dataloader,clusters_true,gene_matrix)\n",
    "        global best_ari,best_nmi\n",
    "        if best_ari < ari and best_nmi < nmi:\n",
    "\n",
    "            best_ari = ari\n",
    "\n",
    "            best_nmi = nmi\n",
    "        \n",
    "        print(\"------------------------------\")\n",
    "        print(f\"epoch:{epoch+1} total_loss:{total_loss/len(dataloader)} \\n ari:{ari.item()} nmi:{nmi.item()} \\n best ari:{best_ari.item()} bset nmi:{best_nmi.item()}\")\n",
    "        \n",
    "            \n",
    "    # move network back to cpu and return\n",
    "    model.cpu()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df200708",
   "metadata": {},
   "source": [
    "# GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64e10e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossover_and_mutation(parents, sigma=0.05):\n",
    "\n",
    "    \n",
    "    base_sd = parents[0].state_dict()\n",
    "    keys = base_sd                    # use all layers to be affected\n",
    "    \n",
    "    # Sum of the weights of the parent\n",
    "    for i in range(1, len(parents)):\n",
    "        parent_sd = parents[i].state_dict()\n",
    "        for key in keys:\n",
    "            base_sd[key] = base_sd[key] + parent_sd[key]\n",
    "            \n",
    "    \n",
    "    # Average and add mutation\n",
    "    num_parents = len(parents)\n",
    "    \n",
    "    for key in keys:\n",
    "        \n",
    "        tensor_size = base_sd[key].size()\n",
    "        random_tensor = torch.normal(mean=0.0, std=sigma, size=tensor_size).to(device)\n",
    "        \n",
    "        base_sd[key] = (base_sd[key] / num_parents) + random_tensor\n",
    "    \n",
    "    # create offspring\n",
    "    \n",
    "    if random.randint(0,1) == 0:\n",
    "        offspring = VQVAE_T(input_dim = gene_matrix.shape[1],\n",
    "                            beta = 1,gama=0.25, n_clusters = centroids.shape[0])\n",
    "    \n",
    "        offspring.load_state_dict(base_sd)\n",
    "    else:\n",
    "    \n",
    "        offspring = VQVAE_N(input_dim = gene_matrix.shape[1],\n",
    "                            beta = 1,gama=0.25, n_clusters = centroids.shape[0])\n",
    "    \n",
    "        offspring.load_state_dict(base_sd)\n",
    "        \n",
    "    return offspring\n",
    "    \n",
    "\n",
    "def create_offspring(population,fitness,rho,sigma):\n",
    "\n",
    "    \n",
    "    # Perform selection\n",
    "    parents = random.choices(population, weights=fitness, k=rho) \n",
    "    \n",
    "    # Perform crossover and mutation\n",
    "    offspring = crossover_and_mutation(parents, sigma)\n",
    "    \n",
    "    \n",
    "    return offspring\n",
    "\n",
    "def sigmoid(x):\n",
    "    \n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def GA_training(population, pop_size, offspring_size, elitist_level, rho, sigma, dataloader,clusters_true,gene_matrix):\n",
    "    \n",
    "    #Calculate fitness of trained population\n",
    "\n",
    "    fitness = [cal_loss(population[i],dataloader)\n",
    "                                for i in range(pop_size)]\n",
    "    \n",
    "    print(f\"--- -- Finished fitness evaluation, length: {len(fitness)}\")\n",
    "    \n",
    "    #Create offspring population\n",
    "    fitness_weighted = [ 1/f for f in fitness]   # take inverse of loss so lower losses get higher fitness-values\n",
    "    \n",
    "    offspring_population = [create_offspring(population,fitness_weighted, rho, sigma) for i in range(offspring_size)]\n",
    "    \n",
    "    print(\"--- -- Finished creating offspring population\")\n",
    "    \n",
    "    #Evaluate fitness of offsprings \n",
    "    \n",
    "    offspring_fitness = [cal_loss(offspring_population[i],dataloader) \n",
    "                                                          for i in range(offspring_size)]\n",
    "    \n",
    "    print(\"--- -- Finished evaluating fitness of offspring population\")\n",
    "    \n",
    "    # Combine fitness and population lists\n",
    "    \n",
    "    combined_fitness = fitness + offspring_fitness\n",
    "    combined_population = population + offspring_population\n",
    "    \n",
    "    # sort and select population by their fitness values\n",
    "    \n",
    "    sorted_population = [pop for _, pop in sorted(zip(combined_fitness, combined_population), key=lambda pair: pair[0])]\n",
    "    sorted_fitness = [loss for loss, _ in sorted(zip(combined_fitness, combined_population), key=lambda pair: pair[0])]\n",
    "    \n",
    "    m = int(pop_size * elitist_level)\n",
    "    new_population = sorted_population[0:m]\n",
    "    \n",
    "    # Fill up rest of population\n",
    "    difference = pop_size - m\n",
    "    remaining_population = list(set(sorted_population) - set(new_population))\n",
    "    filler_population = random.sample(remaining_population, difference)\n",
    "    \n",
    "    # assemble new population and return\n",
    "    new_population = new_population + filler_population\n",
    "    \n",
    "    return new_population, sorted_fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1bc59b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_loss(model,dataloader,non_blocking=True):\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_idx, X_batch in enumerate(dataloader):\n",
    "        \n",
    "        X_batch = X_batch.to(device, non_blocking=non_blocking).float()\n",
    "\n",
    "        x_e, z_e, z_q = model(X_batch)\n",
    "\n",
    "        l = model.loss(X_batch,x_e, z_e, z_q)\n",
    "        \n",
    "        total_loss += l\n",
    "        \n",
    "\n",
    "    return float(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a6f9539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_result(model,dataloader,clusters_true,gene_matrix):\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    gene_matrix = torch.from_numpy(gene_matrix).to(device)\n",
    "\n",
    "    x_e,z_e,z_q = model(gene_matrix)\n",
    "    \n",
    "    z_e = z_e.detach().cpu().numpy()\n",
    "    \n",
    "    adata.obsm['X_unifan'] = z_e\n",
    "    \n",
    "    sc.pp.neighbors(adata, n_pcs=32,use_rep='X_unifan', random_state=123)\n",
    "    \n",
    "    sc.tl.leiden(adata, resolution=0.5, random_state=123)\n",
    "    \n",
    "    clusters_pre = adata.obs['leiden'].astype('int').values  # original as string\n",
    "    \n",
    "    ari = adjusted_rand_score(clusters_pre, clusters_true)\n",
    "\n",
    "    nmi = adjusted_mutual_info_score(clusters_pre, clusters_true)\n",
    "    \n",
    "    ari = torch.from_numpy(np.array(ari)).to(device)\n",
    "    nmi = torch.from_numpy(np.array(nmi)).to(device)\n",
    "    \n",
    "    return (ari,nmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70a2acb",
   "metadata": {},
   "source": [
    "# GA Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7d8796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GA_Neural_train(population,\n",
    "                    pop_size,\n",
    "                    max_generations, \n",
    "                    SGD_steps, GA_steps, \n",
    "                    offspring_size, elitist_level, rho,\n",
    "                    learning_rate,\n",
    "                    dataloader,\n",
    "                    clusters_true,\n",
    "                    gene_matrix):\n",
    "    \n",
    "    \n",
    "    print(f\"Starting with population of size: {pop_size}\")\n",
    "\n",
    "    for k in range(max_generations):\n",
    "        print(f\"Currently in generation {k+1}\")\n",
    "        \n",
    "        #Adam\n",
    "        print(f\"--- Starting Adam\")\n",
    "        \n",
    "        # Sequential version\n",
    "        \n",
    "        population_copy = []\n",
    "        \n",
    "        for i in range(pop_size):\n",
    "            \n",
    "            model = Adam_training(population[i],SGD_steps,learning_rate,dataloader,clusters_true,gene_matrix)\n",
    "            population_copy.append(model)\n",
    "        \n",
    "        print(f\"--- Finished Adam\")\n",
    "        \n",
    "        population = population_copy\n",
    "         \n",
    "        # GA\n",
    "        print(f\"--- Starting Model GA\")\n",
    "        GA_start = time.time()\n",
    "        sorted_fitness = []          # store the sorted fitness values to maybe use in data collection\n",
    "        for i in range(0, GA_steps):\n",
    "            \n",
    "            sigma = 0.01 / (k+1)\n",
    "            population, sorted_fitness = GA_training(population, \n",
    "                                                     pop_size, offspring_size, elitist_level, rho, sigma, dataloader,\n",
    "                                                     clusters_true,gene_matrix)\n",
    "        \n",
    "        GA_end = time.time()\n",
    "        \n",
    "        print(f\"--- Finished Model GA,Time:{(GA_end - GA_start) * 1000}ms\")\n",
    "        \n",
    "        \n",
    "    print(f\"Finished training process\")\n",
    "    \n",
    "    return population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2247f9a1",
   "metadata": {},
   "source": [
    "# Train VQVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f4367a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "#device_train = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 1024\n",
    "pop_size_T = 5\n",
    "pop_size_N = 5\n",
    "max_generations = 50\n",
    "SGD_steps = 10\n",
    "GA_steps = 1\n",
    "offspring_size = 30\n",
    "elitist_level = 0.4\n",
    "rho = 4\n",
    "learning_rate = 1e-5\n",
    "in_feats = 400\n",
    "n_hidden = 200\n",
    "weight_decay = 5e-4\n",
    "best_nmi = 0\n",
    "best_ari = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7094ceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_loader(gene_matrix,batch_size = batch_size)\n",
    "pop_size  = pop_size_T + pop_size_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7caf6397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create population and start training process\n",
    "population_T = [VQVAE_T(input_dim = gene_matrix.shape[1],\n",
    "                            beta = 1,gama=0.25, n_clusters = centroids.shape[0],centroids = centroids).to(device)\n",
    "                     for i in range(pop_size_T)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da257720",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_N = [VQVAE_N(input_dim = gene_matrix.shape[1],\n",
    "                            beta = 1,gama=0.25, n_clusters = centroids.shape[0],centroids = centroids).to(device)\n",
    "                     for i in range(pop_size_N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "96319356",
   "metadata": {},
   "outputs": [],
   "source": [
    "population = population_T + population_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c158087",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with population of size: 10\n",
      "Currently in generation 1\n",
      "--- Starting Adam\n",
      "------------------------------\n",
      "epoch:1 total_loss:41.320098876953125 \n",
      " ari:0.6989767025033613 nmi:0.6692552408737789 \n",
      " best ari:0.6989767025033613 bset nmi:0.6692552408737789\n",
      "------------------------------\n",
      "epoch:2 total_loss:40.96586227416992 \n",
      " ari:0.6858101866845631 nmi:0.6665990628543681 \n",
      " best ari:0.6989767025033613 bset nmi:0.6692552408737789\n",
      "------------------------------\n",
      "epoch:3 total_loss:40.774654388427734 \n",
      " ari:0.7140051227490141 nmi:0.666465276992457 \n",
      " best ari:0.6989767025033613 bset nmi:0.6692552408737789\n",
      "------------------------------\n",
      "epoch:4 total_loss:40.62885284423828 \n",
      " ari:0.7279271039041839 nmi:0.6818900147191096 \n",
      " best ari:0.7279271039041839 bset nmi:0.6818900147191096\n",
      "------------------------------\n",
      "epoch:5 total_loss:40.38605880737305 \n",
      " ari:0.6979671714068743 nmi:0.6641408730459795 \n",
      " best ari:0.7279271039041839 bset nmi:0.6818900147191096\n",
      "------------------------------\n",
      "epoch:6 total_loss:40.22407150268555 \n",
      " ari:0.7367766602465803 nmi:0.6878936313566941 \n",
      " best ari:0.7367766602465803 bset nmi:0.6878936313566941\n",
      "------------------------------\n",
      "epoch:7 total_loss:40.009254455566406 \n",
      " ari:0.7017175834121969 nmi:0.6699631501120112 \n",
      " best ari:0.7367766602465803 bset nmi:0.6878936313566941\n",
      "------------------------------\n",
      "epoch:8 total_loss:39.88274002075195 \n",
      " ari:0.6898915435817665 nmi:0.6602181221171002 \n",
      " best ari:0.7367766602465803 bset nmi:0.6878936313566941\n",
      "------------------------------\n",
      "epoch:9 total_loss:39.698753356933594 \n",
      " ari:0.747337172318242 nmi:0.68896606587726 \n",
      " best ari:0.747337172318242 bset nmi:0.68896606587726\n",
      "------------------------------\n",
      "epoch:10 total_loss:39.51029968261719 \n",
      " ari:0.6965046666268959 nmi:0.6748066649392441 \n",
      " best ari:0.747337172318242 bset nmi:0.68896606587726\n",
      "------------------------------\n",
      "epoch:1 total_loss:41.52750778198242 \n",
      " ari:0.6660219223265461 nmi:0.6414668773181645 \n",
      " best ari:0.747337172318242 bset nmi:0.68896606587726\n",
      "------------------------------\n",
      "epoch:2 total_loss:41.136714935302734 \n",
      " ari:0.7340198180533807 nmi:0.6810978577321185 \n",
      " best ari:0.747337172318242 bset nmi:0.68896606587726\n",
      "------------------------------\n",
      "epoch:3 total_loss:40.95161437988281 \n",
      " ari:0.6811608320675586 nmi:0.650875201739848 \n",
      " best ari:0.747337172318242 bset nmi:0.68896606587726\n",
      "------------------------------\n",
      "epoch:4 total_loss:40.743038177490234 \n",
      " ari:0.7390790231314988 nmi:0.6965243839991893 \n",
      " best ari:0.747337172318242 bset nmi:0.68896606587726\n",
      "------------------------------\n",
      "epoch:5 total_loss:40.46342468261719 \n",
      " ari:0.7514624178784669 nmi:0.6915206707031415 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:6 total_loss:40.26744842529297 \n",
      " ari:0.7247384194053613 nmi:0.6858529199582061 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:7 total_loss:40.08362579345703 \n",
      " ari:0.7299043550513454 nmi:0.6902428582687835 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:8 total_loss:39.78528594970703 \n",
      " ari:0.730475779273765 nmi:0.6894096680634049 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:9 total_loss:39.634151458740234 \n",
      " ari:0.7299256896653671 nmi:0.6906012845534865 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:10 total_loss:39.4103889465332 \n",
      " ari:0.7235720018796878 nmi:0.6833634576752098 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:1 total_loss:41.32273483276367 \n",
      " ari:0.6616057853111199 nmi:0.6164303963272839 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:2 total_loss:41.09139633178711 \n",
      " ari:0.6684663581447206 nmi:0.6158539533944126 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:3 total_loss:40.86431121826172 \n",
      " ari:0.6456047683995322 nmi:0.6139701765154846 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:4 total_loss:40.715248107910156 \n",
      " ari:0.6003675097100997 nmi:0.5771479125706166 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:5 total_loss:40.54372024536133 \n",
      " ari:0.6480960363522081 nmi:0.6122829573537445 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:6 total_loss:40.334049224853516 \n",
      " ari:0.6863890472107912 nmi:0.6336787536457981 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:7 total_loss:40.15721893310547 \n",
      " ari:0.6315106219742999 nmi:0.5921399931970566 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:8 total_loss:40.02093505859375 \n",
      " ari:0.6565799314729135 nmi:0.6197390046091811 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:9 total_loss:39.79273986816406 \n",
      " ari:0.6510983597730751 nmi:0.6137479188335633 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:10 total_loss:39.698909759521484 \n",
      " ari:0.6472932793409433 nmi:0.6197630000016984 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:1 total_loss:41.07225799560547 \n",
      " ari:0.7137278770138213 nmi:0.6681626954787393 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:2 total_loss:40.80431365966797 \n",
      " ari:0.7192975182485633 nmi:0.6696508580877925 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:3 total_loss:40.578208923339844 \n",
      " ari:0.7166487037508481 nmi:0.6699188666664203 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:4 total_loss:40.371681213378906 \n",
      " ari:0.6998773984366146 nmi:0.6533118196105906 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:5 total_loss:40.20183563232422 \n",
      " ari:0.7189639466628908 nmi:0.675548004079277 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:6 total_loss:39.99758529663086 \n",
      " ari:0.7150697766985462 nmi:0.6709638290358376 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:7 total_loss:39.803550720214844 \n",
      " ari:0.7173441589650917 nmi:0.6813690105668797 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:8 total_loss:39.58769226074219 \n",
      " ari:0.7263383985520235 nmi:0.683328405570712 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:9 total_loss:39.434898376464844 \n",
      " ari:0.7248484944313414 nmi:0.6843049532781573 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:10 total_loss:39.2428092956543 \n",
      " ari:0.718717869014886 nmi:0.6785650076727363 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:1 total_loss:41.61506652832031 \n",
      " ari:0.6653047321904266 nmi:0.639871160268598 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:2 total_loss:41.322933197021484 \n",
      " ari:0.6679558585303814 nmi:0.647518517402752 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:3 total_loss:41.10618591308594 \n",
      " ari:0.7004245113785106 nmi:0.6519407285544924 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:4 total_loss:40.925601959228516 \n",
      " ari:0.6753866202295363 nmi:0.6440877951401645 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:5 total_loss:40.78301239013672 \n",
      " ari:0.6457540618099842 nmi:0.6232298577030836 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:6 total_loss:40.61640167236328 \n",
      " ari:0.6838032539825524 nmi:0.656598575300995 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:7 total_loss:40.44373321533203 \n",
      " ari:0.6839718381481008 nmi:0.652728744364174 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "epoch:8 total_loss:40.25709533691406 \n",
      " ari:0.7014098373666928 nmi:0.6552440809567409 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:9 total_loss:40.10688018798828 \n",
      " ari:0.681490642957157 nmi:0.6489256974053764 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:10 total_loss:39.97138214111328 \n",
      " ari:0.7145804702237181 nmi:0.6668675844628787 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:1 total_loss:40.48075866699219 \n",
      " ari:0.6792261802740581 nmi:0.6364792388517426 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:2 total_loss:40.225074768066406 \n",
      " ari:0.7002944245809991 nmi:0.654008660719422 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:3 total_loss:40.03935623168945 \n",
      " ari:0.710667091141086 nmi:0.6559449192369544 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:4 total_loss:39.84650421142578 \n",
      " ari:0.6839454450862906 nmi:0.6350095303724937 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:5 total_loss:39.67523956298828 \n",
      " ari:0.6838152020051788 nmi:0.6377290567886811 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:6 total_loss:39.42426300048828 \n",
      " ari:0.6689089143562654 nmi:0.647564844323927 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:7 total_loss:39.22987365722656 \n",
      " ari:0.6742788405830328 nmi:0.6529084567053776 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:8 total_loss:39.034263610839844 \n",
      " ari:0.6747989717737995 nmi:0.6537421337427506 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:9 total_loss:38.8594970703125 \n",
      " ari:0.6774359803308074 nmi:0.6526763966135991 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:10 total_loss:38.68332290649414 \n",
      " ari:0.6606430215151183 nmi:0.6414213775104731 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:1 total_loss:40.37961959838867 \n",
      " ari:0.6991666931194799 nmi:0.6373970456258596 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:2 total_loss:40.1072998046875 \n",
      " ari:0.6631345614126477 nmi:0.6241249679227392 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:3 total_loss:39.93456268310547 \n",
      " ari:0.6908764513066374 nmi:0.6352297940874082 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:4 total_loss:39.8167839050293 \n",
      " ari:0.6989547701956694 nmi:0.6545726366970573 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:5 total_loss:39.60460662841797 \n",
      " ari:0.7197335673221888 nmi:0.6670115253089856 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:6 total_loss:39.425750732421875 \n",
      " ari:0.6769287260672531 nmi:0.6427241739759617 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:7 total_loss:39.25422668457031 \n",
      " ari:0.6864358564970088 nmi:0.643503618003481 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:8 total_loss:39.09553527832031 \n",
      " ari:0.6859330775442545 nmi:0.6485600698921 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:9 total_loss:38.924224853515625 \n",
      " ari:0.7049478032374912 nmi:0.6701523644958725 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:10 total_loss:38.70829772949219 \n",
      " ari:0.6982653038171492 nmi:0.6617320448626365 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:1 total_loss:40.472347259521484 \n",
      " ari:0.6797427628459712 nmi:0.639802165916603 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:2 total_loss:40.25220489501953 \n",
      " ari:0.6793494583006049 nmi:0.6423610592187475 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:3 total_loss:40.098899841308594 \n",
      " ari:0.6117384477365004 nmi:0.6145184209124128 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:4 total_loss:39.889984130859375 \n",
      " ari:0.5972674111422638 nmi:0.6105068561173675 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:5 total_loss:39.71095657348633 \n",
      " ari:0.6130605418035894 nmi:0.6191475795371886 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:6 total_loss:39.50186538696289 \n",
      " ari:0.6240879500300728 nmi:0.5948982707407945 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:7 total_loss:39.30784225463867 \n",
      " ari:0.6324398637563198 nmi:0.6361037444014052 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:8 total_loss:39.15715026855469 \n",
      " ari:0.5963718299024655 nmi:0.6016971602664419 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:9 total_loss:38.952083587646484 \n",
      " ari:0.6083759782373706 nmi:0.6124260982815857 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:10 total_loss:38.75984573364258 \n",
      " ari:0.6044404718441835 nmi:0.6087147302049337 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:1 total_loss:41.41080856323242 \n",
      " ari:0.7118885877611216 nmi:0.6584617578308665 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:2 total_loss:41.07464599609375 \n",
      " ari:0.7074263389713848 nmi:0.6553397448860788 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:3 total_loss:40.88967514038086 \n",
      " ari:0.7071905998798259 nmi:0.6545215015359241 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:4 total_loss:40.70573043823242 \n",
      " ari:0.7101894451620159 nmi:0.6601585869109314 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:5 total_loss:40.523963928222656 \n",
      " ari:0.7201420976655728 nmi:0.6742146949437675 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:6 total_loss:40.3386344909668 \n",
      " ari:0.7190846672105027 nmi:0.6714158536386828 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:7 total_loss:40.124534606933594 \n",
      " ari:0.7170122824632866 nmi:0.6720774203441233 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:8 total_loss:39.995147705078125 \n",
      " ari:0.7134093802096286 nmi:0.6697169455479981 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:9 total_loss:39.781761169433594 \n",
      " ari:0.723340687868071 nmi:0.6796261471895441 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:10 total_loss:39.66625213623047 \n",
      " ari:0.7201009781686619 nmi:0.6760507775748511 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:1 total_loss:41.04988098144531 \n",
      " ari:0.6825888924939539 nmi:0.6250027495569528 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:2 total_loss:40.798988342285156 \n",
      " ari:0.6770209436398789 nmi:0.6216049360214704 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:3 total_loss:40.55727767944336 \n",
      " ari:0.6779028758602857 nmi:0.6291772894887422 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:4 total_loss:40.39483642578125 \n",
      " ari:0.6710219588676785 nmi:0.6179870493655609 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:5 total_loss:40.21185302734375 \n",
      " ari:0.6308711056766088 nmi:0.605513534029093 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "epoch:6 total_loss:39.98693084716797 \n",
      " ari:0.6539377067044052 nmi:0.6107625434980248 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:7 total_loss:39.82053756713867 \n",
      " ari:0.6564460490134666 nmi:0.6189762111274486 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:8 total_loss:39.67375564575195 \n",
      " ari:0.6414655530433654 nmi:0.6117414467318151 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:9 total_loss:39.43836212158203 \n",
      " ari:0.6653659409063407 nmi:0.6295129788719043 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:10 total_loss:39.28074264526367 \n",
      " ari:0.6448111421646223 nmi:0.618706544182398 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "--- Finished Adam\n",
      "--- Starting Model GA\n",
      "--- -- Finished fitness evaluation, length: 10\n",
      "--- -- Finished creating offspring population\n",
      "--- -- Finished evaluating fitness of offspring population\n",
      "--- Finished Model GA,Time:5630.141258239746ms\n",
      "Currently in generation 2\n",
      "--- Starting Adam\n",
      "------------------------------\n",
      "epoch:1 total_loss:38.55843734741211 \n",
      " ari:0.6514178374309706 nmi:0.636890764263017 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:2 total_loss:38.220298767089844 \n",
      " ari:0.6455103001758802 nmi:0.6349216264699717 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:3 total_loss:38.03727340698242 \n",
      " ari:0.6390671623108751 nmi:0.6354041931445282 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:4 total_loss:37.83135223388672 \n",
      " ari:0.6290316539695205 nmi:0.6352743468868444 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:5 total_loss:37.72383499145508 \n",
      " ari:0.6187634722855597 nmi:0.6302210263201748 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:6 total_loss:37.45149230957031 \n",
      " ari:0.5938667706279306 nmi:0.6283079137368803 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:7 total_loss:37.23828125 \n",
      " ari:0.601275940147214 nmi:0.6306041146532527 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:8 total_loss:37.001991271972656 \n",
      " ari:0.5745921090924113 nmi:0.6224679609492959 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:9 total_loss:36.754634857177734 \n",
      " ari:0.5370657653779015 nmi:0.6020595108414416 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:10 total_loss:36.53861618041992 \n",
      " ari:0.5388948277753024 nmi:0.6088149743946691 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:1 total_loss:38.64171600341797 \n",
      " ari:0.7163491988080842 nmi:0.6800238582200309 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:2 total_loss:38.40037536621094 \n",
      " ari:0.7173533061710565 nmi:0.6840424381043957 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:3 total_loss:38.18965148925781 \n",
      " ari:0.7115351469803961 nmi:0.6807318493623975 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:4 total_loss:38.024566650390625 \n",
      " ari:0.7081316353156366 nmi:0.6721925772814913 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:5 total_loss:37.84613037109375 \n",
      " ari:0.706596735434961 nmi:0.6794036907556751 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:6 total_loss:37.69076919555664 \n",
      " ari:0.7007884119341551 nmi:0.6767689530107485 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:7 total_loss:37.46506881713867 \n",
      " ari:0.6539610843444255 nmi:0.642441371252795 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:8 total_loss:37.29517364501953 \n",
      " ari:0.6971558574870625 nmi:0.6796550940766961 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:9 total_loss:37.09461975097656 \n",
      " ari:0.6567455862969068 nmi:0.6458607708883504 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:10 total_loss:36.939815521240234 \n",
      " ari:0.688901196230305 nmi:0.6670620646010756 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:1 total_loss:38.72780990600586 \n",
      " ari:0.5942761112479205 nmi:0.5982444373792932 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:2 total_loss:38.43621063232422 \n",
      " ari:0.6211327383708284 nmi:0.6327465689342612 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:3 total_loss:38.275753021240234 \n",
      " ari:0.6473769194080795 nmi:0.6383794154795939 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:4 total_loss:38.05574035644531 \n",
      " ari:0.6169949742121642 nmi:0.6468932280706061 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:5 total_loss:37.90239334106445 \n",
      " ari:0.6509206424751097 nmi:0.6582105848132604 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:6 total_loss:37.72933578491211 \n",
      " ari:0.6089581435057522 nmi:0.6522576097258441 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:7 total_loss:37.566688537597656 \n",
      " ari:0.6081429562998322 nmi:0.650338476086592 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:8 total_loss:37.39185333251953 \n",
      " ari:0.5933584111231365 nmi:0.6224364189254274 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:9 total_loss:37.21983337402344 \n",
      " ari:0.6390787426287312 nmi:0.6426075832498157 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:10 total_loss:37.008148193359375 \n",
      " ari:0.5969666751724052 nmi:0.626340413737796 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:1 total_loss:39.20684814453125 \n",
      " ari:0.7185544783147646 nmi:0.6796900329013307 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:2 total_loss:38.86717224121094 \n",
      " ari:0.7141968049802709 nmi:0.6742764183958436 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:3 total_loss:38.72842788696289 \n",
      " ari:0.7051257997673659 nmi:0.6670066591209506 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:4 total_loss:38.480403900146484 \n",
      " ari:0.6988210415455303 nmi:0.6798894361710062 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:5 total_loss:38.28446960449219 \n",
      " ari:0.7102866853695239 nmi:0.6795995212780115 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:6 total_loss:38.11672592163086 \n",
      " ari:0.7161580592405309 nmi:0.6837419827735145 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:7 total_loss:37.918670654296875 \n",
      " ari:0.718880370912248 nmi:0.6879594012709149 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:8 total_loss:37.741905212402344 \n",
      " ari:0.7280771902270801 nmi:0.6924409032777369 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:9 total_loss:37.56020736694336 \n",
      " ari:0.7025033828179333 nmi:0.6787676839401049 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:10 total_loss:37.33971405029297 \n",
      " ari:0.6951708337175849 nmi:0.6861561398927625 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:1 total_loss:40.21118927001953 \n",
      " ari:0.5989068284464494 nmi:0.5646820510634183 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "epoch:2 total_loss:40.12852096557617 \n",
      " ari:0.6388580414220764 nmi:0.5842599811574453 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:3 total_loss:39.989097595214844 \n",
      " ari:0.63693221478141 nmi:0.5838614857592129 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:4 total_loss:39.94097900390625 \n",
      " ari:0.5757647858949668 nmi:0.562042331662251 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:5 total_loss:39.881500244140625 \n",
      " ari:0.6143757347043066 nmi:0.5760879233412888 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:6 total_loss:39.804710388183594 \n",
      " ari:0.6418377385908237 nmi:0.5983910618949444 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:7 total_loss:39.724403381347656 \n",
      " ari:0.6032398445090031 nmi:0.5876856751554262 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:8 total_loss:39.644287109375 \n",
      " ari:0.607652009209867 nmi:0.5847570878272101 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:9 total_loss:39.57439041137695 \n",
      " ari:0.5204553273330252 nmi:0.5716858067846683 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:10 total_loss:39.47828674316406 \n",
      " ari:0.6058458610243023 nmi:0.5976344888238789 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:1 total_loss:39.84104537963867 \n",
      " ari:0.6891978041979362 nmi:0.6701881722068448 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:2 total_loss:39.630226135253906 \n",
      " ari:0.6884005026531342 nmi:0.6606429820445486 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:3 total_loss:39.43478775024414 \n",
      " ari:0.6786170121904194 nmi:0.6562756771644875 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:4 total_loss:39.27420425415039 \n",
      " ari:0.7170407222180708 nmi:0.677725699749175 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:5 total_loss:39.1369743347168 \n",
      " ari:0.6402260934848049 nmi:0.6477366601730787 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:6 total_loss:38.934478759765625 \n",
      " ari:0.6601127658111424 nmi:0.6487897016933505 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:7 total_loss:38.802974700927734 \n",
      " ari:0.6683225770508167 nmi:0.6457216833935138 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:8 total_loss:38.6340217590332 \n",
      " ari:0.6893504145962331 nmi:0.66052356133147 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:9 total_loss:38.448081970214844 \n",
      " ari:0.6848572952296506 nmi:0.6722197976089824 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:10 total_loss:38.30663299560547 \n",
      " ari:0.6948324712248236 nmi:0.6715633727497795 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:1 total_loss:40.16089630126953 \n",
      " ari:0.712976494203762 nmi:0.6681163124550494 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:2 total_loss:40.10212707519531 \n",
      " ari:0.7188327637087585 nmi:0.6664751878502414 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:3 total_loss:40.04224395751953 \n",
      " ari:0.7125307889929262 nmi:0.6535487473186323 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:4 total_loss:39.97266387939453 \n",
      " ari:0.7146424649181242 nmi:0.6668739305927798 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:5 total_loss:39.91665267944336 \n",
      " ari:0.7281914652987425 nmi:0.6723796276374249 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:6 total_loss:39.8212890625 \n",
      " ari:0.719339618493596 nmi:0.6692066871449308 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:7 total_loss:39.78684616088867 \n",
      " ari:0.7217375353199035 nmi:0.6668065000053104 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:8 total_loss:39.732303619384766 \n",
      " ari:0.7012913106915164 nmi:0.6595480493493422 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:9 total_loss:39.647865295410156 \n",
      " ari:0.7037351593239684 nmi:0.6602748136178449 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:10 total_loss:39.54763412475586 \n",
      " ari:0.7034009603570688 nmi:0.6638721229875665 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:1 total_loss:39.57319641113281 \n",
      " ari:0.72195611670823 nmi:0.6786303722769256 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:2 total_loss:39.218589782714844 \n",
      " ari:0.7046601643116377 nmi:0.6695871010546967 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:3 total_loss:39.047054290771484 \n",
      " ari:0.7126802918567278 nmi:0.6758995471598019 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:4 total_loss:38.859039306640625 \n",
      " ari:0.6665511697239236 nmi:0.6531722602466948 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:5 total_loss:38.75927734375 \n",
      " ari:0.6659515378959192 nmi:0.6559107020663733 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:6 total_loss:38.55031967163086 \n",
      " ari:0.6710515028585058 nmi:0.6604687733291202 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:7 total_loss:38.39137268066406 \n",
      " ari:0.6949229101988019 nmi:0.6774310216795407 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:8 total_loss:38.17655563354492 \n",
      " ari:0.7109548275068392 nmi:0.6804633113317776 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:9 total_loss:37.99816131591797 \n",
      " ari:0.675771882897974 nmi:0.6748415956848667 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:10 total_loss:37.75926208496094 \n",
      " ari:0.7095819595525426 nmi:0.6919056483609071 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:1 total_loss:40.11777877807617 \n",
      " ari:0.7104532971709503 nmi:0.6715700979771354 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:2 total_loss:40.02979278564453 \n",
      " ari:0.7000008651441445 nmi:0.6607081297616553 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:3 total_loss:39.93751525878906 \n",
      " ari:0.6954894160023888 nmi:0.6648605964497487 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:4 total_loss:39.85883331298828 \n",
      " ari:0.5167887393826585 nmi:0.5928485762865714 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:5 total_loss:39.77483367919922 \n",
      " ari:0.6888129468905034 nmi:0.6613703675269977 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:6 total_loss:39.69552230834961 \n",
      " ari:0.6959739309912994 nmi:0.6678564490321942 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:7 total_loss:39.672607421875 \n",
      " ari:0.7000385195308811 nmi:0.6714434600279162 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:8 total_loss:39.544246673583984 \n",
      " ari:0.5324147564988964 nmi:0.6044097022654303 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:9 total_loss:39.47404098510742 \n",
      " ari:0.5258423520058042 nmi:0.6035382252056349 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "epoch:10 total_loss:39.390098571777344 \n",
      " ari:0.5422762875619499 nmi:0.612691487686512 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:1 total_loss:40.12989807128906 \n",
      " ari:0.6468395398462696 nmi:0.601528718735103 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:2 total_loss:40.08815002441406 \n",
      " ari:0.6563523414006311 nmi:0.6084638256340459 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:3 total_loss:39.968536376953125 \n",
      " ari:0.6521892488581041 nmi:0.6045517224191183 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:4 total_loss:39.86040115356445 \n",
      " ari:0.6662159692742389 nmi:0.6341282874358692 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:5 total_loss:39.7926139831543 \n",
      " ari:0.6408539356014195 nmi:0.6096719714566855 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:6 total_loss:39.72953796386719 \n",
      " ari:0.6384145590364431 nmi:0.6222444571065223 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:7 total_loss:39.64497375488281 \n",
      " ari:0.6408511949281451 nmi:0.604121575719509 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:8 total_loss:39.527496337890625 \n",
      " ari:0.6169483000887047 nmi:0.6109905035946619 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:9 total_loss:39.41029357910156 \n",
      " ari:0.6563592402164271 nmi:0.615832010284831 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:10 total_loss:39.31658172607422 \n",
      " ari:0.6414687491193486 nmi:0.6105233895802531 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "--- Finished Adam\n",
      "--- Starting Model GA\n",
      "--- -- Finished fitness evaluation, length: 10\n",
      "--- -- Finished creating offspring population\n",
      "--- -- Finished evaluating fitness of offspring population\n",
      "--- Finished Model GA,Time:5529.1993618011475ms\n",
      "Currently in generation 3\n",
      "--- Starting Adam\n",
      "------------------------------\n",
      "epoch:1 total_loss:36.478355407714844 \n",
      " ari:0.5631442847359325 nmi:0.6237434212309427 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:2 total_loss:36.143943786621094 \n",
      " ari:0.560547781078999 nmi:0.6222866063418911 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:3 total_loss:35.94490051269531 \n",
      " ari:0.5073467737634159 nmi:0.5905941627156551 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:4 total_loss:35.722774505615234 \n",
      " ari:0.565066306838453 nmi:0.6095875106250265 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:5 total_loss:35.45140838623047 \n",
      " ari:0.5201640523767367 nmi:0.6030551721404254 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:6 total_loss:35.26148986816406 \n",
      " ari:0.5347897463804235 nmi:0.6016797578228754 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:7 total_loss:35.06066131591797 \n",
      " ari:0.47422553129636863 nmi:0.5827491528743077 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:8 total_loss:34.80296325683594 \n",
      " ari:0.47522731945165314 nmi:0.5848186068546651 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:9 total_loss:34.56842803955078 \n",
      " ari:0.4718321120496861 nmi:0.5817300186905067 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:10 total_loss:34.38142013549805 \n",
      " ari:0.46664020663714045 nmi:0.5858046181020622 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:1 total_loss:36.89132308959961 \n",
      " ari:0.6485992173174537 nmi:0.6524144714276998 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:2 total_loss:36.571895599365234 \n",
      " ari:0.6766100158181949 nmi:0.6659167850947104 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:3 total_loss:36.357852935791016 \n",
      " ari:0.6433786852816373 nmi:0.6602207062781957 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:4 total_loss:36.18610763549805 \n",
      " ari:0.6955025103726145 nmi:0.6858898148013299 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:5 total_loss:35.981666564941406 \n",
      " ari:0.6939067463191134 nmi:0.6980238622442053 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:6 total_loss:35.852718353271484 \n",
      " ari:0.5643209940031886 nmi:0.6419192358955844 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:7 total_loss:35.66680145263672 \n",
      " ari:0.5611120487408249 nmi:0.6344785086585836 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:8 total_loss:35.43729019165039 \n",
      " ari:0.5432724116940731 nmi:0.6385014081693714 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:9 total_loss:35.249595642089844 \n",
      " ari:0.647347942288502 nmi:0.6648118963627478 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:10 total_loss:35.05380630493164 \n",
      " ari:0.5620020376106973 nmi:0.6395244512899498 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:1 total_loss:36.977325439453125 \n",
      " ari:0.49409748217312993 nmi:0.5927041156825866 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:2 total_loss:36.665260314941406 \n",
      " ari:0.4954819133704473 nmi:0.5975244014741461 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:3 total_loss:36.51045227050781 \n",
      " ari:0.5978519504379411 nmi:0.6216152727536741 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n",
      "------------------------------\n",
      "epoch:4 total_loss:36.31727981567383 \n",
      " ari:0.500404562727151 nmi:0.5990444289871015 \n",
      " best ari:0.7514624178784669 bset nmi:0.6915206707031415\n"
     ]
    }
   ],
   "source": [
    "Train_start = time.time()\n",
    "trained_population = GA_Neural_train(population=population,\n",
    "                                    pop_size = pop_size,\n",
    "                                    max_generations=max_generations,\n",
    "                                    SGD_steps=SGD_steps,GA_steps=GA_steps,\n",
    "                                    offspring_size=offspring_size,elitist_level=elitist_level,rho=rho,\n",
    "                                    learning_rate=learning_rate,\n",
    "                                    dataloader=dataloader,clusters_true=clusters_true,gene_matrix=gene_matrix)\n",
    "Train_end = time.time()\n",
    "print(f\"All Time:{(Train_end-Train_start)*1000}ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d918e0cc35f9abef77c217ed8b9a5d2b42afae65fd91b0eb3e5d71ebfa29c6f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
