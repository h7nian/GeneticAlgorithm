{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f045d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from torch.utils.data import random_split,Dataset,DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import tarfile\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "from scipy.sparse import csr_matrix, vstack, save_npz\n",
    "from sklearn.decomposition import PCA\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2d94e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.contrib.sampling import NeighborSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c89f7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13f729ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.version' from '/home/chenhuaguan/.conda/envs/py36/lib/python3.6/site-packages/torch/version.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75d6de9",
   "metadata": {},
   "source": [
    "# Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c953db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20968a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gene = pd.read_csv(\"data/mouse_Fetal_brain4369_data.csv\",index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25c622ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_1</th>\n",
       "      <th>C_2</th>\n",
       "      <th>C_3</th>\n",
       "      <th>C_4</th>\n",
       "      <th>C_5</th>\n",
       "      <th>C_6</th>\n",
       "      <th>C_7</th>\n",
       "      <th>C_8</th>\n",
       "      <th>C_9</th>\n",
       "      <th>C_10</th>\n",
       "      <th>...</th>\n",
       "      <th>C_4360</th>\n",
       "      <th>C_4361</th>\n",
       "      <th>C_4362</th>\n",
       "      <th>C_4363</th>\n",
       "      <th>C_4364</th>\n",
       "      <th>C_4365</th>\n",
       "      <th>C_4366</th>\n",
       "      <th>C_4367</th>\n",
       "      <th>C_4368</th>\n",
       "      <th>C_4369</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0610009B22Rik</th>\n",
       "      <td>2.516851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.691593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.623779</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.82721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0610009E02Rik</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0610009L18Rik</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.673823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.012705</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0610010F05Rik</th>\n",
       "      <td>2.516851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.623779</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.815324</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0610010K14Rik</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.964363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tut4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.623779</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rtl4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rtl3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tut7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zup1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.590051</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18226 rows × 4369 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    C_1       C_2  C_3  C_4       C_5  C_6  C_7  C_8  C_9  \\\n",
       "0610009B22Rik  2.516851  0.000000  0.0  0.0  2.691593  0.0  0.0  0.0  0.0   \n",
       "0610009E02Rik  0.000000  0.000000  0.0  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "0610009L18Rik  0.000000  0.000000  0.0  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "0610010F05Rik  2.516851  0.000000  0.0  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "0610010K14Rik  0.000000  0.000000  0.0  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "...                 ...       ...  ...  ...       ...  ...  ...  ...  ...   \n",
       "Tut4           0.000000  0.000000  0.0  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "Rtl4           0.000000  0.000000  0.0  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "Rtl3           0.000000  0.000000  0.0  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "Tut7           0.000000  0.000000  0.0  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "Zup1           0.000000  2.590051  0.0  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "\n",
       "               C_10  ...  C_4360    C_4361  C_4362    C_4363    C_4364  \\\n",
       "0610009B22Rik   0.0  ...     0.0  2.623779     0.0  0.000000  0.000000   \n",
       "0610009E02Rik   0.0  ...     0.0  0.000000     0.0  0.000000  0.000000   \n",
       "0610009L18Rik   0.0  ...     0.0  0.000000     0.0  0.000000  2.673823   \n",
       "0610010F05Rik   0.0  ...     0.0  2.623779     0.0  2.815324  0.000000   \n",
       "0610010K14Rik   0.0  ...     0.0  0.000000     0.0  0.000000  0.000000   \n",
       "...             ...  ...     ...       ...     ...       ...       ...   \n",
       "Tut4            0.0  ...     0.0  2.623779     0.0  0.000000  0.000000   \n",
       "Rtl4            0.0  ...     0.0  0.000000     0.0  0.000000  0.000000   \n",
       "Rtl3            0.0  ...     0.0  0.000000     0.0  0.000000  0.000000   \n",
       "Tut7            0.0  ...     0.0  0.000000     0.0  0.000000  0.000000   \n",
       "Zup1            0.0  ...     0.0  0.000000     0.0  0.000000  0.000000   \n",
       "\n",
       "               C_4365    C_4366   C_4367  C_4368    C_4369  \n",
       "0610009B22Rik     0.0  0.000000  2.82721     0.0  0.000000  \n",
       "0610009E02Rik     0.0  0.000000  0.00000     0.0  0.000000  \n",
       "0610009L18Rik     0.0  3.012705  0.00000     0.0  0.000000  \n",
       "0610010F05Rik     0.0  0.000000  0.00000     0.0  0.000000  \n",
       "0610010K14Rik     0.0  0.000000  0.00000     0.0  2.964363  \n",
       "...               ...       ...      ...     ...       ...  \n",
       "Tut4              0.0  0.000000  0.00000     0.0  0.000000  \n",
       "Rtl4              0.0  0.000000  0.00000     0.0  0.000000  \n",
       "Rtl3              0.0  0.000000  0.00000     0.0  0.000000  \n",
       "Tut7              0.0  0.000000  0.00000     0.0  0.000000  \n",
       "Zup1              0.0  0.000000  0.00000     0.0  0.000000  \n",
       "\n",
       "[18226 rows x 4369 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7094d6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2gene = data_gene.index.values.tolist()\n",
    "id2gene.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2cc9cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cell = pd.read_csv(\"data/mouse_Fetal_brain4369_celltype.csv\", dtype=np.str, header=0,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac0b9143",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = set()\n",
    "cell_type_list = []\n",
    "data_cell['Cell_type'] = data_cell['Cell_type'].map(str.strip)\n",
    "cell_types = set(data_cell.values[:, 1])\n",
    "cell_type_list.extend(data_cell.values[:, 1].tolist())\n",
    "id2label = list(cell_types)\n",
    "label_statistics = dict(collections.Counter(cell_type_list))\n",
    "total_cell = sum(label_statistics.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb65084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, num in label_statistics.items():\n",
    "    if num / total_cell <= 0.005:\n",
    "        id2label.remove(label)  # remove exclusive labels\n",
    "gene2id = {gene: idx for idx, gene in enumerate(id2gene)}\n",
    "num_genes = len(id2gene)\n",
    "# prepare unified labels\n",
    "num_labels = len(id2label)\n",
    "label2id = {label: idx for idx, label in enumerate(id2label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c58d198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = dgl.DGLGraph()\n",
    "\n",
    "gene_ids = torch.arange(num_genes, dtype=torch.int32, device=device).unsqueeze(-1)\n",
    "graph.add_nodes(num_genes, {'id': gene_ids})\n",
    "all_labels = []\n",
    "matrices = []\n",
    "num_cells = 0\n",
    "cell2type = pd.read_csv(\"data/mouse_Fetal_brain4369_celltype.csv\",index_col=0)\n",
    "cell2type.columns = ['cell', 'type']\n",
    "cell2type['type'] = cell2type['type'].map(str.strip)\n",
    "cell2type['id'] = cell2type['type'].map(label2id)\n",
    "filter_cell = np.where(pd.isnull(cell2type['id']) == False)[0]\n",
    "cell2type = cell2type.iloc[filter_cell]\n",
    "all_labels += cell2type['id'].tolist()\n",
    "df = pd.read_csv(\"data/mouse_Fetal_brain4369_data.csv\",index_col = 0)\n",
    "df = df.transpose(copy=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e610966",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[filter_cell]\n",
    "df = df.rename(columns=gene2id)\n",
    "col = [c for c in df.columns if c in gene2id.values()]\n",
    "df = df[col]\n",
    "arr = df.to_numpy()\n",
    "row_idx, col_idx = np.nonzero(arr > 0)  # intra-dataset index\n",
    "non_zeros = arr[(row_idx, col_idx)]  # non-zero values\n",
    "cell_idx = row_idx + graph.number_of_nodes()  # cell_index\n",
    "gene_idx = df.columns[col_idx].astype(int).tolist()  # gene_index\n",
    "info_shape = (len(df), num_genes)\n",
    "info = csr_matrix((non_zeros, (row_idx, gene_idx)), shape=info_shape)\n",
    "matrices.append(info)\n",
    "\n",
    "num_cells += len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "697ee87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = torch.tensor([-1] * len(df), dtype=torch.int32, device=device).unsqueeze(-1)\n",
    "graph.add_nodes(len(df), {'id': ids})\n",
    "graph.add_edges(cell_idx, gene_idx,\n",
    "                {'weight': torch.tensor(non_zeros, dtype=torch.float32, device=device).unsqueeze(1)})\n",
    "graph.add_edges(gene_idx, cell_idx,\n",
    "                {'weight': torch.tensor(non_zeros, dtype=torch.float32, device=device).unsqueeze(1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cce041d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_feat = vstack(matrices).toarray()  # cell-wise  (cell, gene)\n",
    "gene_pca = PCA(400, random_state=10086).fit(sparse_feat.T)\n",
    "gene_feat = gene_pca.transform(sparse_feat.T)\n",
    "gene_evr = sum(gene_pca.explained_variance_ratio_) * 100\n",
    "sparse_feat = sparse_feat / (np.sum(sparse_feat, axis=1, keepdims=True) + 1e-6)\n",
    "# use weighted gene_feat as cell_feat\n",
    "cell_feat = sparse_feat.dot(gene_feat)\n",
    "gene_feat = torch.from_numpy(gene_feat)  # use shared storage\n",
    "cell_feat = torch.from_numpy(cell_feat)\n",
    "\n",
    "graph.ndata['features'] = torch.cat([gene_feat, cell_feat], dim=0).type(torch.float).to(device)\n",
    "labels = torch.tensor([-1] * num_genes + all_labels, dtype=torch.long, device=device)  # [gene_num+train_num]\n",
    "per = np.random.permutation(range(num_genes, num_genes + num_cells))\n",
    "test_ids = torch.tensor(per[:int(num_cells // ((1 - 0.2) / 0.2 + 1))]).to(device)\n",
    "train_ids = torch.tensor(per[int(num_cells // ((1 - 0.2) / 0.2 + 1)):]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fe01998",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_degrees = graph.in_degrees()\n",
    "for i in range(graph.number_of_nodes()):\n",
    "    src, dst, in_edge_id = graph.in_edges(i, form='all')\n",
    "    if src.shape[0] == 0:\n",
    "        continue\n",
    "    edge_w = graph.edata['weight'][in_edge_id]\n",
    "    graph.edata['weight'][in_edge_id] = in_degrees[i] * edge_w / torch.sum(edge_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "393ec077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add self-loop\n",
    "graph.add_edges(graph.nodes(), graph.nodes(),\n",
    "                {'weight': torch.ones(graph.number_of_nodes(), dtype=torch.float, device=device).unsqueeze(1)})\n",
    "graph.readonly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19854d3b",
   "metadata": {},
   "source": [
    "# GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca283aa",
   "metadata": {},
   "source": [
    "## NodeUpdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14d198e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeUpdate(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation=None, norm=None):\n",
    "        super(NodeUpdate, self).__init__()\n",
    "        self.fc_neigh = nn.Linear(in_features=in_feats, out_features=out_feats)\n",
    "        self.activation = activation\n",
    "        self.norm = norm\n",
    "        nn.init.xavier_uniform_(self.fc_neigh.weight, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def forward(self, node):\n",
    "        h_neigh = node.data['neigh']\n",
    "        h_neigh = self.fc_neigh(h_neigh)\n",
    "        if self.activation is not None:\n",
    "            h_neigh = self.activation(h_neigh)\n",
    "        if self.norm is not None:\n",
    "            h_neigh = self.norm(h_neigh)\n",
    "        return {'activation': h_neigh}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40651976",
   "metadata": {},
   "source": [
    "## GNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5d444bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_classes, n_layers, gene_num, activation=None, norm=None, dropout=0.0):\n",
    "        super(GNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.gene_num = gene_num\n",
    "        if dropout != 0:\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(NodeUpdate(in_feats=in_feats, out_feats=n_hidden, activation=activation, norm=norm))\n",
    "        for _ in range(n_layers - 1):\n",
    "            self.layers.append(NodeUpdate(in_feats=n_hidden, out_feats=n_hidden, activation=activation, norm=norm))\n",
    "\n",
    "        # [gene_num] is alpha of gene-gene, [gene_num+1] is alpha of cell-cell self loop\n",
    "        self.alpha = nn.Parameter(torch.tensor([1] * (self.gene_num + 2), dtype=torch.float32).unsqueeze(-1))\n",
    "        self.linear = nn.Linear(n_hidden, n_classes)\n",
    "        nn.init.xavier_uniform_(self.linear.weight, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def message_func(self, edges: dgl.udf.EdgeBatch):\n",
    "        number_of_edges = edges.src['h'].shape[0]\n",
    "        indices = np.expand_dims(np.array([self.gene_num + 1] * number_of_edges, dtype=np.int32), axis=1)\n",
    "        src_id, dst_id = edges.src['id'].cpu().numpy(), edges.dst['id'].cpu().numpy()\n",
    "        indices = np.where((src_id >= 0) & (dst_id < 0), src_id, indices)  # gene->cell\n",
    "        indices = np.where((dst_id >= 0) & (src_id < 0), dst_id, indices)  # cell->gene\n",
    "        indices = np.where((dst_id >= 0) & (src_id >= 0), self.gene_num, indices)  # gene-gene\n",
    "        h = edges.src['h'] * self.alpha[indices.squeeze()]\n",
    "        # return {'m': h}\n",
    "        return {'m': h * edges.data['weight']}\n",
    "\n",
    "    def forward(self, nf: dgl.NodeFlow):\n",
    "        nf.layers[0].data['activation'] = nf.layers[0].data['features']\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            h = nf.layers[i].data.pop('activation')\n",
    "            if self.dropout:\n",
    "                h = self.dropout(h)\n",
    "            nf.layers[i].data['h'] = h\n",
    "            nf.block_compute(i, self.message_func, fn.mean('m', 'neigh'), layer)\n",
    "        h = nf.layers[-1].data.pop('activation')\n",
    "        h = self.linear(h)\n",
    "        return h\n",
    "\n",
    "    def evaluate(self, nf: dgl.NodeFlow):\n",
    "        def message_func(edges: dgl.EdgeBatch):\n",
    "            # edges.src['h']： (number of edges, feature dim)\n",
    "            number_of_edges = edges.src['h'].shape[0]\n",
    "            indices = np.expand_dims(np.array([self.gene_num + 1] * number_of_edges, dtype=np.int32), axis=1)\n",
    "            src_id, dst_id = edges.src['id'].cpu().numpy(), edges.dst['id'].cpu().numpy()\n",
    "            indices = np.where((src_id >= 0) & (dst_id < 0), src_id, indices)  # gene->cell\n",
    "            indices = np.where((dst_id >= 0) & (src_id < 0), dst_id, indices)  # cell->gene\n",
    "            indices = np.where((dst_id >= 0) & (src_id >= 0), self.gene_num, indices)  # gene-gene\n",
    "            h = edges.src['h'].cpu() * self.alpha[indices.squeeze()]\n",
    "            return {'m': h * edges.data['weight'].cpu()}\n",
    "\n",
    "        nf.layers[0].data['activation'] = nf.layers[0].data['features'].cpu()\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            h = nf.layers[i].data.pop('activation')\n",
    "            if self.dropout:\n",
    "                h = self.dropout(h)\n",
    "            nf.layers[i].data['h'] = h\n",
    "            nf.block_compute(i, message_func, fn.mean('m', 'neigh'), layer)\n",
    "        h = nf.layers[-1].data.pop('activation')\n",
    "        h = self.linear(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e5518d",
   "metadata": {},
   "source": [
    "# Adam_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c7bfc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam_Trainer:\n",
    "    def __init__(self, device_train,gnn_model,lr,weight_decay,Adam_epochs,batch_size,\n",
    "                num_cells,\n",
    "                num_genes,\n",
    "                num_labels,\n",
    "                graph,\n",
    "                train_ids,\n",
    "                test_ids,\n",
    "                labels):\n",
    "        \n",
    "        self.device = device_train\n",
    "        \n",
    "        self.num_cells = num_cells\n",
    "        self.num_genes = num_genes\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        self.graph = graph\n",
    "        \n",
    "        self.graph.readonly(True)\n",
    "        \n",
    "        self.train_ids = train_ids\n",
    "        self.test_ids = test_ids\n",
    "        self.labels = labels \n",
    "        \n",
    "        self.labels = self.labels.to(self.device)\n",
    "        \n",
    "        self.model = gnn_model\n",
    "        \n",
    "        self.lr = lr\n",
    "        \n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        self.epochs = Adam_epochs\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr,\n",
    "                                          weight_decay=self.weight_decay)\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "        \n",
    "        self.num_neighbors = self.num_cells + self.num_genes\n",
    "\n",
    "    def fit(self):\n",
    "        max_test_acc, _train_acc, _epoch = 0, 0, 0\n",
    "        for epoch in range(self.epochs):\n",
    "            loss = self.train()\n",
    "            train_correct, train_unsure = self.evaluate(self.train_ids, 'train')\n",
    "            train_acc = train_correct / len(self.train_ids)\n",
    "            test_correct, test_unsure = self.evaluate(self.test_ids, 'test')\n",
    "            test_acc = test_correct / len(self.test_ids)\n",
    "            if max_test_acc <= test_acc:\n",
    "                final_test_correct_num = test_correct\n",
    "                final_test_unsure_num = test_unsure\n",
    "                _train_acc = train_acc\n",
    "                _epoch = epoch\n",
    "                max_test_acc = test_acc\n",
    "                self.save_model()\n",
    "            print(\n",
    "                f\">>>>Epoch {epoch+1:04d}: Train Acc {train_acc:.4f}, Loss {loss / len(self.train_ids):.4f}, Test correct {test_correct}, \"\n",
    "                f\"Test unsure {test_unsure}, Test Acc {test_acc:.4f}\")\n",
    "            if train_acc == 1:\n",
    "                break\n",
    "\n",
    "        #print(f\"---{self.params.species} {self.params.tissue} Best test result:---\")\n",
    "        print(f\"Epoch {_epoch+1:04d}, Train Acc {_train_acc:.4f}, Test Correct Num {final_test_correct_num}, Test Total Num {len(self.test_ids)}, Test Unsure Num {final_test_unsure_num}, Test Acc {final_test_correct_num / len(self.test_ids):.4f}\")\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        for batch, nf in enumerate(NeighborSampler(g=self.graph,\n",
    "                                                   batch_size=self.batch_size,\n",
    "                                                   expand_factor=self.num_neighbors,\n",
    "                                                   num_hops=1,\n",
    "                                                   neighbor_type='in',\n",
    "                                                   shuffle=True,\n",
    "                                                   num_workers=8,\n",
    "                                                   seed_nodes=self.train_ids.long())):\n",
    "            nf.copy_from_parent()  # Copy node/edge features from the parent graph.\n",
    "            logits = self.model(nf)\n",
    "            batch_nids = nf.layer_parent_nid(-1).type(torch.long).to(device=self.device)\n",
    "            loss = self.loss_fn(logits, self.labels[batch_nids])\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def evaluate(self, ids, type='test'):\n",
    "        self.model.eval()\n",
    "        total_correct, total_unsure = 0, 0\n",
    "        for nf in NeighborSampler(g=self.graph,\n",
    "                                  batch_size=self.batch_size,\n",
    "                                  expand_factor=self.num_cells + self.num_genes,\n",
    "                                  num_hops=1,\n",
    "                                  neighbor_type='in',\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=8,\n",
    "                                  seed_nodes=ids.long()):\n",
    "            nf.copy_from_parent()  # Copy node/edge features from the parent graph.\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(nf).cpu()\n",
    "            batch_nids = nf.layer_parent_nid(-1).type(torch.long)\n",
    "            logits = nn.functional.softmax(logits, dim=1).numpy()\n",
    "            label_list = self.labels.cpu()[batch_nids]\n",
    "            for pred, label in zip(logits, label_list):\n",
    "                max_prob = pred.max().item()\n",
    "                if max_prob < 2 / num_labels:\n",
    "                    total_unsure += 1\n",
    "                elif pred.argmax().item() == label:\n",
    "                    total_correct += 1\n",
    "\n",
    "        return total_correct, total_unsure\n",
    "\n",
    "    def save_model(self):\n",
    "        state = {\n",
    "            'model': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict()\n",
    "        }\n",
    "\n",
    "        torch.save(state, \"GA_Model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9cbc05",
   "metadata": {},
   "source": [
    "# GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36e06989",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def crossover_and_mutation(parents, sigma=0.5):\n",
    "\n",
    "    \n",
    "    base_sd = parents[0].state_dict()\n",
    "    keys = base_sd                    # use all layers to be affected\n",
    "    \n",
    "    # Sum of the weights of the parent\n",
    "    for i in range(1, len(parents)):\n",
    "        parent_sd = parents[i].state_dict()\n",
    "        for key in keys:\n",
    "            base_sd[key] = base_sd[key] + parent_sd[key]\n",
    "            \n",
    "    \n",
    "    # Average and add mutation\n",
    "    num_parents = len(parents)\n",
    "    \n",
    "    for key in keys:\n",
    "        \n",
    "        tensor_size = base_sd[key].size()\n",
    "        random_tensor = torch.normal(mean=0.0, std=sigma, size=tensor_size)\n",
    "        \n",
    "        base_sd[key] = (base_sd[key] / num_parents) + random_tensor\n",
    "    \n",
    "    # create offspring\n",
    "    offspring = GNN(in_feats=in_feats,\n",
    "                         n_hidden=n_hidden,\n",
    "                         n_classes=num_labels,\n",
    "                         n_layers=1,\n",
    "                         gene_num=num_genes,\n",
    "                         activation=F.relu,\n",
    "                         dropout=0.1).to(device_train)\n",
    "    \n",
    "    offspring.load_state_dict(base_sd)\n",
    "    \n",
    "    return offspring\n",
    "    \n",
    "\n",
    "def create_offspring(population, fitness, rho, sigma):\n",
    "\n",
    "    \n",
    "    # Perform selection\n",
    "    parents = random.choices(population,weights=fitness, k=rho) \n",
    "    \n",
    "    # Perform crossover and mutation\n",
    "    offspring = crossover_and_mutation(parents, sigma)\n",
    "    \n",
    "    \n",
    "    return offspring\n",
    "\n",
    "\n",
    "def GA_training(population, pop_size, offspring_size, elitist_level, rho, sigma, train_ids,\n",
    "                test_ids,\n",
    "                graph,\n",
    "                batch_size,\n",
    "                num_cells,\n",
    "                num_genes,\n",
    "                labels):\n",
    "    \n",
    "    #Calculate fitness of trained population\n",
    "\n",
    "    fitness = [calc_loss(population[i],train_ids,graph,batch_size,num_cells,num_genes,labels) for i in range(pop_size)]\n",
    "    \n",
    "    print(f\"--- -- Finished fitness evaluation, length: {len(fitness)}\")\n",
    "    \n",
    "    #Create offspring population\n",
    "    \n",
    "    fitness_weighted = fitness\n",
    "    offspring_population = [create_offspring(population, fitness_weighted, rho, sigma) for i in range(offspring_size)]\n",
    "    \n",
    "    print(\"--- -- Finished creating offspring population\")\n",
    "    \n",
    "    #Evaluate fitness of offsprings \n",
    "    \n",
    "    offspring_fitness = [calc_loss(offspring_population[i],test_ids,graph,batch_size,num_cells,num_genes,labels) for i in range(offspring_size)]\n",
    "    \n",
    "    print(\"--- -- Finished evaluating fitness of offspring population\")\n",
    "    \n",
    "    # Combine fitness and population lists\n",
    "    \n",
    "    combined_fitness = fitness + offspring_fitness\n",
    "    combined_population = population + offspring_population\n",
    "    \n",
    "    # sort and select population by their fitness values\n",
    "    \n",
    "    sorted_population = [pop for _, pop in sorted(zip(combined_fitness, combined_population), key=lambda pair: pair[0])]\n",
    "    sorted_fitness = [loss for loss, _ in sorted(zip(combined_fitness, combined_population), key=lambda pair: pair[0])]\n",
    "    \n",
    "    m = int(pop_size * elitist_level)\n",
    "    new_population = sorted_population[0:m]\n",
    "    \n",
    "    # Fill up rest of population\n",
    "    difference = pop_size - m\n",
    "    remaining_population = list(set(sorted_population) - set(new_population))\n",
    "    filler_population = random.sample(remaining_population, difference)\n",
    "    \n",
    "    # assemble new population and return\n",
    "    new_population = new_population + filler_population\n",
    "    \n",
    "    return new_population, sorted_fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d538178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(model, train_ids,graph,batch_size,num_cells,num_genes,labels):\n",
    "    model.eval()\n",
    "    total_correct, total_unsure = 0, 0\n",
    "    for nf in NeighborSampler(g=graph,\n",
    "                              batch_size=batch_size,\n",
    "                              expand_factor=num_cells + num_genes,\n",
    "                              num_hops=1,\n",
    "                              neighbor_type='in',\n",
    "                              shuffle=True,\n",
    "                              num_workers=8,\n",
    "                              seed_nodes=train_ids.long()):\n",
    "        nf.copy_from_parent()  # Copy node/edge features from the parent graph.\n",
    "        with torch.no_grad():\n",
    "            logits = model(nf).cpu()\n",
    "        batch_nids = nf.layer_parent_nid(-1).type(torch.long)\n",
    "        logits = nn.functional.softmax(logits, dim=1).numpy()\n",
    "        label_list = labels.cpu()[batch_nids]\n",
    "        for pred, label in zip(logits, label_list):\n",
    "            max_prob = pred.max().item()\n",
    "            if max_prob < 2 / num_labels:\n",
    "                total_unsure += 1\n",
    "            elif pred.argmax().item() == label:\n",
    "                total_correct += 1\n",
    "\n",
    "    return total_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9657950d",
   "metadata": {},
   "source": [
    "# GA_Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f14faa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GA_Neural_train(population,\n",
    "                    pop_size,\n",
    "                    max_generations, \n",
    "                    Adam_epochs, GA_steps, \n",
    "                    offspring_size, elitist_level, rho,\n",
    "                    learning_rate,\n",
    "                   weight_decay,\n",
    "                   batch_size,\n",
    "                   num_cells,num_genes,graph,train_ids,test_ids,labels):\n",
    "    \n",
    "    print(f\"Starting with population of size: {pop_size}\")\n",
    "    \n",
    "    \n",
    "    for k in range(max_generations):\n",
    "        print(f\"Currently in generation {k+1}\")\n",
    "        \n",
    "        #SGD\n",
    "        print(f\"--- Starting Adam\")\n",
    "        \n",
    "        # Sequential version\n",
    "        #population = [SGD_training(population[i], SGD_steps, learning_rate, 0.9, train_loader) for i in range(pop_size)]\n",
    "        for i in range(pop_size):\n",
    "            train = Adam_Trainer(device_train = device_train,gnn_model = population[i],\n",
    "                                 lr = learning_rate,weight_decay = weight_decay,Adam_epochs = Adam_epochs,\n",
    "                                 batch_size = batch_size,num_cells = num_cells,\n",
    "                                                        num_genes = num_genes,\n",
    "                                                        num_labels = num_labels,\n",
    "                                                        graph = graph,\n",
    "                                                        train_ids = train_ids,\n",
    "                                                        test_ids = test_ids,\n",
    "                                                        labels = labels)\n",
    "            train.fit()\n",
    "        \n",
    "        print(f\"--- Finished Adam\")\n",
    "         \n",
    "        # GA\n",
    "        print(f\"--- Starting GA\")\n",
    "        GA_start = time.time()\n",
    "        sorted_fitness = []          # store the sorted fitness values to maybe use in data collection\n",
    "        for i in range(0, GA_steps):\n",
    "            \n",
    "            sigma = 0.01 / (k+1)\n",
    "            population, sorted_fitness = GA_training(population, pop_size, offspring_size, elitist_level, rho, sigma, train_ids,\n",
    "                                                    test_ids,\n",
    "                                                    graph,\n",
    "                                                    batch_size,\n",
    "                                                    num_cells,\n",
    "                                                    num_genes,\n",
    "                                                    labels)\n",
    "        \n",
    "        GA_end = time.time()\n",
    "        print(f\"--- Finished GA,Time:{(GA_end-GA_start)*1000}ms\")\n",
    "        \n",
    "        \n",
    "    print(f\"Finished training process\")\n",
    "    return population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ab74ac",
   "metadata": {},
   "source": [
    "# Start training process\n",
    "We have now defined the whole training algorithm. The next step is to actually perform training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e61e033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "#device_train = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "device_train = torch.device('cpu')\n",
    "batch_size = 512\n",
    "pop_size = 10\n",
    "max_generations = 100\n",
    "Adam_epochs = 10\n",
    "GA_steps = 1\n",
    "offspring_size = 50\n",
    "elitist_level = 0.6\n",
    "rho = 3\n",
    "learning_rate = 1e-3\n",
    "in_feats = 400\n",
    "n_hidden = 200\n",
    "weight_decay = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67dcc22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create population and start training process\n",
    "population = [GNN(in_feats=in_feats,\n",
    "                         n_hidden=n_hidden,\n",
    "                         n_classes=num_labels,\n",
    "                         n_layers=1,\n",
    "                         gene_num=num_genes,\n",
    "                         activation=F.relu,\n",
    "                         dropout=0.1).to(device_train) for i in range(pop_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fbe831",
   "metadata": {},
   "source": [
    "# Train Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e0c8c7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with population of size: 10\n",
      "Currently in generation 1\n",
      "--- Starting Adam\n",
      ">>>>Epoch 0001: Train Acc 0.1364, Loss 4.3658, Test correct 127, Test unsure 0, Test Acc 0.1455\n",
      ">>>>Epoch 0002: Train Acc 0.1573, Loss 2.7871, Test correct 119, Test unsure 0, Test Acc 0.1363\n",
      ">>>>Epoch 0003: Train Acc 0.2709, Loss 2.4439, Test correct 208, Test unsure 1, Test Acc 0.2383\n",
      ">>>>Epoch 0004: Train Acc 0.3458, Loss 2.1954, Test correct 307, Test unsure 3, Test Acc 0.3517\n",
      ">>>>Epoch 0005: Train Acc 0.4665, Loss 2.0179, Test correct 382, Test unsure 1, Test Acc 0.4376\n",
      ">>>>Epoch 0006: Train Acc 0.4863, Loss 1.8728, Test correct 395, Test unsure 1, Test Acc 0.4525\n",
      ">>>>Epoch 0007: Train Acc 0.5335, Loss 1.7603, Test correct 445, Test unsure 1, Test Acc 0.5097\n",
      ">>>>Epoch 0008: Train Acc 0.5698, Loss 1.6602, Test correct 469, Test unsure 1, Test Acc 0.5372\n",
      ">>>>Epoch 0009: Train Acc 0.5715, Loss 1.5701, Test correct 443, Test unsure 0, Test Acc 0.5074\n",
      ">>>>Epoch 0010: Train Acc 0.6184, Loss 1.4776, Test correct 494, Test unsure 1, Test Acc 0.5659\n",
      "Epoch 0010, Train Acc 0.6184, Test Correct Num 494, Test Total Num 873, Test Unsure Num 1, Test Acc 0.5659\n",
      ">>>>Epoch 0001: Train Acc 0.1862, Loss 6.9605, Test correct 161, Test unsure 0, Test Acc 0.1844\n",
      ">>>>Epoch 0002: Train Acc 0.1925, Loss 4.0109, Test correct 185, Test unsure 0, Test Acc 0.2119\n",
      ">>>>Epoch 0003: Train Acc 0.2451, Loss 2.7710, Test correct 213, Test unsure 0, Test Acc 0.2440\n",
      ">>>>Epoch 0004: Train Acc 0.3498, Loss 2.3708, Test correct 282, Test unsure 1, Test Acc 0.3230\n",
      ">>>>Epoch 0005: Train Acc 0.3587, Loss 2.1030, Test correct 303, Test unsure 1, Test Acc 0.3471\n",
      ">>>>Epoch 0006: Train Acc 0.4213, Loss 1.9428, Test correct 358, Test unsure 0, Test Acc 0.4101\n",
      ">>>>Epoch 0007: Train Acc 0.4751, Loss 1.8056, Test correct 376, Test unsure 0, Test Acc 0.4307\n",
      ">>>>Epoch 0008: Train Acc 0.5438, Loss 1.6791, Test correct 438, Test unsure 1, Test Acc 0.5017\n",
      ">>>>Epoch 0009: Train Acc 0.5787, Loss 1.5768, Test correct 441, Test unsure 0, Test Acc 0.5052\n",
      ">>>>Epoch 0010: Train Acc 0.5809, Loss 1.4755, Test correct 445, Test unsure 0, Test Acc 0.5097\n",
      "Epoch 0010, Train Acc 0.5809, Test Correct Num 445, Test Total Num 873, Test Unsure Num 0, Test Acc 0.5097\n",
      ">>>>Epoch 0001: Train Acc 0.1590, Loss 6.8615, Test correct 139, Test unsure 0, Test Acc 0.1592\n",
      ">>>>Epoch 0002: Train Acc 0.1427, Loss 3.7570, Test correct 134, Test unsure 0, Test Acc 0.1535\n",
      ">>>>Epoch 0003: Train Acc 0.1416, Loss 2.7361, Test correct 115, Test unsure 0, Test Acc 0.1317\n",
      ">>>>Epoch 0004: Train Acc 0.2283, Loss 2.4614, Test correct 191, Test unsure 0, Test Acc 0.2188\n",
      ">>>>Epoch 0005: Train Acc 0.3684, Loss 2.2319, Test correct 313, Test unsure 1, Test Acc 0.3585\n",
      ">>>>Epoch 0006: Train Acc 0.4348, Loss 2.0788, Test correct 353, Test unsure 4, Test Acc 0.4044\n",
      ">>>>Epoch 0007: Train Acc 0.4777, Loss 1.9613, Test correct 384, Test unsure 1, Test Acc 0.4399\n",
      ">>>>Epoch 0008: Train Acc 0.4542, Loss 1.8515, Test correct 393, Test unsure 2, Test Acc 0.4502\n",
      ">>>>Epoch 0009: Train Acc 0.5180, Loss 1.7460, Test correct 412, Test unsure 0, Test Acc 0.4719\n",
      ">>>>Epoch 0010: Train Acc 0.5415, Loss 1.6500, Test correct 442, Test unsure 2, Test Acc 0.5063\n",
      "Epoch 0010, Train Acc 0.5415, Test Correct Num 442, Test Total Num 873, Test Unsure Num 2, Test Acc 0.5063\n",
      ">>>>Epoch 0001: Train Acc 0.1685, Loss 6.9359, Test correct 164, Test unsure 0, Test Acc 0.1879\n",
      ">>>>Epoch 0002: Train Acc 0.1928, Loss 3.5959, Test correct 178, Test unsure 0, Test Acc 0.2039\n",
      ">>>>Epoch 0003: Train Acc 0.2085, Loss 2.7254, Test correct 173, Test unsure 0, Test Acc 0.1982\n",
      ">>>>Epoch 0004: Train Acc 0.2569, Loss 2.3029, Test correct 200, Test unsure 0, Test Acc 0.2291\n",
      ">>>>Epoch 0005: Train Acc 0.3833, Loss 2.1655, Test correct 309, Test unsure 1, Test Acc 0.3540\n",
      ">>>>Epoch 0006: Train Acc 0.4723, Loss 1.9956, Test correct 369, Test unsure 0, Test Acc 0.4227\n",
      ">>>>Epoch 0007: Train Acc 0.5398, Loss 1.8968, Test correct 418, Test unsure 0, Test Acc 0.4788\n",
      ">>>>Epoch 0008: Train Acc 0.5569, Loss 1.7855, Test correct 450, Test unsure 1, Test Acc 0.5155\n",
      ">>>>Epoch 0009: Train Acc 0.5566, Loss 1.6888, Test correct 444, Test unsure 1, Test Acc 0.5086\n",
      ">>>>Epoch 0010: Train Acc 0.5847, Loss 1.6003, Test correct 470, Test unsure 1, Test Acc 0.5384\n",
      "Epoch 0010, Train Acc 0.5847, Test Correct Num 470, Test Total Num 873, Test Unsure Num 1, Test Acc 0.5384\n",
      ">>>>Epoch 0001: Train Acc 0.1244, Loss 5.3591, Test correct 114, Test unsure 0, Test Acc 0.1306\n",
      ">>>>Epoch 0002: Train Acc 0.1590, Loss 3.4712, Test correct 142, Test unsure 0, Test Acc 0.1627\n",
      ">>>>Epoch 0003: Train Acc 0.1891, Loss 2.6617, Test correct 141, Test unsure 15, Test Acc 0.1615\n",
      ">>>>Epoch 0004: Train Acc 0.3018, Loss 2.4066, Test correct 233, Test unsure 0, Test Acc 0.2669\n",
      ">>>>Epoch 0005: Train Acc 0.4308, Loss 2.1899, Test correct 341, Test unsure 0, Test Acc 0.3906\n",
      ">>>>Epoch 0006: Train Acc 0.4608, Loss 2.0551, Test correct 359, Test unsure 1, Test Acc 0.4112\n",
      ">>>>Epoch 0007: Train Acc 0.4700, Loss 1.9158, Test correct 391, Test unsure 6, Test Acc 0.4479\n",
      ">>>>Epoch 0008: Train Acc 0.5343, Loss 1.8013, Test correct 418, Test unsure 1, Test Acc 0.4788\n",
      ">>>>Epoch 0009: Train Acc 0.5492, Loss 1.6962, Test correct 436, Test unsure 2, Test Acc 0.4994\n",
      ">>>>Epoch 0010: Train Acc 0.5895, Loss 1.5917, Test correct 460, Test unsure 0, Test Acc 0.5269\n",
      "Epoch 0010, Train Acc 0.5895, Test Correct Num 460, Test Total Num 873, Test Unsure Num 0, Test Acc 0.5269\n",
      ">>>>Epoch 0001: Train Acc 0.1047, Loss 5.1394, Test correct 83, Test unsure 0, Test Acc 0.0951\n",
      ">>>>Epoch 0002: Train Acc 0.2019, Loss 3.4461, Test correct 156, Test unsure 0, Test Acc 0.1787\n",
      ">>>>Epoch 0003: Train Acc 0.2486, Loss 2.8102, Test correct 214, Test unsure 0, Test Acc 0.2451\n",
      ">>>>Epoch 0004: Train Acc 0.3879, Loss 2.3317, Test correct 303, Test unsure 16, Test Acc 0.3471\n",
      ">>>>Epoch 0005: Train Acc 0.3289, Loss 2.1339, Test correct 244, Test unsure 0, Test Acc 0.2795\n",
      ">>>>Epoch 0006: Train Acc 0.4597, Loss 1.9773, Test correct 364, Test unsure 0, Test Acc 0.4170\n",
      ">>>>Epoch 0007: Train Acc 0.4828, Loss 1.8616, Test correct 385, Test unsure 0, Test Acc 0.4410\n",
      ">>>>Epoch 0008: Train Acc 0.5366, Loss 1.7480, Test correct 419, Test unsure 0, Test Acc 0.4800\n",
      ">>>>Epoch 0009: Train Acc 0.5492, Loss 1.6437, Test correct 427, Test unsure 0, Test Acc 0.4891\n",
      ">>>>Epoch 0010: Train Acc 0.6021, Loss 1.5459, Test correct 454, Test unsure 0, Test Acc 0.5200\n",
      "Epoch 0010, Train Acc 0.6021, Test Correct Num 454, Test Total Num 873, Test Unsure Num 0, Test Acc 0.5200\n",
      ">>>>Epoch 0001: Train Acc 0.0741, Loss 10.1960, Test correct 64, Test unsure 0, Test Acc 0.0733\n",
      ">>>>Epoch 0002: Train Acc 0.2203, Loss 5.7008, Test correct 204, Test unsure 0, Test Acc 0.2337\n",
      ">>>>Epoch 0003: Train Acc 0.1284, Loss 3.4729, Test correct 90, Test unsure 4, Test Acc 0.1031\n",
      ">>>>Epoch 0004: Train Acc 0.1793, Loss 2.8636, Test correct 134, Test unsure 0, Test Acc 0.1535\n",
      ">>>>Epoch 0005: Train Acc 0.2420, Loss 2.4996, Test correct 187, Test unsure 0, Test Acc 0.2142\n",
      ">>>>Epoch 0006: Train Acc 0.2883, Loss 2.2600, Test correct 238, Test unsure 5, Test Acc 0.2726\n",
      ">>>>Epoch 0007: Train Acc 0.4405, Loss 2.0560, Test correct 332, Test unsure 3, Test Acc 0.3803\n",
      ">>>>Epoch 0008: Train Acc 0.4989, Loss 1.9158, Test correct 393, Test unsure 1, Test Acc 0.4502\n",
      ">>>>Epoch 0009: Train Acc 0.5040, Loss 1.7879, Test correct 418, Test unsure 1, Test Acc 0.4788\n",
      ">>>>Epoch 0010: Train Acc 0.5541, Loss 1.6912, Test correct 439, Test unsure 1, Test Acc 0.5029\n",
      "Epoch 0010, Train Acc 0.5541, Test Correct Num 439, Test Total Num 873, Test Unsure Num 1, Test Acc 0.5029\n",
      ">>>>Epoch 0001: Train Acc 0.2460, Loss 8.1399, Test correct 206, Test unsure 0, Test Acc 0.2360\n",
      ">>>>Epoch 0002: Train Acc 0.1711, Loss 4.7189, Test correct 158, Test unsure 0, Test Acc 0.1810\n",
      ">>>>Epoch 0003: Train Acc 0.1957, Loss 2.7548, Test correct 177, Test unsure 0, Test Acc 0.2027\n",
      ">>>>Epoch 0004: Train Acc 0.2314, Loss 2.5195, Test correct 182, Test unsure 0, Test Acc 0.2085\n",
      ">>>>Epoch 0005: Train Acc 0.3135, Loss 2.2401, Test correct 263, Test unsure 0, Test Acc 0.3013\n",
      ">>>>Epoch 0006: Train Acc 0.4471, Loss 2.0108, Test correct 369, Test unsure 1, Test Acc 0.4227\n",
      ">>>>Epoch 0007: Train Acc 0.4943, Loss 1.8957, Test correct 400, Test unsure 1, Test Acc 0.4582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>Epoch 0008: Train Acc 0.5152, Loss 1.7704, Test correct 421, Test unsure 0, Test Acc 0.4822\n",
      ">>>>Epoch 0009: Train Acc 0.5395, Loss 1.6796, Test correct 442, Test unsure 0, Test Acc 0.5063\n",
      ">>>>Epoch 0010: Train Acc 0.5549, Loss 1.5890, Test correct 433, Test unsure 0, Test Acc 0.4960\n",
      "Epoch 0009, Train Acc 0.5395, Test Correct Num 442, Test Total Num 873, Test Unsure Num 0, Test Acc 0.5063\n",
      ">>>>Epoch 0001: Train Acc 0.1193, Loss 5.8731, Test correct 91, Test unsure 0, Test Acc 0.1042\n",
      ">>>>Epoch 0002: Train Acc 0.1971, Loss 3.4412, Test correct 146, Test unsure 0, Test Acc 0.1672\n",
      ">>>>Epoch 0003: Train Acc 0.2171, Loss 2.7644, Test correct 174, Test unsure 0, Test Acc 0.1993\n",
      ">>>>Epoch 0004: Train Acc 0.2918, Loss 2.3538, Test correct 232, Test unsure 0, Test Acc 0.2658\n",
      ">>>>Epoch 0005: Train Acc 0.3450, Loss 2.1559, Test correct 261, Test unsure 18, Test Acc 0.2990\n",
      ">>>>Epoch 0006: Train Acc 0.4826, Loss 2.0021, Test correct 392, Test unsure 0, Test Acc 0.4490\n",
      ">>>>Epoch 0007: Train Acc 0.4957, Loss 1.8769, Test correct 402, Test unsure 0, Test Acc 0.4605\n",
      ">>>>Epoch 0008: Train Acc 0.5326, Loss 1.7621, Test correct 419, Test unsure 1, Test Acc 0.4800\n",
      ">>>>Epoch 0009: Train Acc 0.5675, Loss 1.6656, Test correct 447, Test unsure 0, Test Acc 0.5120\n",
      ">>>>Epoch 0010: Train Acc 0.5815, Loss 1.5741, Test correct 460, Test unsure 0, Test Acc 0.5269\n",
      "Epoch 0010, Train Acc 0.5815, Test Correct Num 460, Test Total Num 873, Test Unsure Num 0, Test Acc 0.5269\n",
      ">>>>Epoch 0001: Train Acc 0.0801, Loss 7.2918, Test correct 68, Test unsure 0, Test Acc 0.0779\n",
      ">>>>Epoch 0002: Train Acc 0.1682, Loss 3.8054, Test correct 137, Test unsure 0, Test Acc 0.1569\n",
      ">>>>Epoch 0003: Train Acc 0.1568, Loss 3.1510, Test correct 149, Test unsure 0, Test Acc 0.1707\n",
      ">>>>Epoch 0004: Train Acc 0.3501, Loss 2.6524, Test correct 298, Test unsure 1, Test Acc 0.3414\n",
      ">>>>Epoch 0005: Train Acc 0.3021, Loss 2.1373, Test correct 251, Test unsure 2, Test Acc 0.2875\n",
      ">>>>Epoch 0006: Train Acc 0.4857, Loss 2.0204, Test correct 382, Test unsure 3, Test Acc 0.4376\n",
      ">>>>Epoch 0007: Train Acc 0.4860, Loss 1.8512, Test correct 400, Test unsure 2, Test Acc 0.4582\n",
      ">>>>Epoch 0008: Train Acc 0.5229, Loss 1.7595, Test correct 433, Test unsure 2, Test Acc 0.4960\n",
      ">>>>Epoch 0009: Train Acc 0.5727, Loss 1.6535, Test correct 453, Test unsure 3, Test Acc 0.5189\n",
      ">>>>Epoch 0010: Train Acc 0.5466, Loss 1.5573, Test correct 445, Test unsure 4, Test Acc 0.5097\n",
      "Epoch 0009, Train Acc 0.5727, Test Correct Num 453, Test Total Num 873, Test Unsure Num 3, Test Acc 0.5189\n",
      "--- Finished Adam\n",
      "--- Starting GA\n",
      "--- -- Finished fitness evaluation, length: 10\n",
      "--- -- Finished creating offspring population\n",
      "--- -- Finished evaluating fitness of offspring population\n",
      "--- Finished GA,Time:107138.77820968628ms\n",
      "Currently in generation 2\n",
      "--- Starting Adam\n",
      ">>>>Epoch 0001: Train Acc 0.2998, Loss 2.5921, Test correct 243, Test unsure 0, Test Acc 0.2784\n",
      ">>>>Epoch 0002: Train Acc 0.4273, Loss 2.1005, Test correct 340, Test unsure 0, Test Acc 0.3895\n",
      ">>>>Epoch 0003: Train Acc 0.4783, Loss 1.9701, Test correct 409, Test unsure 0, Test Acc 0.4685\n",
      ">>>>Epoch 0004: Train Acc 0.4551, Loss 1.8471, Test correct 368, Test unsure 2, Test Acc 0.4215\n",
      ">>>>Epoch 0005: Train Acc 0.5652, Loss 1.7257, Test correct 461, Test unsure 4, Test Acc 0.5281\n",
      ">>>>Epoch 0006: Train Acc 0.5395, Loss 1.6216, Test correct 433, Test unsure 0, Test Acc 0.4960\n",
      ">>>>Epoch 0007: Train Acc 0.6219, Loss 1.5143, Test correct 490, Test unsure 2, Test Acc 0.5613\n",
      ">>>>Epoch 0008: Train Acc 0.5984, Loss 1.4124, Test correct 472, Test unsure 0, Test Acc 0.5407\n",
      ">>>>Epoch 0009: Train Acc 0.6585, Loss 1.3359, Test correct 509, Test unsure 0, Test Acc 0.5830\n",
      ">>>>Epoch 0010: Train Acc 0.6659, Loss 1.2567, Test correct 509, Test unsure 1, Test Acc 0.5830\n",
      "Epoch 0010, Train Acc 0.6659, Test Correct Num 509, Test Total Num 873, Test Unsure Num 1, Test Acc 0.5830\n",
      ">>>>Epoch 0001: Train Acc 0.2663, Loss 2.4936, Test correct 226, Test unsure 1, Test Acc 0.2589\n",
      ">>>>Epoch 0002: Train Acc 0.4637, Loss 2.0728, Test correct 374, Test unsure 0, Test Acc 0.4284\n",
      ">>>>Epoch 0003: Train Acc 0.5655, Loss 1.9502, Test correct 450, Test unsure 1, Test Acc 0.5155\n",
      ">>>>Epoch 0004: Train Acc 0.5386, Loss 1.8149, Test correct 427, Test unsure 0, Test Acc 0.4891\n",
      ">>>>Epoch 0005: Train Acc 0.5778, Loss 1.6778, Test correct 452, Test unsure 0, Test Acc 0.5178\n",
      ">>>>Epoch 0006: Train Acc 0.5895, Loss 1.5665, Test correct 474, Test unsure 1, Test Acc 0.5430\n",
      ">>>>Epoch 0007: Train Acc 0.6141, Loss 1.4669, Test correct 484, Test unsure 0, Test Acc 0.5544\n",
      ">>>>Epoch 0008: Train Acc 0.6496, Loss 1.3752, Test correct 511, Test unsure 0, Test Acc 0.5853\n",
      ">>>>Epoch 0009: Train Acc 0.6622, Loss 1.2941, Test correct 502, Test unsure 0, Test Acc 0.5750\n",
      ">>>>Epoch 0010: Train Acc 0.6679, Loss 1.2211, Test correct 520, Test unsure 1, Test Acc 0.5956\n",
      "Epoch 0010, Train Acc 0.6679, Test Correct Num 520, Test Total Num 873, Test Unsure Num 1, Test Acc 0.5956\n",
      ">>>>Epoch 0001: Train Acc 0.4691, Loss 2.2544, Test correct 364, Test unsure 0, Test Acc 0.4170\n",
      ">>>>Epoch 0002: Train Acc 0.5034, Loss 2.0195, Test correct 417, Test unsure 1, Test Acc 0.4777\n",
      ">>>>Epoch 0003: Train Acc 0.5423, Loss 1.8770, Test correct 436, Test unsure 4, Test Acc 0.4994\n",
      ">>>>Epoch 0004: Train Acc 0.5589, Loss 1.7500, Test correct 438, Test unsure 7, Test Acc 0.5017\n",
      ">>>>Epoch 0005: Train Acc 0.5612, Loss 1.6338, Test correct 464, Test unsure 4, Test Acc 0.5315\n",
      ">>>>Epoch 0006: Train Acc 0.6038, Loss 1.5395, Test correct 462, Test unsure 2, Test Acc 0.5292\n",
      ">>>>Epoch 0007: Train Acc 0.6247, Loss 1.4398, Test correct 477, Test unsure 1, Test Acc 0.5464\n",
      ">>>>Epoch 0008: Train Acc 0.6882, Loss 1.3565, Test correct 528, Test unsure 2, Test Acc 0.6048\n",
      ">>>>Epoch 0009: Train Acc 0.6353, Loss 1.2714, Test correct 501, Test unsure 0, Test Acc 0.5739\n",
      ">>>>Epoch 0010: Train Acc 0.7208, Loss 1.2076, Test correct 552, Test unsure 0, Test Acc 0.6323\n",
      "Epoch 0010, Train Acc 0.7208, Test Correct Num 552, Test Total Num 873, Test Unsure Num 0, Test Acc 0.6323\n",
      ">>>>Epoch 0001: Train Acc 0.3241, Loss 2.8341, Test correct 292, Test unsure 0, Test Acc 0.3345\n",
      ">>>>Epoch 0002: Train Acc 0.3547, Loss 2.1253, Test correct 263, Test unsure 0, Test Acc 0.3013\n",
      ">>>>Epoch 0003: Train Acc 0.5237, Loss 2.0034, Test correct 427, Test unsure 0, Test Acc 0.4891\n",
      ">>>>Epoch 0004: Train Acc 0.4568, Loss 1.8484, Test correct 383, Test unsure 0, Test Acc 0.4387\n",
      ">>>>Epoch 0005: Train Acc 0.5735, Loss 1.7258, Test correct 434, Test unsure 3, Test Acc 0.4971\n",
      ">>>>Epoch 0006: Train Acc 0.5669, Loss 1.6289, Test correct 459, Test unsure 1, Test Acc 0.5258\n",
      ">>>>Epoch 0007: Train Acc 0.5904, Loss 1.5217, Test correct 479, Test unsure 0, Test Acc 0.5487\n",
      ">>>>Epoch 0008: Train Acc 0.6376, Loss 1.4245, Test correct 498, Test unsure 0, Test Acc 0.5704\n",
      ">>>>Epoch 0009: Train Acc 0.6404, Loss 1.3448, Test correct 508, Test unsure 0, Test Acc 0.5819\n",
      ">>>>Epoch 0010: Train Acc 0.6779, Loss 1.2672, Test correct 533, Test unsure 0, Test Acc 0.6105\n",
      "Epoch 0010, Train Acc 0.6779, Test Correct Num 533, Test Total Num 873, Test Unsure Num 0, Test Acc 0.6105\n",
      ">>>>Epoch 0001: Train Acc 0.2849, Loss 2.5738, Test correct 262, Test unsure 0, Test Acc 0.3001\n",
      ">>>>Epoch 0002: Train Acc 0.2789, Loss 2.0879, Test correct 208, Test unsure 0, Test Acc 0.2383\n",
      ">>>>Epoch 0003: Train Acc 0.4614, Loss 1.9661, Test correct 388, Test unsure 0, Test Acc 0.4444\n",
      ">>>>Epoch 0004: Train Acc 0.5166, Loss 1.8242, Test correct 415, Test unsure 0, Test Acc 0.4754\n",
      ">>>>Epoch 0005: Train Acc 0.6078, Loss 1.7105, Test correct 480, Test unsure 2, Test Acc 0.5498\n",
      ">>>>Epoch 0006: Train Acc 0.5644, Loss 1.5988, Test correct 456, Test unsure 1, Test Acc 0.5223\n",
      ">>>>Epoch 0007: Train Acc 0.6307, Loss 1.5005, Test correct 499, Test unsure 0, Test Acc 0.5716\n",
      ">>>>Epoch 0008: Train Acc 0.6021, Loss 1.4085, Test correct 479, Test unsure 0, Test Acc 0.5487\n",
      ">>>>Epoch 0009: Train Acc 0.6773, Loss 1.3386, Test correct 519, Test unsure 1, Test Acc 0.5945\n",
      ">>>>Epoch 0010: Train Acc 0.6404, Loss 1.2582, Test correct 497, Test unsure 0, Test Acc 0.5693\n",
      "Epoch 0009, Train Acc 0.6773, Test Correct Num 519, Test Total Num 873, Test Unsure Num 1, Test Acc 0.5945\n",
      ">>>>Epoch 0001: Train Acc 0.4388, Loss 2.4589, Test correct 358, Test unsure 0, Test Acc 0.4101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>Epoch 0002: Train Acc 0.4720, Loss 2.0520, Test correct 391, Test unsure 2, Test Acc 0.4479\n",
      ">>>>Epoch 0003: Train Acc 0.5086, Loss 1.9080, Test correct 412, Test unsure 7, Test Acc 0.4719\n",
      ">>>>Epoch 0004: Train Acc 0.5355, Loss 1.7761, Test correct 446, Test unsure 1, Test Acc 0.5109\n",
      ">>>>Epoch 0005: Train Acc 0.5452, Loss 1.6559, Test correct 446, Test unsure 3, Test Acc 0.5109\n",
      ">>>>Epoch 0006: Train Acc 0.5758, Loss 1.5574, Test correct 452, Test unsure 3, Test Acc 0.5178\n",
      ">>>>Epoch 0007: Train Acc 0.6250, Loss 1.4540, Test correct 503, Test unsure 2, Test Acc 0.5762\n",
      ">>>>Epoch 0008: Train Acc 0.6144, Loss 1.3745, Test correct 467, Test unsure 1, Test Acc 0.5349\n",
      ">>>>Epoch 0009: Train Acc 0.6610, Loss 1.2912, Test correct 532, Test unsure 0, Test Acc 0.6094\n",
      ">>>>Epoch 0010: Train Acc 0.6911, Loss 1.2138, Test correct 531, Test unsure 0, Test Acc 0.6082\n",
      "Epoch 0009, Train Acc 0.6610, Test Correct Num 532, Test Total Num 873, Test Unsure Num 0, Test Acc 0.6094\n",
      ">>>>Epoch 0001: Train Acc 0.5998, Loss 1.4274, Test correct 445, Test unsure 0, Test Acc 0.5097\n",
      ">>>>Epoch 0002: Train Acc 0.6665, Loss 1.3310, Test correct 495, Test unsure 1, Test Acc 0.5670\n",
      ">>>>Epoch 0003: Train Acc 0.6259, Loss 1.2413, Test correct 494, Test unsure 0, Test Acc 0.5659\n",
      ">>>>Epoch 0004: Train Acc 0.7148, Loss 1.1719, Test correct 544, Test unsure 0, Test Acc 0.6231\n",
      ">>>>Epoch 0005: Train Acc 0.6939, Loss 1.0951, Test correct 542, Test unsure 0, Test Acc 0.6208\n",
      ">>>>Epoch 0006: Train Acc 0.7497, Loss 1.0361, Test correct 572, Test unsure 0, Test Acc 0.6552\n",
      ">>>>Epoch 0007: Train Acc 0.7594, Loss 0.9769, Test correct 579, Test unsure 0, Test Acc 0.6632\n",
      ">>>>Epoch 0008: Train Acc 0.7646, Loss 0.9276, Test correct 579, Test unsure 0, Test Acc 0.6632\n",
      ">>>>Epoch 0009: Train Acc 0.7663, Loss 0.8930, Test correct 570, Test unsure 0, Test Acc 0.6529\n",
      ">>>>Epoch 0010: Train Acc 0.7732, Loss 0.8448, Test correct 587, Test unsure 0, Test Acc 0.6724\n",
      "Epoch 0010, Train Acc 0.7732, Test Correct Num 587, Test Total Num 873, Test Unsure Num 0, Test Acc 0.6724\n",
      ">>>>Epoch 0001: Train Acc 0.3822, Loss 2.3603, Test correct 310, Test unsure 0, Test Acc 0.3551\n",
      ">>>>Epoch 0002: Train Acc 0.4614, Loss 2.0329, Test correct 352, Test unsure 1, Test Acc 0.4032\n",
      ">>>>Epoch 0003: Train Acc 0.4848, Loss 1.8492, Test correct 401, Test unsure 2, Test Acc 0.4593\n",
      ">>>>Epoch 0004: Train Acc 0.5289, Loss 1.7328, Test correct 438, Test unsure 4, Test Acc 0.5017\n",
      ">>>>Epoch 0005: Train Acc 0.5944, Loss 1.6184, Test correct 477, Test unsure 3, Test Acc 0.5464\n",
      ">>>>Epoch 0006: Train Acc 0.5575, Loss 1.5193, Test correct 462, Test unsure 2, Test Acc 0.5292\n",
      ">>>>Epoch 0007: Train Acc 0.6273, Loss 1.4267, Test correct 491, Test unsure 2, Test Acc 0.5624\n",
      ">>>>Epoch 0008: Train Acc 0.6081, Loss 1.3458, Test correct 487, Test unsure 2, Test Acc 0.5578\n",
      ">>>>Epoch 0009: Train Acc 0.6776, Loss 1.2657, Test correct 521, Test unsure 1, Test Acc 0.5968\n",
      ">>>>Epoch 0010: Train Acc 0.6782, Loss 1.1861, Test correct 523, Test unsure 3, Test Acc 0.5991\n",
      "Epoch 0010, Train Acc 0.6782, Test Correct Num 523, Test Total Num 873, Test Unsure Num 3, Test Acc 0.5991\n",
      ">>>>Epoch 0001: Train Acc 0.5469, Loss 1.5849, Test correct 438, Test unsure 1, Test Acc 0.5017\n",
      ">>>>Epoch 0002: Train Acc 0.6433, Loss 1.4695, Test correct 510, Test unsure 0, Test Acc 0.5842\n",
      ">>>>Epoch 0003: Train Acc 0.6310, Loss 1.3706, Test correct 510, Test unsure 0, Test Acc 0.5842\n",
      ">>>>Epoch 0004: Train Acc 0.6785, Loss 1.2781, Test correct 515, Test unsure 0, Test Acc 0.5899\n",
      ">>>>Epoch 0005: Train Acc 0.6653, Loss 1.2144, Test correct 526, Test unsure 0, Test Acc 0.6025\n",
      ">>>>Epoch 0006: Train Acc 0.7140, Loss 1.1497, Test correct 544, Test unsure 0, Test Acc 0.6231\n",
      ">>>>Epoch 0007: Train Acc 0.6699, Loss 1.0841, Test correct 512, Test unsure 0, Test Acc 0.5865\n",
      ">>>>Epoch 0008: Train Acc 0.7623, Loss 1.0304, Test correct 576, Test unsure 0, Test Acc 0.6598\n",
      ">>>>Epoch 0009: Train Acc 0.7620, Loss 0.9811, Test correct 561, Test unsure 0, Test Acc 0.6426\n",
      ">>>>Epoch 0010: Train Acc 0.7574, Loss 0.9204, Test correct 568, Test unsure 0, Test Acc 0.6506\n",
      "Epoch 0008, Train Acc 0.7623, Test Correct Num 576, Test Total Num 873, Test Unsure Num 0, Test Acc 0.6598\n",
      ">>>>Epoch 0001: Train Acc 0.3850, Loss 2.3379, Test correct 323, Test unsure 1, Test Acc 0.3700\n",
      ">>>>Epoch 0002: Train Acc 0.4946, Loss 2.0594, Test correct 401, Test unsure 0, Test Acc 0.4593\n",
      ">>>>Epoch 0003: Train Acc 0.5226, Loss 1.9247, Test correct 431, Test unsure 3, Test Acc 0.4937\n",
      ">>>>Epoch 0004: Train Acc 0.5320, Loss 1.7861, Test correct 427, Test unsure 2, Test Acc 0.4891\n",
      ">>>>Epoch 0005: Train Acc 0.5767, Loss 1.6610, Test correct 461, Test unsure 1, Test Acc 0.5281\n",
      ">>>>Epoch 0006: Train Acc 0.5884, Loss 1.5442, Test correct 462, Test unsure 0, Test Acc 0.5292\n",
      ">>>>Epoch 0007: Train Acc 0.6261, Loss 1.4586, Test correct 487, Test unsure 1, Test Acc 0.5578\n",
      ">>>>Epoch 0008: Train Acc 0.6493, Loss 1.3594, Test correct 499, Test unsure 0, Test Acc 0.5716\n",
      ">>>>Epoch 0009: Train Acc 0.6702, Loss 1.2658, Test correct 513, Test unsure 2, Test Acc 0.5876\n",
      ">>>>Epoch 0010: Train Acc 0.6879, Loss 1.1908, Test correct 533, Test unsure 2, Test Acc 0.6105\n",
      "Epoch 0010, Train Acc 0.6879, Test Correct Num 533, Test Total Num 873, Test Unsure Num 2, Test Acc 0.6105\n",
      "--- Finished Adam\n",
      "--- Starting GA\n",
      "--- -- Finished fitness evaluation, length: 10\n",
      "--- -- Finished creating offspring population\n",
      "--- -- Finished evaluating fitness of offspring population\n",
      "--- Finished GA,Time:118185.54759025574ms\n",
      "Currently in generation 3\n",
      "--- Starting Adam\n",
      ">>>>Epoch 0001: Train Acc 0.4359, Loss 1.8775, Test correct 360, Test unsure 5, Test Acc 0.4124\n",
      ">>>>Epoch 0002: Train Acc 0.5787, Loss 1.7039, Test correct 459, Test unsure 1, Test Acc 0.5258\n",
      ">>>>Epoch 0003: Train Acc 0.5838, Loss 1.5925, Test correct 451, Test unsure 3, Test Acc 0.5166\n",
      ">>>>Epoch 0004: Train Acc 0.6213, Loss 1.4918, Test correct 499, Test unsure 1, Test Acc 0.5716\n",
      ">>>>Epoch 0005: Train Acc 0.6731, Loss 1.4103, Test correct 510, Test unsure 2, Test Acc 0.5842\n",
      ">>>>Epoch 0006: Train Acc 0.6628, Loss 1.3218, Test correct 522, Test unsure 1, Test Acc 0.5979\n",
      ">>>>Epoch 0007: Train Acc 0.6862, Loss 1.2602, Test correct 526, Test unsure 2, Test Acc 0.6025\n",
      ">>>>Epoch 0008: Train Acc 0.7142, Loss 1.1661, Test correct 537, Test unsure 2, Test Acc 0.6151\n",
      ">>>>Epoch 0009: Train Acc 0.7260, Loss 1.1055, Test correct 565, Test unsure 0, Test Acc 0.6472\n",
      ">>>>Epoch 0010: Train Acc 0.7537, Loss 1.0448, Test correct 559, Test unsure 0, Test Acc 0.6403\n",
      "Epoch 0009, Train Acc 0.7260, Test Correct Num 565, Test Total Num 873, Test Unsure Num 0, Test Acc 0.6472\n",
      ">>>>Epoch 0001: Train Acc 0.5684, Loss 1.7055, Test correct 446, Test unsure 0, Test Acc 0.5109\n",
      ">>>>Epoch 0002: Train Acc 0.6568, Loss 1.5115, Test correct 521, Test unsure 0, Test Acc 0.5968\n",
      ">>>>Epoch 0003: Train Acc 0.6713, Loss 1.3711, Test correct 515, Test unsure 1, Test Acc 0.5899\n",
      ">>>>Epoch 0004: Train Acc 0.6931, Loss 1.2979, Test correct 537, Test unsure 1, Test Acc 0.6151\n",
      ">>>>Epoch 0005: Train Acc 0.6776, Loss 1.2062, Test correct 514, Test unsure 0, Test Acc 0.5888\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:300] . unexpected pos 901888 vs 901840",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0mnum_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-dc66c529c011>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                \u001b[0mnum_cells\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_cells\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_genes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_genes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                      \u001b[0mtrain_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                                      labels = labels)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mTrain_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"All Time:{(Train_start-Train_end)*1000}ms\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-18769dddc60c>\u001b[0m in \u001b[0;36mGA_Neural_train\u001b[0;34m(population, pop_size, max_generations, Adam_epochs, GA_steps, offspring_size, elitist_level, rho, learning_rate, weight_decay, batch_size, num_cells, num_genes, graph, train_ids, test_ids, labels)\u001b[0m\n\u001b[1;32m     30\u001b[0m                                                         \u001b[0mtest_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                                                         labels = labels)\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"--- Finished Adam\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-97c4b4ac5ba7>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0m_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mmax_test_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             print(\n\u001b[1;32m     60\u001b[0m                 \u001b[0;34mf\">>>>Epoch {epoch+1:04d}: Train Acc {train_acc:.4f}, Loss {loss / len(self.train_ids):.4f}, Test correct {test_correct}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-97c4b4ac5ba7>\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m         }\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"GA_Model.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:300] . unexpected pos 901888 vs 901840"
     ]
    }
   ],
   "source": [
    "Train_start = time.time()\n",
    "trained_population = GA_Neural_train(population=population,\n",
    "                                    pop_size = pop_size,\n",
    "                                    max_generations=max_generations,\n",
    "                                    Adam_epochs=Adam_epochs,GA_steps=GA_steps,\n",
    "                                    offspring_size=offspring_size,elitist_level=elitist_level,rho=rho,\n",
    "                                    learning_rate=learning_rate,\n",
    "                               weight_decay = weight_decay,\n",
    "                               batch_size = batch_size,\n",
    "                               num_cells = num_cells,num_genes = num_genes,graph = graph,\n",
    "                                     train_ids = train_ids,test_ids = test_ids,\n",
    "                                     labels = labels)\n",
    "Train_end = time.time()\n",
    "print(f\"All Time:{(Train_start-Train_end)*1000}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28b3285",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
